{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of Contents\n\n* [1. Code explanation](#section1)\n* [2. Module installation, import](#section2)\n* [3. Identify environment](#section3)\n* [4. Configurations](#section4)\n* [5. Dataset](#section5)\n* [6. Model](#section6)\n* [7. Function to train and get accuracy](#section7)\n* [8. Hyperparameter tuning with Optuna library](#section8)","metadata":{}},{"cell_type":"markdown","source":"# 1. Code explanation<a class=\"anchor\" id=\"section1\"></a>\n1. 머신 러닝 코드 competition이 개최되는 **Kaggle**의 대회인 **Feedback Prize - English Language Learning**에 사용할 목적으로 작성  \n   대회의 목적: 미국의 8학년6개의 평가 항목에 점수가 매겨진 글들을 학습하여 어떤 글이 주어졌을 때 각 항목에서 최대한 정확한 점수 도출   \n   --> 정확한 피드백으로학습에 도움 주려는 목적\n2. Transformer 언어 모델의 일종인 Deberta-v3 모델을 대회 데이터로 fine-tuning  \n   --> 하이퍼 파라미터 최적화를 위해 **OPTUNA** 라이브러리 사용, **weight-and-bias (WandB)** 라이브러리와 사이트를 통한 로그 기록(결과 비교 및 분석)\n2. Kaggle 사이트 및 로컬 컴퓨터에서 모두 사용 가능한 코드  \n3. **TPU** (Tensor processing unit, Kaggle에서 사용 가능, 일반적으로 GPU(NVIDIA P100) 보다 계산 속도 수 배 이상 빠름)  \n   / **GPU** (Graphics processing unit, Tensorflow의 mirrored strategy를 통해 여러 GPU 사용 학습) 겸용으로 만들었으나  \n   아래의 **링크에 설명된 이슈에 따라 TPU가 더 느린 문제점 존재**  \n    https://github.com/huggingface/transformers/issues/18239\n4. 아래 링크에서 모델 가져와서 OPTUNA, WANDB 추가하고 TPU 사용 가능 포멧으로 변경한 뒤 하이퍼 파라미터 튜닝  \n   Reference: https://www.kaggle.com/code/electro/deberta-layerwiselr-lastlayerreinit-tensorflow  \n   ","metadata":{}},{"cell_type":"markdown","source":"# 2. Module installation, import<a class=\"anchor\" id=\"section2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Kaggle TPU의 tensorflow 버전 디폴트 세팅으로는 deberta-v3 모델을 사용할 수 없어서 tensorflow 2.7.4 버전으로 업데이트\nReference: https://www.kaggle.com/getting-started/210020  ","metadata":{}},{"cell_type":"code","source":"try:\n    from cloud_tpu_client import Client\n    c = Client()\n    c.configure_tpu_version('tensorflow==2.7.4', restart_type='ifNeeded')\n    !pip uninstall -y transformers\n    !pip install -qU git+https://github.com/huggingface/transformers\n\n    !pip uninstall keras -y\n    !pip uninstall keras-nightly -y\n    !pip uninstall keras-Preprocessing -y\n    !pip uninstall keras-vis -y\n    !pip uninstall tensorflow -y\n\n    !pip install -qU tensorflow==2.7.4\n    !pip install -qU keras==2.7.0\n    is_tpu = True\nexcept:\n    print('This is not TPU notebook')\n    is_tpu = False","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:51:57.385024Z","iopub.execute_input":"2023-01-02T20:51:57.385379Z","iopub.status.idle":"2023-01-02T20:51:57.436169Z","shell.execute_reply.started":"2023-01-02T20:51:57.385301Z","shell.execute_reply":"2023-01-02T20:51:57.435112Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"This is not TPU notebook\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom transformers import  AutoTokenizer,  TFAutoModel, AutoConfig\nfrom transformers import RobertaConfig\nfrom transformers import TFDebertaV2Model\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport random\nimport optuna\n\nfrom tensorflow.keras import Model\nimport pickle\nimport gc\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nimport tensorflow_addons as tfa","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:51:57.437978Z","iopub.execute_input":"2023-01-02T20:51:57.438338Z","iopub.status.idle":"2023-01-02T20:52:15.278573Z","shell.execute_reply.started":"2023-01-02T20:51:57.438300Z","shell.execute_reply":"2023-01-02T20:52:15.277512Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2023-01-02 20:52:07.087358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:07.088374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:07.089416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:07.090208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:07.090992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:07.091746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:07.094979: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-02 20:52:07.353456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:07.354309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:07.355029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:07.355785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:07.356565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:07.357297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:14.585943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:14.587132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:14.587904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:14.588723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:14.589541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:14.590226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13351 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2023-01-02 20:52:14.594055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-02 20:52:14.594757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13351 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3. Identify environment<a class=\"anchor\" id=\"section3\"></a>","metadata":{}},{"cell_type":"code","source":"#계산 환경 파악 --> 로컬 or Kaggle\ntry:\n    from kaggle_datasets import KaggleDatasets\n    from kaggle_secrets import UserSecretsClient\n    is_local = False\nexcept:\n    print('Running in my computer!')\n    is_local = True\n    \ntry:\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"WANDB\")\nexcept:\n    print('Your WANDB key must be attached to Add-ons --> secrets')\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:52:15.279942Z","iopub.execute_input":"2023-01-02T20:52:15.282324Z","iopub.status.idle":"2023-01-02T20:52:15.488026Z","shell.execute_reply.started":"2023-01-02T20:52:15.282270Z","shell.execute_reply":"2023-01-02T20:52:15.487136Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    try:\n        strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n        if(strategy.num_replicas_in_sync==1):\n            strategy = tf.distribute.experimental.CentralStorageStrategy()\n    \n    except: strategy = tf.distribute.experimental.CentralStorageStrategy()\n\n        \nprint('Number of replicas:', strategy.num_replicas_in_sync)\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:52:15.490940Z","iopub.execute_input":"2023-01-02T20:52:15.491451Z","iopub.status.idle":"2023-01-02T20:52:15.817243Z","shell.execute_reply.started":"2023-01-02T20:52:15.491410Z","shell.execute_reply":"2023-01-02T20:52:15.816319Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Number of replicas: 2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4. Configurations<a class=\"anchor\" id=\"section4\"></a>\nWandB 로깅 여부 및 fold 개수, batch_size, 디버깅 모드(빠른 계산 통한 에러 해결) 등 계산 옵션 설정 ","metadata":{}},{"cell_type":"code","source":"try:\n    gcs_path = KaggleDatasets().get_gcs_path('feedback-deberta-v3-tfrecord')\nexcept:\n    gcs_path = '../input/feedback-deberta-v3-tfrecord'\n\nconfig={\n    'is_debug': False,\n    'is_wandb': False,\n    'save_model': True,\n    'project': 'Feedback prize outputwise error',\n    'name'   : 'deberta-v3-base_rmse_error',\n    'group'  : 'run_test',    \n    'fold_select': 0,\n    'seed': 22,\n    'is_base_model_fixed': False,\n    'batch_size':2*strategy.num_replicas_in_sync,\n    'buffer_size': 3200,\n    'seq_len': 512,\n    'checkpoint_path': './mymodel',\n    'epochs': 30,\n    'auto': tf.data.experimental.AUTOTUNE,\n    'model': '/kaggle/input/debertav3base',\n    'model_path': '/kaggle/input/debertav3base',\n    'n_fold': 5,\n    'target_cols': ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n    'gcs_path': gcs_path,\n    'data_path': '/feedback_deberta-v3-base_train',\n    'is_tpu': is_tpu,\n    'is_local': is_local    \n}\n\ndel gcs_path\ngc.collect()\n\nif(config['is_debug']):\n    config['epochs'] = 2    ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:52:15.818663Z","iopub.execute_input":"2023-01-02T20:52:15.819302Z","iopub.status.idle":"2023-01-02T20:52:20.262725Z","shell.execute_reply.started":"2023-01-02T20:52:15.819263Z","shell.execute_reply":"2023-01-02T20:52:20.261693Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# ⭐ WandB\n<img src=\"https://camo.githubusercontent.com/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\" width=600>\n\nWeights & Biases (W&B) is MLOps platform for tracking our experiemnts. We can use it to Build better models faster with experiment tracking, dataset versioning, and model management. Some of the cool features of W&B:\n\n* Track, compare, and visualize ML experiments\n* Get live metrics, terminal logs, and system stats streamed to the centralized dashboard.\n* Explain how your model works, show graphs of how model versions improved, discuss bugs, and demonstrate progress towards milestones.\n* https://wandb.ai","metadata":{"tags":[]}},{"cell_type":"code","source":"if(not is_local):\n    if(config['is_wandb']):\n        !pip install -qU wandb --upgrade\n        import wandb\n        #from wandb.keras import WandbCallback\n        os.environ[\"WANDB_SILENT\"] = \"true\"\n        wandb.login(key = secret_value_0)\nelse:\n    import wandb\n    os.environ[\"WANDB_SILENT\"] = \"true\"\n    wandb.login()  \n    ","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:52:20.265917Z","iopub.execute_input":"2023-01-02T20:52:20.266725Z","iopub.status.idle":"2023-01-02T20:52:20.273774Z","shell.execute_reply.started":"2023-01-02T20:52:20.266695Z","shell.execute_reply":"2023-01-02T20:52:20.272438Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(config['seed'])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:52:20.275427Z","iopub.execute_input":"2023-01-02T20:52:20.275885Z","iopub.status.idle":"2023-01-02T20:52:20.290461Z","shell.execute_reply.started":"2023-01-02T20:52:20.275842Z","shell.execute_reply":"2023-01-02T20:52:20.289296Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# 5. Dataset<a class=\"anchor\" id=\"section5\"></a>","metadata":{}},{"cell_type":"markdown","source":"TFRecord format --> input, target을 묶어서 이진화 하여 용량 감소 + 처리 속도 향상","metadata":{}},{"cell_type":"code","source":"n_fold_samples = [782, 783, 782, 782, 782]","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:52:20.292108Z","iopub.execute_input":"2023-01-02T20:52:20.292544Z","iopub.status.idle":"2023-01-02T20:52:20.302285Z","shell.execute_reply.started":"2023-01-02T20:52:20.292510Z","shell.execute_reply":"2023-01-02T20:52:20.301046Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"ids\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"mask\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        \"targets\": tf.io.FixedLenFeature([], tf.string)        \n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    ids = tf.io.parse_tensor(example['ids'], out_type=tf.int32)\n    mask = tf.io.parse_tensor(example['mask'], out_type=tf.int32)\n    targets = tf.io.parse_tensor(example['targets'], out_type = tf.float32)\n    \n    #out = tf.cast(example['output'], tf.\n    return (tf.reshape(ids, [512]), tf.reshape(mask, [512])), tf.reshape(targets, [6])\n\n\ndef load_dataset(filenames, labeled = True, ordered = True):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n    \n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n        \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) # automatically interleaves reads from multiple files  \n    dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n    return dataset\n\n\ndef load_outputwise_dataset(filenames, select_output = 0, labeled = True, ordered = True):\n    def read_labeled_tfrecord(example):\n        LABELED_TFREC_FORMAT = {\n            \"ids\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n            \"mask\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n            \"targets\": tf.io.FixedLenFeature([], tf.string)        \n        }\n        example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n        ids = tf.io.parse_tensor(example['ids'], out_type=tf.int32)\n        mask = tf.io.parse_tensor(example['mask'], out_type=tf.int32)\n        targets = tf.io.parse_tensor(example['targets'], out_type = tf.float32)\n    \n        #out = tf.cast(example['output'], tf.\n        return (tf.reshape(ids, [512]), tf.reshape(mask, [512])), tf.reshape(targets[select_output], [1])\n\n    \n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n    \n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = True\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n        \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) # automatically interleaves reads from multiple files  \n    dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n    return dataset\n\ndef get_training_dataset(dataset, batch_size = config['batch_size']):\n    #if do_aug: dataset = dataset.map(transform, num_parallel_calls=AUTO) # note we put AFTER batching    \n    dataset.shuffle(1024)\n    if(config['is_tpu']):\n        dataset = dataset.repeat()\n        dataset = dataset.batch(batch_size, drop_remainder = True)\n    else:\n        dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(dataset, batch_size = config['batch_size']):\n    #if do_aug: dataset = dataset.map(transform, num_parallel_calls=AUTO) # note we put AFTER batching\n    #!!!!!!!!!!Must be considered (duplication of score of elements repeated)    \n    #dataset.shuffle(1024)\n    if(config['is_tpu']):  \n        dataset = dataset.repeat()\n        dataset = dataset.batch(batch_size, drop_remainder = True)\n    else:\n        dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:52:20.303612Z","iopub.execute_input":"2023-01-02T20:52:20.304489Z","iopub.status.idle":"2023-01-02T20:52:20.323190Z","shell.execute_reply.started":"2023-01-02T20:52:20.304451Z","shell.execute_reply":"2023-01-02T20:52:20.322235Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# 6. Model<a class=\"anchor\" id=\"section6\"></a>\nDeberta-v3-base 모델의 끝부분에 Kaggle 대회의 target 형식인 6 X float 출력 크기 가진 Dense layer 추가\n1. N_REINIT_LAYERS: 모델 output 출력 부분에서 몇층의 layer weight을 초기화 할지\n2. N_HIDDEN_POOL: 출력 부분에서 몇 층의 layer를 묶어서 최종 추론에 사용 할지\n3. INIT_LR: deberta-v3-base 모델의 부분 초기 학습률\n4. LLRDR: deberta-v3-base 모델의 층 별 학습률 감소 정도\n5. DECAY_RATE: deberta-v3-base 모델의 학습률 decay rate\n6. HEAD_INIT_LR: 모델 output 출력 부분의 학습률\n7. LOSS: 사용 loss 종류\n8. FINAL_ACITVATION: 출력 부분에서의 활성함수 사용 종류","metadata":{}},{"cell_type":"markdown","source":"**Mean Pool** is very useful. I tend to add GlobalAveragePooling1D at the output of Bert Model and then connect a Dense layer. But just average embedding across the whole sequence length may be not a good idea because there are some paddings in many sequences. Simply speaking, mean pool is used to average only non-paddings embeddings in the sequence.\n\nIn my experiments, it improves LB score from 0.5 -> 0.48 only by adding this layer. About more information, you could refer to [this great notebook](https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently). \n\nMy implementation of mean pool is refer to [this notebook](https://www.kaggle.com/code/electro/fp3-roberta-meanpool-kfold-tensorflow).","metadata":{}},{"cell_type":"markdown","source":"**No dropout**. It turns out setting dropout = 0.0 in Bert Model can make validation score more smooth and yield much lower validation loss. (refer to [this great discussion](https://www.kaggle.com/competitions/commonlitreadabilityprize/discussion/260729))","metadata":{}},{"cell_type":"code","source":"model_config = AutoConfig.from_pretrained(config[\"model\"], output_hidden_states = True)\nmodel_config.attention_probs_dropout_prob = 0.0\nmodel_config.hidden_dropout_prob = 0.0\nmodel_config","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:52:20.326985Z","iopub.execute_input":"2023-01-02T20:52:20.327316Z","iopub.status.idle":"2023-01-02T20:52:20.351621Z","shell.execute_reply.started":"2023-01-02T20:52:20.327291Z","shell.execute_reply":"2023-01-02T20:52:20.350739Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DebertaV2Config {\n  \"_name_or_path\": \"/kaggle/input/debertav3base\",\n  \"attention_probs_dropout_prob\": 0.0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}"},"metadata":{}}]},{"cell_type":"code","source":"class MeanPool(tf.keras.layers.Layer):\n    def call(self, inputs, mask=None):\n        # inputs: (None, 512, 768)\n        \n        # (None, 512, 1)\n        broadcast_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n        \n        # (None, 768)\n        embedding_sum = tf.reduce_sum(inputs * broadcast_mask, axis=1)\n        \n        # (None, 1)\n        mask_sum = tf.reduce_sum(broadcast_mask, axis=1)\n        \n        mask_sum = tf.math.maximum(mask_sum, tf.constant([1e-9]))\n        return embedding_sum / mask_sum\n    \nclass WeightsSumOne(tf.keras.constraints.Constraint):\n    def __call__(self, w):\n        return tf.nn.softmax(w, axis=0)    ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:52:20.352983Z","iopub.execute_input":"2023-01-02T20:52:20.353385Z","iopub.status.idle":"2023-01-02T20:52:20.360734Z","shell.execute_reply.started":"2023-01-02T20:52:20.353340Z","shell.execute_reply":"2023-01-02T20:52:20.359607Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def build_model(params):\n    N_REINIT_LAYERS = params['n_reinit_layers']\n    N_HIDDEN_POOL    = params['n_hidden_pool']\n    INIT_LR          = params['init_lr']\n    LLRDR            = params['llrdr']\n    DECAY_RATE       = params['decay_rate']\n    HEAD_INIT_LR     = params['head_init_lr']\n    LOSS             = params['loss']\n    FINAL_ACTIVATION = params['final_activation']\n    # Multi inputs\n    if(config['is_tpu']):\n        input_ids = keras.Input(shape=(512,), batch_size=config['batch_size'], dtype= \"int32\", name=\"input_ids\")\n        attention_masks = keras.Input(shape=(512,), batch_size=config['batch_size'], dtype= \"int32\", name=\"attention_mask\")\n    else:\n        input_ids = keras.Input(shape=(512,), dtype= \"int32\", name=\"input_ids\")\n        attention_masks = keras.Input(shape=(512,),  dtype= \"int32\", name=\"attention_mask\")        \n        \n    base_model  = TFAutoModel.from_pretrained(config[\"model_path\"],                                                 \n                                                 config = model_config)\n\n\n\n    REINIT_LAYERS = 1\n    normal_initializer = tf.keras.initializers.GlorotUniform( seed= config['seed'])\n    zeros_initializer = tf.keras.initializers.Zeros()\n    ones_initializer = tf.keras.initializers.Ones()\n\n    if(config['is_base_model_fixed']): \n        #Fix layers' parameters except output hidden and last encoder layer\n        for encoder_block in base_model.deberta.encoder.layer[:-REINIT_LAYERS]:\n            for layer in encoder_block.submodules:\n                layer.trainable = False\n\n        base_model.deberta.embeddings.trainable = False\n\n\n    for encoder_block in base_model.deberta.encoder.layer[-N_REINIT_LAYERS:]:\n    #for encoder_block in model.layers[8]:\n        for layer in encoder_block.submodules:\n            if isinstance(layer, tf.keras.layers.Dense):\n                layer.kernel.assign(normal_initializer(shape=layer.kernel.shape,\n                                                       dtype=layer.kernel.dtype))\n                if layer.bias is not None:\n                    layer.bias.assign(zeros_initializer(shape=layer.bias.shape,\n                                                        dtype=layer.bias.dtype))\n\n            elif isinstance(layer, tf.keras.layers.LayerNormalization):\n                layer.beta.assign(zeros_initializer(shape=layer.beta.shape,\n                                                    dtype=layer.beta.dtype))\n                layer.gamma.assign(ones_initializer(shape=layer.gamma.shape,\n                                                    dtype=layer.gamma.dtype))\n\n    base_model_output = base_model(input_ids, attention_mask=attention_masks)\n    hidden_states = base_model_output.hidden_states # (None, 512, 768) 여러개\n\n    # WeightedLayerPool + MeanPool of the last 4 hidden states\n    stack_meanpool = tf.stack([MeanPool()(hidden_s, mask=attention_masks)\n                               for hidden_s in hidden_states[-N_HIDDEN_POOL:]],\n                              axis=2) # (None, 768, 4)\n\n    weighted_layer_pool = layers.Dense(1, use_bias=False,\n                                      kernel_constraint=WeightsSumOne())(stack_meanpool)\n\n    weighted_layer_pool = tf.squeeze(weighted_layer_pool, axis=-1)\n\n    x = layers.Dense(6, activation=FINAL_ACTIVATION)(weighted_layer_pool)\n    output = layers.Rescaling(scale=N_HIDDEN_POOL, offset=1.0)(x)\n    model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output)\n\n    # Compile model with Layer-wise Learning Rate Decay\n    layer_list = [base_model.deberta.embeddings] + list(base_model.deberta.encoder.layer)\n    layer_list.reverse()\n\n    LR_SCH_DECAY_STEPS = 3128 //config['batch_size'] # 2 * len(train_df) // BATCH_SIZE\n    lr_schedules = [tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=INIT_LR * LLRDR ** i,\n        decay_steps=LR_SCH_DECAY_STEPS,\n        decay_rate=DECAY_RATE) for i in range(len(layer_list))]\n\n    lr_schedule_head = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=HEAD_INIT_LR,\n        decay_steps=LR_SCH_DECAY_STEPS,\n        decay_rate=DECAY_RATE\n    )\n\n    optimizers = [tf.keras.optimizers.Adam(learning_rate=lr_sch) for lr_sch in lr_schedules]\n\n    optimizers_and_layers = [(tf.keras.optimizers.Adam(learning_rate=lr_schedule_head),\n                              model.layers[-4:])] + list(zip(optimizers, layer_list))\n\n    optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n\n\n    model.compile(optimizer=optimizer,\n                  loss=LOSS,\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()])    \n\n    return model","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:52:20.362675Z","iopub.execute_input":"2023-01-02T20:52:20.363066Z","iopub.status.idle":"2023-01-02T20:52:20.384626Z","shell.execute_reply.started":"2023-01-02T20:52:20.363028Z","shell.execute_reply":"2023-01-02T20:52:20.383618Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 7. Function to train and get accuracy<a class=\"anchor\" id=\"section7\"></a>","metadata":{}},{"cell_type":"code","source":"def get_accuracy(params):\n    score_list=[]\n    for fold in range(0, config['n_fold']):\n        if(config['fold_select'] is not None):\n            if(fold != config['fold_select']):\n                continue\n\n        valid_folds = [fold]\n        train_folds = np.arange(0, config['n_fold'])\n        train_folds = np.delete(train_folds, fold)\n\n        steps_per_validation = (n_fold_samples[fold])//config['batch_size']+1\n        steps_per_epoch     = (np.sum(n_fold_samples) - steps_per_validation)//config['batch_size']+1\n\n        if(config['is_debug']):\n            steps_per_validation = 1\n            steps_per_epoch      = 1\n\n\n        train_dataset = load_dataset(tf.io.gfile.glob([config['gcs_path'] + config['data_path'] +\n                                                           '%d.tfrec'%x for x in train_folds]))\n\n        valid_dataset = load_dataset(tf.io.gfile.glob([config['gcs_path'] + config['data_path'] +\n                                                           '%d.tfrec'%x for x in valid_folds]))\n\n\n        train_dataset = get_training_dataset(train_dataset, config['batch_size'])\n        valid_dataset = get_validation_dataset(valid_dataset, config['batch_size'])\n\n        targets = []\n        \n        for example in valid_dataset:\n            targets.append(example[1].numpy())\n        targets=  np.vstack(targets)    \n\n        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(f\"best_model_fold{fold}.h5\", \n                                                                       monitor=\"val_root_mean_squared_error\",\n                                                         mode=\"min\", save_best_only=True,\n                                                         verbose=1, save_weights_only=True),\n\n        earlystop_callback = keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error',\n                                                           patience=3,\n                                                           verbose=1)\n\n        reducelr_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                              factor=0.5,\n                                                              patience=5,\n                                                              verbose=1)\n\n        callbacks = [\n            earlystop_callback,\n        ]\n        \n        if(config['save_model']):\n            callbacks.append(model_checkpoint_callback)\n\n        if(not config['is_local']):\n            with strategy.scope():\n                model = build_model(params)\n        else:\n            model = build_model(params)\n\n        if(config['is_tpu'] or config['is_debug']):\n            freeze_history = model.fit(train_dataset, \n                                   validation_data  = valid_dataset, \n                                   steps_per_epoch  = steps_per_epoch,\n                                   validation_steps = steps_per_validation,\n                                   callbacks        = callbacks, \n                                   shuffle          = True,\n                                   epochs           = config['epochs'],\n                                   verbose          = 1\n                                   )\n        else:\n            freeze_history = model.fit(train_dataset, \n                                   validation_data  = valid_dataset, \n                                   callbacks        = callbacks, \n                                   shuffle          = True,\n                                   epochs           = config['epochs'],\n                                   batch_size       = config['batch_size'],\n                                   verbose          = 1\n                                   )        \n        \n        if(not config['is_debug']):\n            pred = model.predict(valid_dataset)\n        else:\n            pred = np.ones(np.shape(targets))\n        error =pred - targets\n        error **=2\n        rmse_error = np.zeros(6)\n        \n        for i in range(0, 6):\n            rmse_error[i] = np.sqrt(np.average(error[:,i]))    \n\n        def merge_two_dicts(x, y):\n            z = x.copy()   # start with keys and values of x\n            z.update(y)    # modifies z with keys and values of y\n            return z\n        run_parameter_info = merge_two_dicts(config, params)\n\n        \n        #WandB Log 전송\n        if(config['is_wandb']):\n            wandb.init(project = config['project'], name = f\"{config['name']}_{fold}\",\n               group = config['group'], config = run_parameter_info)\n            wandb.log({\"Best val rmse\": np.min(freeze_history.history['val_root_mean_squared_error'])})\n            wandb.log({\n                         'cohesion'    : rmse_error[0],\n                         'syntax'      : rmse_error[1],\n                         'vocabulary'  : rmse_error[2],\n                         'phraseology' : rmse_error[3],\n                         'grammar'     : rmse_error[4],\n                         'conventions' : rmse_error[5]\n                      })\n            for i in range(0, len(freeze_history.history['loss'])):\n                wandb.log({\n                            'loss': freeze_history.history['loss'][i], \n                            'root_mean_squared_error': freeze_history.history['root_mean_squared_error'][i],\n                            'val_loss': freeze_history.history['val_loss'][i],\n                            'val_root_mean_squared_error': freeze_history.history['val_root_mean_squared_error'][i],\n                          })\n\n            wandb.finish()\n        score_list.append(np.min(freeze_history.history['val_root_mean_squared_error']))\n        \n        tf.keras.backend.clear_session()\n        del model, train_dataset, valid_dataset, pred, error, targets, rmse_error, callbacks, freeze_history\n        gc.collect() \n    return np.average(score_list)\n\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:52:20.386199Z","iopub.execute_input":"2023-01-02T20:52:20.386597Z","iopub.status.idle":"2023-01-02T20:52:20.409772Z","shell.execute_reply.started":"2023-01-02T20:52:20.386565Z","shell.execute_reply":"2023-01-02T20:52:20.408953Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# 8. Hyperparameter tuning with Optuna library<a class=\"anchor\" id=\"section8\"></a>","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params={\n        'n_reinit_layers': trial.suggest_int('n_reinit_layers', 0, 12, 1),\n        'n_hidden_pool': trial.suggest_int('n_hidden_pool', 1, 12, 1),\n        'init_lr': trial.suggest_float('init_lr', 1e-7, 1e-3),\n        'llrdr'  : trial.suggest_float('llrdr', 0.01, 0.99),\n        'decay_rate': trial.suggest_float('decay_rate', 0.01, 0.99),\n        'head_init_lr': trial.suggest_float('head_init_lr', 1e-7, 1e-3),\n        'loss': trial.suggest_categorical('loss', ['categorical_crossentropy', 'categorical_hinge', \n                                                    'mean_absolute_error', 'mean_squared_error' , 'huber_loss']),        \n        \"final_activation\": trial.suggest_categorical(\"final_activation\",[\"relu\", \"sigmoid\", \"softmax\", \"softplus\", \n                                                                           \"softsign\", \"tanh\", \"selu\", \"elu\", \"exponential\", \"linear\"])\n    }   \n    \n    accuracy = get_accuracy(params)\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:52:20.411232Z","iopub.execute_input":"2023-01-02T20:52:20.411620Z","iopub.status.idle":"2023-01-02T20:52:20.424758Z","shell.execute_reply.started":"2023-01-02T20:52:20.411587Z","shell.execute_reply":"2023-01-02T20:52:20.423918Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"init_params={\n            'n_reinit_layers' : 1,\n            'n_hidden_pool'   : 4,\n            'init_lr'         : 1e-5,\n            'llrdr'           : 0.9,\n            'decay_rate'      : 0.3,\n            'head_init_lr'    : 1e-4,\n            'loss'            : 'mean_squared_error',\n            'final_activation': 'sigmoid'\n}","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:52:20.427686Z","iopub.execute_input":"2023-01-02T20:52:20.427932Z","iopub.status.idle":"2023-01-02T20:52:20.439057Z","shell.execute_reply.started":"2023-01-02T20:52:20.427909Z","shell.execute_reply":"2023-01-02T20:52:20.438129Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction=\"minimize\")","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:52:20.440451Z","iopub.execute_input":"2023-01-02T20:52:20.440830Z","iopub.status.idle":"2023-01-02T20:52:20.453662Z","shell.execute_reply.started":"2023-01-02T20:52:20.440797Z","shell.execute_reply":"2023-01-02T20:52:20.452733Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2023-01-02 20:52:20,448]\u001b[0m A new study created in memory with name: no-name-96d3cf9b-0ded-4d1b-94d4-ba0696614854\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"arr_init_params=[]\nfor i in range(0, 3):\n    cpy_init_params= init_params.copy()\n    cpy_init_params['n_reinit_layers'] = i\n    arr_init_params.append(cpy_init_params)\n    \n\nfor i in range(3, 6):\n    cpy_init_params = init_params.copy()\n    cpy_init_params['n_hidden_pool']= i\n    arr_init_params.append(cpy_init_params)\n\nlr_list = np.geomspace(1e-5, 1e-3, 3)\nfor i in range(0, 3):\n    cpy_init_params = init_params.copy()\n    cpy_init_params['init_lr'] = lr_list[i]\n    arr_init_params.append(cpy_init_params)\n    \n    cpy_init_params = init_params.copy()\n    cpy_init_params['head_init_lr'] = lr_list[i]\n    arr_init_params.append(cpy_init_params)\n    \n\nfor i in range(7, 10):\n    cpy_init_params = init_params.copy()\n    cpy_init_params['llrdr'] = i*0.1\n    arr_init_params.append(cpy_init_params)\n\nfor i in range(2, 5):\n    cpy_init_params = init_params.copy()\n    cpy_init_params['decay_rate'] = i*0.1\n    arr_init_params.append(cpy_init_params)\n        \nlosses = ['categorical_crossentropy', 'categorical_hinge', \n         'mean_absolute_error', 'mean_squared_error' , 'huber_loss']\n\nfinal_activations = [\"relu\", \"sigmoid\", \"softmax\", \"softplus\", \n                    \"softsign\", \"tanh\", \"selu\", \"elu\", \"exponential\", \"linear\"]\nfor loss in losses:\n    cpy_init_params = init_params.copy()\n    cpy_init_params['loss'] = loss\n    arr_init_params.append(cpy_init_params)\n    \nfor final_activation in final_activations:\n    cpy_init_params = init_params.copy()\n    cpy_init_params['final_activation'] = final_activation\n    arr_init_params.append(cpy_init_params) ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:52:20.455018Z","iopub.execute_input":"2023-01-02T20:52:20.455359Z","iopub.status.idle":"2023-01-02T20:52:20.467037Z","shell.execute_reply.started":"2023-01-02T20:52:20.455316Z","shell.execute_reply":"2023-01-02T20:52:20.465680Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"len(arr_init_params)","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:52:20.468834Z","iopub.execute_input":"2023-01-02T20:52:20.469487Z","iopub.status.idle":"2023-01-02T20:52:20.483314Z","shell.execute_reply.started":"2023-01-02T20:52:20.469449Z","shell.execute_reply":"2023-01-02T20:52:20.482357Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"33"},"metadata":{}}]},{"cell_type":"code","source":"#for init_params in arr_init_params:\n#    study.enqueue_trial(init_params)\nstudy.enqueue_trial(init_params)\n\nstudy.optimize(objective, n_trials=300, show_progress_bar=True, gc_after_trial = True)\n\nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial:\")","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2023-01-02T20:52:20.484768Z","iopub.execute_input":"2023-01-02T20:52:20.485240Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/optuna/progress_bar.py:49: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n  self._init_valid()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b493920617a4bc4b027f2ece68b38f3"}},"metadata":{}},{"name":"stderr","text":"2023-01-02 20:52:20.659857: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Failed precondition: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n2023-01-02 20:52:21.492287: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\nAll model checkpoint layers were used when initializing TFDebertaV2Model.\n\nAll the layers of TFDebertaV2Model were initialized from the model checkpoint at /kaggle/input/debertav3base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaV2Model for predictions without further training.\n2023-01-02 20:52:31.291993: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n     28/Unknown - 107s 429ms/step - loss: 0.5150 - root_mean_squared_error: 0.7177","output_type":"stream"}]},{"cell_type":"code","source":"try:\n    import gc    \n    tf.keras.backend.clear_session()  \n    del model\n    gc.collect()\nexcept:\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}