{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T02:55:55.625369Z","iopub.status.busy":"2024-01-18T02:55:55.624984Z","iopub.status.idle":"2024-01-18T02:55:55.63366Z","shell.execute_reply":"2024-01-18T02:55:55.63157Z","shell.execute_reply.started":"2024-01-18T02:55:55.625341Z"},"trusted":true},"outputs":[],"source":["import warnings\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T03:14:05.452505Z","iopub.status.busy":"2024-01-18T03:14:05.452028Z","iopub.status.idle":"2024-01-18T03:14:36.005589Z","shell.execute_reply":"2024-01-18T03:14:36.00434Z","shell.execute_reply.started":"2024-01-18T03:14:05.452464Z"},"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n","from sklearn.metrics import roc_auc_score\n","from catboost import CatBoostClassifier\n","from lightgbm import LGBMClassifier\n","\n","\n","from transformers import PreTrainedTokenizerFast\n","from tokenizers import (\n","    decoders,\n","    models,\n","    normalizers,\n","    pre_tokenizers,\n","    processors,\n","    trainers,\n","    Tokenizer,\n",")\n","\n","\n","from datasets import Dataset\n","from tqdm.auto import tqdm\n","import pandas as pd\n","import numpy as np\n","import sys\n","import gc\n","\n","import nltk\n","import spacy\n","import string\n","import pickle\n","\n","import re\n","from collections import Counter\n","import os\n","sys.path.append(\"../lib\")\n","from leven_search import LevenSearch, EditCost, EditCostConfig, GranularEditCostConfig\n","if os.path.isdir(\"/kaggle/input\"):\n","    IS_KAGGLE = True\n","    !pip install /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\n","\n","    !mkdir /tmp/corpora\n","    !cp -r /kaggle/input/wordnet/wordnet /tmp/corpora\n","    !pip install /kaggle/input/swifter-1-4-0/wheelhouse/swifter-1.4.0-py3-none-any.whl  \n","    nltk.data.path.append(\"/tmp\")  \n","    with open('/kaggle/usr/lib/install_levenshtein_search_library/leven_search.pkl', 'rb') as file:\n","        lev_search = pickle.load(file)    \n","else:\n","    IS_KAGGLE = False\n","    with open(\"../lib/leven_search.pkl\", \"rb\") as file:\n","        lev_search = pickle.load(file)\n","\n","\n","\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","\n","from spellchecker import SpellChecker\n","\n","\n","import swifter\n","\n","from joblib import Parallel, delayed"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fast save\n","IS_RERUN = False\n","if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n","    IS_RERUN = True\n","    pass\n","else:\n","    try:\n","        sub = pd.read_csv(\n","            \"/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\"\n","        )\n","        sub.to_csv(\"submission.csv\", index=False)\n","    except:\n","        sub = pd.read_csv(\"../input/llm-detect-ai-generated-text/sample_submission.csv\")\n","        sub.to_csv(\"submission.csv\", index=False)\n","if (not IS_RERUN) and (IS_KAGGLE):\n","    sys.exit()"]},{"cell_type":"markdown","metadata":{},"source":["# Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T04:27:34.646307Z","iopub.status.busy":"2024-01-18T04:27:34.645932Z","iopub.status.idle":"2024-01-18T04:27:34.65341Z","shell.execute_reply":"2024-01-18T04:27:34.6515Z","shell.execute_reply.started":"2024-01-18T04:27:34.646279Z"},"trusted":true},"outputs":[],"source":["LOWER_CASE = True\n","REMOVE_PUNCT = True\n","REMOVE_STOP_WORDS = True\n","REMOVE_FREQ_WORDS = False\n","REMOVE_RARE_WORDS = False\n","STEM_WORDS = True\n","LEMMATIZE = True\n","REMOVE_EMOJI = False\n","CONVERT_EMOJI = True\n","REMOVE_URLS = True\n","REMOVE_HTML = True\n","CHATWORD_CONV = True\n","SPELL_CORRECT = True\n","\n","N_PROCESSES = 4"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T04:27:34.809411Z","iopub.status.busy":"2024-01-18T04:27:34.809002Z","iopub.status.idle":"2024-01-18T04:27:35.892159Z","shell.execute_reply":"2024-01-18T04:27:35.89072Z","shell.execute_reply.started":"2024-01-18T04:27:34.809382Z"},"trusted":true},"outputs":[],"source":["if IS_KAGGLE:\n","    DATA_PATH = \"/kaggle/input\"\n","    train_path = (\n","        f\"{DATA_PATH}/llm-daigt-5fold-split-seed7-train/train_v2_drcat_02_refined.csv\"\n","    )\n","else:\n","    DATA_PATH = \"../input\"\n","    train_path = f\"{DATA_PATH}/daigt-v2-train-dataset/train_v2_drcat_02.csv\"\n","test = pd.read_csv(f\"{DATA_PATH}/llm-detect-ai-generated-text/test_essays.csv\")\n","sub = pd.read_csv(f\"{DATA_PATH}/llm-detect-ai-generated-text/sample_submission.csv\")\n","train = pd.read_csv(train_path, sep=\",\")\n","train = train.drop_duplicates(subset=[\"text\"])\n","train.reset_index(drop=True, inplace=True)\n","train.head()\n","excluded_prompt_name_list = [\n","    \"Distance learning\",\n","    \"Grades for extracurricular activities\",\n","    \"Summer projects\",\n","]\n","train = train[~(train[\"prompt_name\"].isin(excluded_prompt_name_list))]\n","train = train.drop_duplicates(subset=[\"text\"])\n","train.reset_index(drop=True, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Text preprocessing\n","Ref: https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-18T04:27:35.895079Z","iopub.status.busy":"2024-01-18T04:27:35.894702Z","iopub.status.idle":"2024-01-18T04:27:35.901257Z","shell.execute_reply":"2024-01-18T04:27:35.900001Z","shell.execute_reply.started":"2024-01-18T04:27:35.895052Z"},"trusted":true},"outputs":[],"source":["chat_words_str = \"\"\"\n","AFAIK=As Far As I Know\n","AFK=Away From Keyboard\n","ASAP=As Soon As Possible\n","ATK=At The Keyboard\n","ATM=At The Moment\n","A3=Anytime, Anywhere, Anyplace\n","BAK=Back At Keyboard\n","BBL=Be Back Later\n","BBS=Be Back Soon\n","BFN=Bye For Now\n","B4N=Bye For Now\n","BRB=Be Right Back\n","BRT=Be Right There\n","BTW=By The Way\n","B4=Before\n","B4N=Bye For Now\n","CU=See You\n","CUL8R=See You Later\n","CYA=See You\n","FAQ=Frequently Asked Questions\n","FC=Fingers Crossed\n","FWIW=For What It's Worth\n","FYI=For Your Information\n","GAL=Get A Life\n","GG=Good Game\n","GN=Good Night\n","GMTA=Great Minds Think Alike\n","GR8=Great!\n","G9=Genius\n","IC=I See\n","ICQ=I Seek you (also a chat program)\n","ILU=ILU: I Love You\n","IMHO=In My Honest/Humble Opinion\n","IMO=In My Opinion\n","IOW=In Other Words\n","IRL=In Real Life\n","KISS=Keep It Simple, Stupid\n","LDR=Long Distance Relationship\n","LMAO=Laugh My A.. Off\n","LOL=Laughing Out Loud\n","LTNS=Long Time No See\n","L8R=Later\n","MTE=My Thoughts Exactly\n","M8=Mate\n","NRN=No Reply Necessary\n","OIC=Oh I See\n","PITA=Pain In The A..\n","PRT=Party\n","PRW=Parents Are Watching\n","ROFL=Rolling On The Floor Laughing\n","ROFLOL=Rolling On The Floor Laughing Out Loud\n","ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n","SK8=Skate\n","STATS=Your sex and age\n","ASL=Age, Sex, Location\n","THX=Thank You\n","TTFN=Ta-Ta For Now!\n","TTYL=Talk To You Later\n","U=You\n","U2=You Too\n","U4E=Yours For Ever\n","WB=Welcome Back\n","WTF=What The F...\n","WTG=Way To Go!\n","WUF=Where Are You From?\n","W8=Wait...\n","7K=Sick:-D Laugher\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T04:49:16.481802Z","iopub.status.busy":"2024-01-18T04:49:16.481391Z","iopub.status.idle":"2024-01-18T04:49:16.565327Z","shell.execute_reply":"2024-01-18T04:49:16.564434Z","shell.execute_reply.started":"2024-01-18T04:49:16.48177Z"},"trusted":true},"outputs":[],"source":["PUNCT_TO_REMOVE = string.punctuation\n","\n","\n","def remove_punctuation(text):\n","    \"\"\"custom function to remove the punctuation\"\"\"\n","    return text.translate(str.maketrans(\"\", \"\", PUNCT_TO_REMOVE))\n","\n","\n","\", \".join(stopwords.words(\"english\"))\n","\n","STOPWORDS = set(stopwords.words(\"english\"))\n","\n","\n","def remove_stopwords(text):\n","    \"\"\"custom function to remove the st\n","    opwords\"\"\"\n","    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n","\n","\n","stemmer = PorterStemmer()\n","\n","\n","def stem_words(text):\n","    return \" \".join([stemmer.stem(word) for word in text.split()])\n","\n","\n","lemmatizer = WordNetLemmatizer()\n","wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n","\n","\n","def lemmatize_words(text):\n","    pos_tagged_text = nltk.pos_tag(text.split())\n","    return \" \".join(\n","        [\n","            lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN))\n","            for word, pos in pos_tagged_text\n","        ]\n","    )\n","\n","\n","# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n","def remove_emoji(string):\n","    emoji_pattern = re.compile(\n","        \"[\"\n","        \"\\U0001F600-\\U0001F64F\"  # emoticons\n","        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        \"\\U00002702-\\U000027B0\"\n","        \"\\U000024C2-\\U0001F251\"\n","        \"]+\",\n","        flags=re.UNICODE,\n","    )\n","    return emoji_pattern.sub(r\"\", string)\n","\n","\n","def remove_urls(text):\n","    url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n","    return url_pattern.sub(r\"\", text)\n","\n","\n","def remove_html(text):\n","    html_pattern = re.compile(\"<.*?>\")\n","    return html_pattern.sub(r\"\", text)\n","\n","\n","chat_words_map_dict = {}\n","chat_words_list = []\n","for line in chat_words_str.split(\"\\n\"):\n","    if line != \"\":\n","        cw = line.split(\"=\")[0]\n","        cw_expanded = line.split(\"=\")[1]\n","        chat_words_list.append(cw)\n","        chat_words_map_dict[cw] = cw_expanded\n","chat_words_list = set(chat_words_list)\n","\n","\n","def chat_words_conversion(text):\n","    new_text = []\n","    for w in text.split():\n","        if w.upper() in chat_words_list:\n","            new_text.append(chat_words_map_dict[w.upper()])\n","        else:\n","            new_text.append(w)\n","    return \" \".join(new_text)\n","\n","\n","spell = SpellChecker()\n","\n","\n","def correct_spellings(text):\n","    corrected_text = []\n","    misspelled_words = spell.unknown(text.split())\n","    for word in text.split():\n","        if word in misspelled_words:\n","            corrected_text.append(spell.correction(word))\n","        else:\n","            corrected_text.append(word)\n","    return \" \".join(filter(None, corrected_text))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def fix_text(origin_text):\n","    update_stats = {}\n","    split_texts = origin_text.split()\n","\n","    fixed_split_texts = []\n","    for text in split_texts:\n","        dist_result = lev_search.find_dist(text, max_distance=1).words\n","        # print(dist_result)\n","        if len(dist_result) == 1:\n","            fixed_split_texts.append(dist_result[list(dist_result.keys())[0]].word)\n","        else:\n","            fixed_split_texts.append(text)\n","        right_word = False\n","        for k in dist_result:\n","            cur_mod = str(dist_result[k].updates)\n","            if cur_mod == \"[]\":  # if right, pass\n","                right_word = True\n","                break\n","        if right_word:\n","            continue\n","        for k in dist_result:\n","            cur_mod = str(dist_result[k].updates)\n","            if cur_mod not in update_stats.keys():\n","                update_stats[cur_mod] = 1\n","            else:\n","                update_stats[cur_mod] += 1\n","\n","    max_freq = -1\n","    for k, v in update_stats.items():\n","        if v > max_freq:\n","            max_freq = v\n","            max_conv = k\n","    # print(max_conv[1:-1])\n","\n","    if max_freq >= int(0.06 * len(split_texts)):\n","        max_freq_change = str(max_conv[1:-1]).split()\n","\n","        gec = GranularEditCostConfig(\n","            default_cost=10,\n","            edit_costs=[EditCost(max_freq_change[0], max_freq_change[-1], 1)],\n","        )\n","\n","        reupdate_stats = {}\n","        refixed_split_texts = []\n","        for text in fixed_split_texts:\n","            dist_result = lev_search.find_dist(\n","                text, max_distance=9, edit_cost_config=gec\n","            ).words\n","            if len(dist_result) > 0:  # make sense is better than not make sense things\n","                refixed_split_texts.append(\n","                    dist_result[list(dist_result.keys())[0]].word\n","                )\n","            # else:\n","            #    refixed_split_texts.append(text)\n","            right_word = False\n","            for k in dist_result:\n","                cur_mod = str(dist_result[k].updates)\n","                if cur_mod == \"[]\":  # if right, pass\n","                    right_word = True\n","                    break\n","            if right_word:\n","                continue\n","            for k in dist_result:\n","                cur_mod = str(dist_result[k].updates)\n","                if cur_mod not in reupdate_stats.keys():\n","                    reupdate_stats[cur_mod] = 1\n","                else:\n","                    reupdate_stats[cur_mod] += 1\n","        fixed_split_texts = refixed_split_texts\n","    return \" \".join(fixed_split_texts)\n","\n","\n","# origin_text = \"Ai one poini I believed ihe elecioral college was a bad idea\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def correct_spellings(train, n_processes=N_PROCESSES):\n","    fixed_texts = []\n","    rng_texts = np.linspace(0, len(train), n_processes + 1, dtype=int)\n","\n","    def process_wrapper(process_ind, rng_texts=rng_texts, train=train):\n","        fixed_texts = []\n","        for i in tqdm(range(rng_texts[process_ind], rng_texts[process_ind + 1])):\n","            fixed_texts.append(fix_text(train.iloc[i].text))\n","        return fixed_texts\n","\n","    fixed_textss = Parallel(n_jobs=n_processes)(\n","        delayed(process_wrapper)(i) for i in range(n_processes)\n","    )\n","    for i in range(1, len(fixed_textss)):\n","        fixed_textss[0] += fixed_textss[i]\n","    del fixed_textss[1:]\n","    gc.collect()\n","    return fixed_textss[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T04:27:35.987274Z","iopub.status.busy":"2024-01-18T04:27:35.98692Z","iopub.status.idle":"2024-01-18T04:38:44.19892Z","shell.execute_reply":"2024-01-18T04:38:44.197077Z","shell.execute_reply.started":"2024-01-18T04:27:35.987248Z"},"trusted":true},"outputs":[],"source":["def preprocess_text_df(df):\n","    if LOWER_CASE:\n","        df[\"text\"] = df[\"text\"].str.lower()\n","    if REMOVE_PUNCT:\n","        df[\"text\"] = df[\"text\"].swifter.apply(lambda text: remove_punctuation(text))\n","    if REMOVE_STOP_WORDS:\n","        df[\"text\"] = df[\"text\"].swifter.apply(lambda text: remove_stopwords(text))\n","\n","    if SPELL_CORRECT:\n","        df[\"text\"] = correct_spellings(df)\n","    cnt = Counter()\n","    for text in df[\"text\"].values:\n","        for word in text.split():\n","            cnt[word] += 1\n","\n","    FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n","\n","    def remove_freqwords(text):\n","        \"\"\"custom function to remove the frequent words\"\"\"\n","        return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n","\n","    if REMOVE_FREQ_WORDS:\n","        df[\"text\"] = df[\"text\"].swifter.apply(lambda text: remove_freqwords(text))\n","\n","    n_rare_words = 10\n","    RAREWORDS = set([w for (w, wc) in cnt.most_common()[: -n_rare_words - 1 : -1]])\n","\n","    def remove_rarewords(text):\n","        \"\"\"custom function to remove the rare words\"\"\"\n","        return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n","\n","    if REMOVE_RARE_WORDS:\n","        df[\"text\"] = df[\"text\"].swifter.apply(lambda text: remove_rarewords(text))\n","    if STEM_WORDS:\n","        df[\"text\"] = df[\"text\"].swifter.apply(lambda text: stem_words(text))\n","    if LEMMATIZE:\n","        df[\"text\"] = df[\"text\"].swifter.apply(lambda text: lemmatize_words(text))\n","    if REMOVE_URLS:\n","        df[\"text\"] = df[\"text\"].swifter.apply(lambda text: remove_urls(text))\n","    if REMOVE_HTML:\n","        df[\"text\"] = df[\"text\"].swifter.apply(lambda text: remove_urls(text))\n","    if CHATWORD_CONV:\n","        df[\"text\"] = df[\"text\"].swifter.apply(lambda text: chat_words_conversion(text))\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not IS_KAGGLE:\n","    train = preprocess_text_df(train)\n","    train.to_csv(\n","        f\"{DATA_PATH}/daigt-v2-train-dataset/train_v2_drcat_02_refined.csv\", index=False\n","    )\n","test = preprocess_text_df(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T15:42:08.795072Z","iopub.status.busy":"2024-01-15T15:42:08.794502Z","iopub.status.idle":"2024-01-15T15:42:08.801789Z","shell.execute_reply":"2024-01-15T15:42:08.800892Z","shell.execute_reply.started":"2024-01-15T15:42:08.795041Z"},"trusted":true},"outputs":[],"source":["LOWERCASE = False\n","VOCAB_SIZE = 14_000_000"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T15:42:08.803475Z","iopub.status.busy":"2024-01-15T15:42:08.803121Z","iopub.status.idle":"2024-01-15T15:43:38.585741Z","shell.execute_reply":"2024-01-15T15:43:38.584786Z","shell.execute_reply.started":"2024-01-15T15:42:08.803445Z"},"trusted":true},"outputs":[],"source":["# Creating Byte-Pair Encoding tokenizer\n","raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n","\n","\n","# Adding normalization and pre_tokenizer\n","raw_tokenizer.normalizer = normalizers.Sequence(\n","    [normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else []\n",")\n","\n","\n","raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n","\n","# Adding special tokens and creating trainer instance\n","special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n","trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n","\n","\n","# Creating huggingface dataset object\n","dataset = Dataset.from_pandas(test[[\"text\"]])\n","\n","\n","def train_corp_iter():\n","    \"\"\"\n","    A generator function for iterating over a dataset in chunks.\n","    \"\"\"\n","    for i in range(0, len(dataset), 1000):\n","        yield dataset[i : i + 1000][\"text\"]\n","\n","\n","# Training from iterator REMEMBER it's training on test set...\n","raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n","\n","tokenizer = PreTrainedTokenizerFast(\n","    tokenizer_object=raw_tokenizer,\n","    unk_token=\"[UNK]\",\n","    pad_token=\"[PAD]\",\n","    cls_token=\"[CLS]\",\n","    sep_token=\"[SEP]\",\n","    mask_token=\"[MASK]\",\n",")\n","\n","\n","# Tokenize test set with new tokenizer\n","tokenized_texts_test = []\n","for text in tqdm(test[\"text\"].tolist()):\n","    tokenized_texts_test.append(tokenizer.tokenize(text))\n","\n","\n","# Tokenize train set\n","tokenized_texts_train = []\n","for text in tqdm(train[\"text\"].tolist()):\n","    tokenized_texts_train.append(tokenizer.tokenize(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T15:43:38.587621Z","iopub.status.busy":"2024-01-15T15:43:38.587213Z","iopub.status.idle":"2024-01-15T15:43:38.593556Z","shell.execute_reply":"2024-01-15T15:43:38.592462Z","shell.execute_reply.started":"2024-01-15T15:43:38.587586Z"},"trusted":true},"outputs":[],"source":["print(tokenized_texts_test[1])\n","print()\n","print(tokenized_texts_test[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T15:43:38.595056Z","iopub.status.busy":"2024-01-15T15:43:38.594777Z","iopub.status.idle":"2024-01-15T15:46:46.458784Z","shell.execute_reply":"2024-01-15T15:46:46.457778Z","shell.execute_reply.started":"2024-01-15T15:43:38.595032Z"},"trusted":true},"outputs":[],"source":["def dummy(text):\n","    \"\"\"\n","    A dummy function to use as tokenizer for TfidfVectorizer.\n","    It returns the text as it is since we already tokenized it.\n","    \"\"\"\n","    return text\n","\n","\n","# Fitting TfidfVectoizer on test set\n","vectorizer = TfidfVectorizer(\n","    ngram_range=(3, 5),\n","    lowercase=False,\n","    sublinear_tf=True,\n","    analyzer=\"word\",\n","    tokenizer=dummy,\n","    preprocessor=dummy,\n","    token_pattern=None,\n","    strip_accents=\"unicode\",\n",")\n","\n","\n","vectorizer.fit(tokenized_texts_test)\n","\n","# Getting vocab\n","vocab = vectorizer.vocabulary_\n","print(vocab)\n","\n","\n","# Here we fit our vectorizer on train set but this time we use vocabulary from test fit.\n","vectorizer = TfidfVectorizer(\n","    ngram_range=(3, 5),\n","    lowercase=False,\n","    sublinear_tf=True,\n","    vocabulary=vocab,\n","    analyzer=\"word\",\n","    tokenizer=dummy,\n","    preprocessor=dummy,\n","    token_pattern=None,\n","    strip_accents=\"unicode\",\n",")\n","\n","tf_train = vectorizer.fit_transform(tokenized_texts_train)\n","tf_test = vectorizer.transform(tokenized_texts_test)\n","del vectorizer\n","gc.collect()\n","\n","print(tf_train.shape)\n","print(tf_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T15:46:46.460716Z","iopub.status.busy":"2024-01-15T15:46:46.460424Z","iopub.status.idle":"2024-01-15T15:46:46.465067Z","shell.execute_reply":"2024-01-15T15:46:46.464148Z","shell.execute_reply.started":"2024-01-15T15:46:46.460691Z"},"trusted":true},"outputs":[],"source":["y_train_label = train[\"label\"].values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T15:46:46.490464Z","iopub.status.busy":"2024-01-15T15:46:46.490203Z","iopub.status.idle":"2024-01-15T15:46:46.499189Z","shell.execute_reply":"2024-01-15T15:46:46.498325Z","shell.execute_reply.started":"2024-01-15T15:46:46.490441Z"},"trusted":true},"outputs":[],"source":["tf_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T15:46:46.502979Z","iopub.status.busy":"2024-01-15T15:46:46.502609Z","iopub.status.idle":"2024-01-15T15:46:46.515704Z","shell.execute_reply":"2024-01-15T15:46:46.514724Z","shell.execute_reply.started":"2024-01-15T15:46:46.502954Z"},"trusted":true},"outputs":[],"source":["if len(test.text.values) <= 5:\n","    sub.to_csv(\"submission.csv\", index=False)\n","else:\n","    clf = MultinomialNB(alpha=0.0225)\n","\n","    sgd_model = SGDClassifier(\n","        max_iter=9000, tol=1e-4, random_state=6743, loss=\"modified_huber\"\n","    )\n","\n","    p = {\n","        \"verbose\": -1,\n","        \"n_iter\": 3000,\n","        \"colsample_bytree\": 0.7800,\n","        \"colsample_bynode\": 0.8000,\n","        \"random_state\": 6743,\n","        \"metric\": \"auc\",\n","        \"objective\": \"cross_entropy\",\n","        \"learning_rate\": 0.00581909898961407,\n","    }\n","    lgb = LGBMClassifier(**p)\n","\n","    cat = CatBoostClassifier(\n","        iterations=3000,\n","        verbose=0,\n","        subsample=0.35,\n","        random_seed=6543,\n","        allow_const_label=True,\n","        loss_function=\"CrossEntropy\",\n","        learning_rate=0.005599066836106983,\n","    )\n","\n","    ensemble = VotingClassifier(\n","        estimators=[(\"mnb\", clf), (\"sgd\", sgd_model), (\"lgb\", lgb), (\"cat\", cat)],\n","        weights=[0.1, 0.31, 0.28, 0.67],\n","        voting=\"soft\",\n","        n_jobs=-1,\n","    )\n","\n","    ensemble.fit(tf_train, y_train_label)\n","    gc.collect()\n","\n","    final_preds = ensemble.predict_proba(tf_test)[:, 1]\n","    sub[\"generated\"] = final_preds\n","    sub.to_csv(\"submission.csv\", index=False)\n","    sub.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf_train.shape"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6888007,"sourceId":61542,"sourceType":"competition"},{"datasetId":4005256,"sourceId":6977472,"sourceType":"datasetVersion"},{"datasetId":3596984,"sourceId":6258399,"sourceType":"datasetVersion"},{"datasetId":3534587,"sourceId":6161220,"sourceType":"datasetVersion"},{"datasetId":3768687,"sourceId":6519082,"sourceType":"datasetVersion"},{"sourceId":159219517,"sourceType":"kernelVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
