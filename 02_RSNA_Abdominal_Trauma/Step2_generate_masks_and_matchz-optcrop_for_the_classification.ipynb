{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Modification of the original code\n","I just took the code of the winner of the RSNA 2022 cervical spine fracture detection competition.  \n","Link: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage1"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:25.743219Z","iopub.status.busy":"2022-10-29T06:00:25.742950Z","iopub.status.idle":"2022-10-29T06:00:34.162024Z","shell.execute_reply":"2022-10-29T06:00:34.160905Z","shell.execute_reply.started":"2022-10-29T06:00:25.743184Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import sys\n","import gc\n","import ast\n","import cv2\n","import imageio\n","import time\n","import timm\n","import pickle\n","import random\n","import pydicom\n","import dicomsdl\n","import argparse\n","import warnings\n","import numpy as np\n","import cupy as cp\n","import pandas as pd\n","import glob\n","import nibabel as nib\n","from PIL import Image\n","from tqdm import tqdm\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gzip\n","import pickle\n","from joblib import Parallel, delayed\n","import lz4.frame\n","import mgzip\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.cuda.amp as amp\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","\n","\n","%matplotlib inline\n","rcParams['figure.figsize'] = 20, 8\n","device = torch.device('cuda')\n","torch.backends.cudnn.benchmark = True\n","\n","os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n","sys.path.append('./lib_models')\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","DEVICE"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:29.485211Z","iopub.status.busy":"2022-10-29T06:03:29.484820Z","iopub.status.idle":"2022-10-29T06:03:29.494317Z","shell.execute_reply":"2022-10-29T06:03:29.493294Z","shell.execute_reply.started":"2022-10-29T06:03:29.485168Z"},"trusted":true},"outputs":[],"source":["RESOL = 128\n","CROP_RESOL = 128\n","\n","BASE_PATH = '/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection'\n","MASK_SAVE_PATH = f'{BASE_PATH}/mask_preprocessed'\n","MASK_VALID_PATH = f'{BASE_PATH}/mask_validation'\n","TRAIN_PATH = f'{BASE_PATH}/train_images'\n","\n","BATCH_MASK_PRED = 8\n","N_PROCESS_CROP = 18\n","PREPROC_NORM_OR_STD = False # True: normalization, False: standardization\n","\n","seg_weight_name = 'timm3d_res50d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep_fold0_dicom_std_aug_0.93.pt'\n","load_kernel = None\n","load_last = True\n","n_blocks = 4\n","backbone = 'resnet50d'\n","\n","image_sizes = [128, 128, 128]\n","data_dir = '../input/rsna-2022-cervical-spine-fracture-detection'\n","use_amp = True\n","\n","\n","num_workers = 24\n","out_dim = 5\n","\n","model_dir = f'{BASE_PATH}/seg_models_backup'\n","seg_inference_dir = f'{BASE_PATH}/seg_infer_results'\n","cropped_img_dir   = f'{BASE_PATH}/3d_preprocessed_crop_ratio'\n","os.makedirs(model_dir, exist_ok=True)\n","os.makedirs(MASK_VALID_PATH, exist_ok=True)\n","os.makedirs(seg_inference_dir, exist_ok = True)\n","os.makedirs(cropped_img_dir, exist_ok = True)"]},{"cell_type":"markdown","metadata":{},"source":["# Path to save cropped image"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.404497Z","iopub.status.busy":"2022-10-29T06:00:34.403952Z","iopub.status.idle":"2022-10-29T06:00:34.486417Z","shell.execute_reply":"2022-10-29T06:00:34.485281Z","shell.execute_reply.started":"2022-10-29T06:00:34.404459Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv(f'{BASE_PATH}/train_meta.csv')\n","mask_paths = []\n","cropped_paths = []\n","for i in range(0, len(df_train)):\n","    row = df_train.iloc[i]\n","    file_name = row['path'].split('/')[-1]\n","    mask_paths.append(f'{seg_inference_dir}/{file_name}')\n","    cropped_paths.append(f'{cropped_img_dir}/{file_name}')\n","df_train['cropped_path'] = cropped_paths\n","df_train['mask_path']    = mask_paths\n","df_train.tail()\n","\n","df_train.to_csv(f'{BASE_PATH}/train_meta.csv', index = False)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>patient_id</th>\n","      <th>series</th>\n","      <th>bowel_healthy</th>\n","      <th>bowel_injury</th>\n","      <th>extravasation_healthy</th>\n","      <th>extravasation_injury</th>\n","      <th>kidney_healthy</th>\n","      <th>kidney_low</th>\n","      <th>kidney_high</th>\n","      <th>liver_healthy</th>\n","      <th>liver_low</th>\n","      <th>liver_high</th>\n","      <th>spleen_healthy</th>\n","      <th>spleen_low</th>\n","      <th>spleen_high</th>\n","      <th>any_injury</th>\n","      <th>fold</th>\n","      <th>path</th>\n","      <th>mask_path</th>\n","      <th>cropped_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>26473</td>\n","      <td>11365</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>35348</td>\n","      <td>41274</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>37260</td>\n","      <td>21973</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37260</td>\n","      <td>19788</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>18301</td>\n","      <td>1407</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4706</th>\n","      <td>8076</td>\n","      <td>41087</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4707</th>\n","      <td>20087</td>\n","      <td>30769</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4708</th>\n","      <td>20087</td>\n","      <td>45305</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4709</th>\n","      <td>43639</td>\n","      <td>64933</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4710</th>\n","      <td>43639</td>\n","      <td>59734</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4711 rows × 20 columns</p>\n","</div>"],"text/plain":["      patient_id  series  bowel_healthy  bowel_injury  extravasation_healthy  \\\n","0          26473   11365              1             0                      1   \n","1          35348   41274              1             0                      1   \n","2          37260   21973              1             0                      1   \n","3          37260   19788              1             0                      1   \n","4          18301    1407              1             0                      1   \n","...          ...     ...            ...           ...                    ...   \n","4706        8076   41087              1             0                      1   \n","4707       20087   30769              1             0                      1   \n","4708       20087   45305              1             0                      1   \n","4709       43639   64933              1             0                      1   \n","4710       43639   59734              1             0                      1   \n","\n","      extravasation_injury  kidney_healthy  kidney_low  kidney_high  \\\n","0                        0               1           0            0   \n","1                        0               1           0            0   \n","2                        0               1           0            0   \n","3                        0               1           0            0   \n","4                        0               1           0            0   \n","...                    ...             ...         ...          ...   \n","4706                     0               1           0            0   \n","4707                     0               0           1            0   \n","4708                     0               0           1            0   \n","4709                     0               1           0            0   \n","4710                     0               1           0            0   \n","\n","      liver_healthy  liver_low  liver_high  spleen_healthy  spleen_low  \\\n","0                 1          0           0               1           0   \n","1                 1          0           0               1           0   \n","2                 1          0           0               1           0   \n","3                 1          0           0               1           0   \n","4                 1          0           0               1           0   \n","...             ...        ...         ...             ...         ...   \n","4706              1          0           0               0           1   \n","4707              1          0           0               1           0   \n","4708              1          0           0               1           0   \n","4709              1          0           0               1           0   \n","4710              1          0           0               1           0   \n","\n","      spleen_high  any_injury  fold  \\\n","0               0           0     3   \n","1               0           0     0   \n","2               0           0     0   \n","3               0           0     0   \n","4               0           0     4   \n","...           ...         ...   ...   \n","4706            0           1     1   \n","4707            0           1     2   \n","4708            0           1     2   \n","4709            0           0     2   \n","4710            0           0     2   \n","\n","                                                   path  \\\n","0     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","1     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","2     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","3     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","...                                                 ...   \n","4706  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4707  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4708  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4709  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4710  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","\n","                                              mask_path  \\\n","0     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","1     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","2     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","3     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","...                                                 ...   \n","4706  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4707  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4708  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4709  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4710  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","\n","                                           cropped_path  \n","0     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","1     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","2     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","3     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","...                                                 ...  \n","4706  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4707  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4708  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4709  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4710  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","\n","[4711 rows x 20 columns]"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["df_train"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.497353Z","iopub.status.busy":"2022-10-29T06:00:34.496904Z","iopub.status.idle":"2022-10-29T06:00:34.514198Z","shell.execute_reply":"2022-10-29T06:00:34.513162Z","shell.execute_reply.started":"2022-10-29T06:00:34.497317Z"},"trusted":true},"outputs":[],"source":["def compress(name, data):\n","    with gzip.open(name, 'wb') as f:\n","        pickle.dump(data, f)\n","\n","def decompress(name):\n","    with gzip.open(name, 'rb') as f:\n","        data = pickle.load(f)\n","    return data\n","\n","def compress_fast(name, data):\n","    with open(name, 'wb') as f:\n","        pickle.dump(data, f)\n","\n","def decompress_fast(name):\n","    with open(name, 'rb') as f:\n","        data = pickle.load(f)\n","    return data\n","\n","class SEGDataset(Dataset):\n","    def __init__(self, df, mode):\n","\n","        self.df = df.reset_index()\n","        self.mode = mode\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        \n","        image = decompress(row['path']).unsqueeze(0)\n","        #image = torch.from_numpy(image).to(torch.float32)\n","        save_path = row['mask_path']\n","\n","        return image, save_path\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.059903Z","iopub.status.busy":"2022-10-29T06:02:59.058663Z","iopub.status.idle":"2022-10-29T06:02:59.070787Z","shell.execute_reply":"2022-10-29T06:02:59.069705Z","shell.execute_reply.started":"2022-10-29T06:02:59.059860Z"},"trusted":true},"outputs":[],"source":["class TimmSegModel(nn.Module):\n","    def __init__(self, backbone, segtype='unet', pretrained=False):\n","        super(TimmSegModel, self).__init__()\n","\n","        self.encoder = timm.create_model(\n","            backbone,\n","            in_chans=1,\n","            features_only=True,\n","            pretrained=pretrained\n","        )\n","        g = self.encoder(torch.rand(1, 1, 64, 64))\n","        encoder_channels = [1] + [_.shape[1] for _ in g]\n","        decoder_channels = [256, 128, 64, 32, 16]\n","        if segtype == 'unet':\n","            self.decoder = smp.unet.decoder.UnetDecoder(\n","                encoder_channels=encoder_channels[:n_blocks+1],\n","                decoder_channels=decoder_channels[:n_blocks],\n","                n_blocks=n_blocks,\n","            )\n","\n","        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","    def forward(self,x):\n","        global_features = [0] + self.encoder(x)[:n_blocks]\n","        seg_features = self.decoder(*global_features)\n","        seg_features = self.segmentation_head(seg_features)\n","        return seg_features"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.073051Z","iopub.status.busy":"2022-10-29T06:02:59.072642Z","iopub.status.idle":"2022-10-29T06:03:13.720715Z","shell.execute_reply":"2022-10-29T06:03:13.719595Z","shell.execute_reply.started":"2022-10-29T06:02:59.073017Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 5, 128, 128, 128])"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["from timm.models.layers.conv2d_same import Conv2dSame\n","from conv3d_same import Conv3dSame\n","\n","\n","def convert_3d(module):\n","\n","    module_output = module\n","    if isinstance(module, torch.nn.BatchNorm2d):\n","        module_output = torch.nn.BatchNorm3d(\n","            module.num_features,\n","            module.eps,\n","            module.momentum,\n","            module.affine,\n","            module.track_running_stats,\n","        )\n","        if module.affine:\n","            with torch.no_grad():\n","                module_output.weight = module.weight\n","                module_output.bias = module.bias\n","        module_output.running_mean = module.running_mean\n","        module_output.running_var = module.running_var\n","        module_output.num_batches_tracked = module.num_batches_tracked\n","        if hasattr(module, \"qconfig\"):\n","            module_output.qconfig = module.qconfig\n","            \n","    elif isinstance(module, Conv2dSame):\n","        module_output = Conv3dSame(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.Conv2d):\n","        module_output = torch.nn.Conv3d(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","            padding_mode=module.padding_mode\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.MaxPool2d):\n","        module_output = torch.nn.MaxPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            dilation=module.dilation,\n","            ceil_mode=module.ceil_mode,\n","        )\n","    elif isinstance(module, torch.nn.AvgPool2d):\n","        module_output = torch.nn.AvgPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            ceil_mode=module.ceil_mode,\n","        )\n","\n","    for name, child in module.named_children():\n","        module_output.add_module(\n","            name, convert_3d(child)\n","        )\n","    del module\n","\n","    return module_output\n","\n","m = TimmSegModel(backbone)\n","m = convert_3d(m)\n","m(torch.rand(1, 1, 128,128,128)).shape"]},{"cell_type":"markdown","metadata":{},"source":["# Inference mask"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.738907Z","iopub.status.busy":"2022-10-29T06:03:13.738511Z","iopub.status.idle":"2022-10-29T06:03:13.755925Z","shell.execute_reply":"2022-10-29T06:03:13.755044Z","shell.execute_reply.started":"2022-10-29T06:03:13.738871Z"},"trusted":true},"outputs":[],"source":["def infer_func(model, loader_valid):\n","    model.eval()\n","    valid_loss = []\n","    outputs = []\n","    th = 0.1\n","    batch_metrics = [[]]\n","    bar = tqdm(loader_valid)\n","    \n","    with torch.no_grad():\n","        with amp.autocast():\n","            for images, save_paths in bar:\n","                images = images.cuda()\n","                logits = model(images)\n","                preds = (logits.sigmoid() > th).float().detach().cpu().numpy()\n","                y_preds = (preds+0.1).astype(np.uint8)\n","                def save_mask(ind, preds = y_preds): \n","                    compress(save_paths[ind], preds[ind])\n","                \n","                Parallel(n_jobs = len(y_preds))(delayed(save_mask)(i) for i in range(len(y_preds)))"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.979257Z","iopub.status.busy":"2022-10-29T06:03:13.978679Z","iopub.status.idle":"2022-10-29T06:03:13.992437Z","shell.execute_reply":"2022-10-29T06:03:13.991378Z","shell.execute_reply.started":"2022-10-29T06:03:13.979202Z"},"trusted":true},"outputs":[],"source":["def run(fold):\n","    model_file = os.path.join(model_dir, f'{seg_weight_name}')\n","    dataset_train = SEGDataset(df_train, 'valid')\n","    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_MASK_PRED, shuffle=False, num_workers=BATCH_MASK_PRED)\n","\n","    model = TimmSegModel(backbone, pretrained=True)\n","    model = convert_3d(model)\n","\n","    model.load_state_dict(torch.load(model_file))\n","    model = model.to(device)\n","\n","    print(len(dataset_train))\n","    \n","    infer_func(model, loader_train)\n","\n","    del model\n","    torch.cuda.empty_cache()\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:33.006143Z","iopub.status.busy":"2022-10-29T06:03:33.005515Z"},"trusted":true},"outputs":[],"source":["#run(0)"]},{"cell_type":"markdown","metadata":{},"source":["# Postprocss to get crop regions"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["#Returns GPU array\n","def standardize_pixel_array(pixel_array, dcm_rows):\n","    \"\"\"\n","    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n","    \"\"\"\n","    # Correct DICOM pixel_array if PixelRepresentation == 1.\n","    for z in range(0, len(pixel_array)):\n","        if int(dcm_rows[z]['PixelRepresentation']) == 1:\n","            bit_shift = dcm_rows[z]['BitsAllocated'] - dcm_rows[z]['BitsStored']\n","            dtype = pixel_array[z].dtype \n","            pixel_array[z] = (pixel_array[z] << bit_shift).astype(dtype) >>  bit_shift\n","\n","    pixel_array = torch.from_numpy(pixel_array.astype(np.float16)).to(DEVICE).to(torch.float16)    \n","\n","    for z in range(0, len(pixel_array)):\n","        intercept = float(dcm_rows[z]['RescaleIntercept'])\n","        slope = float(dcm_rows[z]['RescaleSlope'])\n","        center = int(dcm_rows[z]['WindowCenter'])\n","        width = int(dcm_rows[z]['WindowWidth'])\n","        low = center - width / 2\n","        high = center + width / 2    \n","        \n","        pixel_array[z] = (pixel_array[z] * slope) + intercept\n","        pixel_array[z] = torch.clip(pixel_array[z], low, high)\n","        \n","    gc.collect()    \n","    return pixel_array\n"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["#The order of the crop region data format\n","#Z start/end, Y start/end, X start/end for each mask channels + total region for the extravasation prediction\n","def calc_crop_region(mask):\n","    crop_range = np.zeros((6, 6))\n","    crop_range[:,::2]=10000\n","    mask_z = np.max(mask, axis = (2, 3)).astype(bool)\n","    mask_y = np.max(mask, axis = (1, 3)).astype(bool)\n","    mask_x = np.max(mask, axis = (1, 2)).astype(bool)\n","    \n","    template_range = np.arange(0, RESOL)\n","\n","    for mi in range(0, 5):\n","        zrange = template_range[mask_z[mi]]\n","        yrange = template_range[mask_y[mi]]\n","        xrange = template_range[mask_x[mi]]\n","        # For incomplete organ\n","        if(len(zrange)==0):\n","            zrange = template_range.copy()\n","            yrange = template_range.copy()\n","            xrange = template_range.copy()\n","\n","        crop_range[mi] = np.min(zrange), np.max(zrange)+1, np.min(yrange), np.max(yrange)+1, np.min(xrange), np.max(xrange)+1\n","\n","    crop_range[5] = np.min(crop_range[:5, 0]), np.max(crop_range[:5, 1]), np.min(crop_range[:5, 2]), \\\n","                    np.max(crop_range[:5, 3]), np.min(crop_range[:5,4]), np.max(crop_range[:5, 5])\n","    \n","    crop_range[:,:2]/=len(mask_z[0])\n","    crop_range[:,2:4]/=len(mask_y[0])\n","    crop_range[:,4:6]/=len(mask_x[0])\n","\n","    # Then make extravasation (# 5 mask) to reference one and convert other mask's crop respective to it\n","    # --> To minimize the loading size due to speed issue.\n","    zmin, rel_zrange = crop_range[5,0], crop_range[5,1]-crop_range[5,0]\n","    ymin, rel_yrange = crop_range[5,2], crop_range[5,3]-crop_range[5,2]\n","    xmin, rel_xrange = crop_range[5,4], crop_range[5,5]-crop_range[5,4]\n","\n","    crop_range[:5,:2] = (crop_range[:5,:2]-zmin)/rel_zrange\n","    crop_range[:5,2:4] = (crop_range[:5,2:4]-ymin)/rel_yrange\n","    crop_range[:5,4:6] = (crop_range[:5,4:6]-xmin)/rel_xrange\n","\n","    return crop_range\n","\n","def crop_resize_avg_and_std_3d(data, region, resize_shape, is_norm = PREPROC_NORM_OR_STD):\n","    shapes = data.shape\n","    region[:2]*=shapes[0]\n","    region[2:4]*=shapes[1]\n","    region[4:6]*=shapes[2]\n","    region = region.astype(int)\n","\n","    cropped = torch.clone(data[region[0]:region[1], region[2]:region[3], region[4]:region[5]])    \n","\n","    #resize xy\n","    cropped = transforms.Resize((int(resize_shape[1]), int(resize_shape[2])), antialias = True)(cropped)\n","    \n","    #zyx to xzy\n","    cropped = torch.permute(cropped, (2, 0, 1))\n","    #Resize yz\n","    cropped = transforms.Resize((int(resize_shape[0]), int(resize_shape[1])), antialias = True)(cropped)\n","    #xzy to zyx\n","    cropped = torch.permute(cropped, (1, 2, 0))\n","\n","    if is_norm:\n","        bottom = torch.min(data)\n","        data -= bottom\n","        top    = torch.max(data)\n","        data/=top\n","        del top, bottom\n","    else:\n","        avg = torch.mean(data, (0, 1, 2))\n","        std = torch.std(data, (0, 1, 2))\n","        data = (data-avg)/std\n","        del avg, std\n","        \n","    del shapes, region\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    return cropped\n","\n","# Read each slice and stack them to make 3d data\n","def process_3d_crop(save_path, mask_path, resize_shapes, data_path = TRAIN_PATH):\n","    tmp = save_path.split('/')[-1][:-4]\n","    tmp = tmp.split('_')\n","    patient, study = int(tmp[0]), int(tmp[1])\n","    \n","    mask = decompress(mask_path)\n","    crop_regions = calc_crop_region(mask)\n","    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n","\n","    del mask\n","    gc.collect()\n","    crop_regions[5] = 0, 1, 0, 1, 0, 1\n","\n","    imgs = {}    \n","    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):  \n","        pos_z = -int((f.split('/')[-1])[:-4])\n","        imgs[pos_z] = f\n","\n","    imgs_3d = []\n","    n_imgs = len(imgs)    \n","    z_crop_range= (absolute_crop[0:2]*n_imgs).astype(int)\n","    \n","    dcm_rows = []\n","    for i, k in enumerate(sorted(imgs.keys())):\n","        if(i >= z_crop_range[0] and i < z_crop_range[1]):\n","            IS_XY_CROP = False\n","            f = imgs[k]\n","            #Exception for the corrupted dicom file\n","            if (f=='/kaggle/input/rsna-2023-abdominal-trauma-detection/test_images/3124/5842/514.dcm'):\n","                continue\n","            opened_dicom = dicomsdl.open(f)\n","            img = opened_dicom.pixelData(storedvalue=True)\n","            params = opened_dicom.getPixelDataInfo()\n","\n","            if not IS_XY_CROP:\n","                img_shape = np.shape(img)\n","                xy_crop_range = absolute_crop[2:].copy()   \n","                xy_crop_range[0:2]*=img_shape[0]\n","                xy_crop_range[2:4]*=img_shape[1]            \n","                xy_crop_range = xy_crop_range.astype(int)                \n","                IS_XY_CROP = True\n","                \n","            img = img[xy_crop_range[0]:xy_crop_range[1], xy_crop_range[2]:xy_crop_range[3]]             \n","\n","            #dcm_row = pd.DataFrame.from_dict(params)                   \n","            dcm_rows.append(params)                  \n","            imgs_3d.append(img[None])\n","\n","    del opened_dicom\n","    gc.collect()\n","                \n","    imgs_3d = np.vstack(imgs_3d)\n","\n","    imgs_3d  = standardize_pixel_array(imgs_3d, dcm_rows)\n","\n","    min_imgs = torch.min(imgs_3d)\n","    max_imgs = torch.max(imgs_3d)\n","        \n","    imgs_3d = ((imgs_3d - min_imgs) / (max_imgs - min_imgs + 1e-6))\n","\n","    if str(dcm_rows[0]['PhotometricInterpretation']) == \"MONOCHROME1\":\n","        imgs_3d = 1.0 - imgs_3d\n","\n","    #Loaded original imgs_3d    \n","    origin_shape = imgs_3d.shape\n","    for i in range(0, 6):    \n","        #To deal with almost not detected slices\n","        try:   \n","            # To deal with possible noises\n","            if(((crop_regions[i,1]-crop_regions[i,0]) < 10/origin_shape[0]) or \n","                ((crop_regions[i,3]-crop_regions[i,2]) < 10/origin_shape[1]) or \n","                ((crop_regions[i,5]-crop_regions[i,4]) < 10/origin_shape[2])):\n","                dummy_failure_function()\n","            \n","            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, crop_regions[i], resize_shapes[i])).to(torch.float16).to('cpu')\n","            compress_fast(f'{save_path}_{i}', processed_img_3d)      \n","\n","            del processed_img_3d\n","            gc.collect()\n","        except:\n","            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, np.array([0, 1, 0, 1, 0, 1]), resize_shapes[i])).to(torch.float16).to('cpu')\n","            compress_fast(f'{save_path}_{i}', processed_img_3d)\n","            del processed_img_3d\n","            gc.collect()  \n","\n","    del imgs, img, imgs_3d\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    \n","def calc_size_3d_crop(save_path, mask_path, data_path = TRAIN_PATH):\n","    tmp = save_path.split('/')[-1][:-4]\n","    tmp = tmp.split('_')\n","    patient, study = int(tmp[0]), int(tmp[1])\n","    \n","    mask = decompress(mask_path)\n","    crop_regions = calc_crop_region(mask)\n","    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n","\n","    crop_regions[5] = 0, 1, 0, 1, 0, 1\n","\n","    imgs = {}    \n","    \n","    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):      \n","        try:            \n","            img = dicomsdl.open(f).pixelData(storedvalue=True)\n","            img_shape = np.shape(img)\n","            xy_crop_range = absolute_crop[2:].copy()   \n","            xy_crop_range[0:2]*=img_shape[0]\n","            xy_crop_range[2:4]*=img_shape[1]            \n","            xy_crop_range = xy_crop_range.astype(int)\n","            img = img.astype(float)\n","            break\n","        except:\n","            continue\n","    \n","    base_crop_shape = np.zeros(3, float)\n","    base_crop_shape[:] = absolute_crop[1::2] - absolute_crop[0::2]\n","    base_crop_shape[0]*= len(glob.glob(data_path + f'/{patient}/{study}/*.dcm'))\n","    base_crop_shape[1:3] = xy_crop_range[1::2] - xy_crop_range[0::2]\n","    \n","    all_crop_shape = crop_regions[:,1::2] - crop_regions[:,0::2]\n","    for i in range(0, 6):\n","        all_crop_shape[i] *= base_crop_shape\n","    del img\n","    gc.collect()\n","    return all_crop_shape"]},{"cell_type":"markdown","metadata":{},"source":["# Calculate average crop size"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["# Preprocess dataset\n","rng_samples = np.linspace(0, len(df_train), N_PROCESS_CROP+1, dtype = int)\n","def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = df_train):\n","    partial_crop_shapes = []\n","    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n","        partial_crop_shapes.append(calc_size_3d_crop(df_train.iloc[i]['cropped_path'], df_train.iloc[i]['mask_path'])[None])\n","    partial_crop_shapes = np.vstack(partial_crop_shapes)\n","    return partial_crop_shapes"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 262/262 [00:13<00:00, 18.97it/s]\n","100%|██████████| 262/262 [00:13<00:00, 18.92it/s]\n","100%|██████████| 261/261 [00:14<00:00, 18.28it/s]\n","100%|██████████| 261/261 [00:14<00:00, 17.98it/s]\n","100%|██████████| 261/261 [00:14<00:00, 17.92it/s]\n","100%|██████████| 262/262 [00:14<00:00, 17.93it/s]\n","100%|██████████| 262/262 [00:14<00:00, 17.90it/s]\n","100%|██████████| 262/262 [00:14<00:00, 17.65it/s]\n","100%|██████████| 261/261 [00:14<00:00, 17.66it/s]\n","100%|██████████| 262/262 [00:14<00:00, 17.70it/s]\n","100%|██████████| 262/262 [00:14<00:00, 17.57it/s]\n","100%|██████████| 262/262 [00:15<00:00, 17.33it/s]\n","100%|██████████| 262/262 [00:15<00:00, 17.37it/s]\n","100%|██████████| 262/262 [00:15<00:00, 17.23it/s]\n","100%|██████████| 262/262 [00:15<00:00, 17.07it/s]\n","100%|██████████| 262/262 [00:15<00:00, 16.99it/s]\n","100%|██████████| 261/261 [00:15<00:00, 16.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 235 ms, sys: 376 ms, total: 611 ms\n","Wall time: 16.3 s\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 262/262 [00:15<00:00, 16.60it/s]\n"]}],"source":["%%time\n","all_crop_shapes = Parallel(n_jobs = N_PROCESS_CROP)(delayed(process_3d_wrapper)(i) for i in range(N_PROCESS_CROP))\n","all_crop_shapes = np.vstack(all_crop_shapes)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["norm_all_crop_shapes = (all_crop_shapes[:,:,0]*all_crop_shapes[:,:,1]*all_crop_shapes[:,:,2])**(1/3)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/plain":["array([[123, 120, 166],\n","       [102, 146, 169],\n","       [100, 157, 157],\n","       [ 74, 167, 199],\n","       [ 80, 174, 180],\n","       [116, 123, 175]])"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["normed_all_crop_shapes = all_crop_shapes.copy()\n","for i in range(0, len(df_train)):\n","    for j in range(0, 6):\n","        normed_all_crop_shapes[i,j]/=norm_all_crop_shapes[i, j]\n","\n","avg_normed_all_crop_shapes = np.average(normed_all_crop_shapes, axis = 0)\n","avg_normed_all_crop_shapes*=RESOL\n","avg_normed_all_crop_shapes = avg_normed_all_crop_shapes.astype(int)\n","avg_normed_all_crop_shapes"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"data":{"text/plain":["array([[ 87,  85, 118],\n","       [ 87, 125, 145],\n","       [ 87, 137, 137],\n","       [ 87, 197, 235],\n","       [ 87, 190, 197],\n","       [ 87,  92, 132]])"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["#make z same and roughly match the number of pixel to RESOL**3 case\n","avg_normed_all_crop_shapes = avg_normed_all_crop_shapes.astype(float)\n","avg_z = np.average(avg_normed_all_crop_shapes[:,0])\n","avg_pixels = 0\n","for i in range(0, len(avg_normed_all_crop_shapes)):\n","    rescale_factor = avg_z / avg_normed_all_crop_shapes[i,0]\n","    avg_normed_all_crop_shapes[i,:]*=rescale_factor\n","    avg_pixels+=(avg_normed_all_crop_shapes[i,0] * avg_normed_all_crop_shapes[i,1] * avg_normed_all_crop_shapes[i,2])\n","avg_pixels/=6    \n","\n","total_rescale_factor = (RESOL**3/avg_pixels)**(1/3)\n","avg_normed_all_crop_shapes*=total_rescale_factor\n","avg_normed_all_crop_shapes[:,0]+=0.1\n","\n","avg_normed_all_crop_shapes = (avg_normed_all_crop_shapes*CROP_RESOL/RESOL).astype(int)\n","avg_normed_all_crop_shapes"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["# Preprocess dataset\n","rng_samples = np.linspace(0, len(df_train), N_PROCESS_CROP+1, dtype = int)\n","def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = df_train, resize_shapes = avg_normed_all_crop_shapes):\n","    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n","        if not os.path.isfile(f\"{df_train.iloc[i]['cropped_path']}_0\"):\n","            process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'], resize_shapes)     \n","        \n","        #For error correction\n","        else:            \n","            try:\n","                data = decompress(f\"df_train.iloc[i]['cropped_path']_0\")\n","            except: \n","                process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'], resize_shapes)  \n","        "]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 262/262 [08:39<00:00,  1.98s/it]\n","100%|██████████| 261/261 [08:40<00:00,  2.00s/it]\n","100%|██████████| 262/262 [08:42<00:00,  1.99s/it]\n","100%|██████████| 262/262 [08:47<00:00,  2.01s/it]\n","100%|██████████| 261/261 [08:48<00:00,  2.03s/it]\n","100%|██████████| 262/262 [08:49<00:00,  2.02s/it]\n","100%|██████████| 262/262 [08:50<00:00,  2.02s/it]\n","100%|██████████| 262/262 [08:52<00:00,  2.03s/it]\n","100%|██████████| 262/262 [08:53<00:00,  2.04s/it]\n","100%|██████████| 262/262 [08:54<00:00,  2.04s/it]\n","100%|██████████| 262/262 [08:54<00:00,  2.04s/it]\n","100%|██████████| 262/262 [08:56<00:00,  2.05s/it]\n","100%|██████████| 262/262 [08:56<00:00,  2.05s/it]\n","100%|██████████| 261/261 [08:59<00:00,  2.07s/it]\n","100%|██████████| 261/261 [09:00<00:00,  2.07s/it]\n","100%|██████████| 261/261 [09:00<00:00,  2.07s/it]\n","100%|██████████| 262/262 [09:04<00:00,  2.08s/it]\n","100%|█████████▉| 261/262 [09:07<00:00,  1.04it/s]"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 1.95 s, sys: 596 ms, total: 2.54 s\n","Wall time: 9min 9s\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 262/262 [09:08<00:00,  2.09s/it]\n"]},{"data":{"text/plain":["[None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None]"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","#process_3d_wrapper(0)\n","Parallel(n_jobs = N_PROCESS_CROP)(delayed(process_3d_wrapper)(i) for i in range(N_PROCESS_CROP))"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["ind = 0\n","cropped = decompress_fast(f\"{df_train.iloc[ind]['cropped_path']}_0\")\n","mask    = decompress(df_train.iloc[ind]['mask_path'])"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f04ef333090>"]},"execution_count":61,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 2000x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.imshow(mask[2,:,64,:])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
