{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Modification of the original code\n","I just took the code of the winner of the RSNA 2022 cervical spine fracture detection competition.  \n","Link: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage1"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:25.743219Z","iopub.status.busy":"2022-10-29T06:00:25.742950Z","iopub.status.idle":"2022-10-29T06:00:34.162024Z","shell.execute_reply":"2022-10-29T06:00:34.160905Z","shell.execute_reply.started":"2022-10-29T06:00:25.743184Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import sys\n","import gc\n","import ast\n","import cv2\n","import imageio\n","import time\n","import timm\n","import pickle\n","import random\n","import pydicom\n","import dicomsdl\n","import argparse\n","import warnings\n","import numpy as np\n","import cupy as cp\n","import pandas as pd\n","import glob\n","import nibabel as nib\n","from PIL import Image\n","from tqdm import tqdm\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gzip\n","import pickle\n","from joblib import Parallel, delayed\n","import lz4.frame\n","import mgzip\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.cuda.amp as amp\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","\n","\n","%matplotlib inline\n","rcParams['figure.figsize'] = 20, 8\n","device = torch.device('cuda')\n","torch.backends.cudnn.benchmark = True\n","\n","os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n","sys.path.append('./lib_models')\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","DEVICE"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:29.485211Z","iopub.status.busy":"2022-10-29T06:03:29.484820Z","iopub.status.idle":"2022-10-29T06:03:29.494317Z","shell.execute_reply":"2022-10-29T06:03:29.493294Z","shell.execute_reply.started":"2022-10-29T06:03:29.485168Z"},"trusted":true},"outputs":[],"source":["RESOL = [96, 176, 176]\n","CROP_RESOL = 128\n","\n","BASE_PATH = '/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection'\n","MASK_SAVE_PATH = f'{BASE_PATH}/mask_preprocessed'\n","MASK_VALID_PATH = f'{BASE_PATH}/mask_validation'\n","TRAIN_PATH = f'{BASE_PATH}/train_images'\n","\n","BATCH_MASK_PRED = 8\n","N_PROCESS_CROP = 24\n","PREPROC_NORM_OR_STD = False # True: normalization, False: standardization\n","\n","seg_weight_name = '230929_timm3d_res50d_unet4b_fold0_optshape_CV0.89.pth'\n","load_kernel = None\n","load_last = True\n","n_blocks = 4\n","backbone = 'resnet50d'\n","\n","image_sizes = [96, 176, 176]\n","data_dir = '../input/rsna-2022-cervical-spine-fracture-detection'\n","use_amp = True\n","\n","\n","num_workers = 24\n","out_dim = 5\n","\n","model_dir = f'{BASE_PATH}/seg_models_backup'\n","seg_inference_dir = f'{BASE_PATH}/seg_infer_results'\n","cropped_img_dir   = f'{BASE_PATH}/3d_preprocessed_crop_ratio'\n","os.makedirs(model_dir, exist_ok=True)\n","os.makedirs(MASK_VALID_PATH, exist_ok=True)\n","os.makedirs(seg_inference_dir, exist_ok = True)\n","os.makedirs(cropped_img_dir, exist_ok = True)"]},{"cell_type":"markdown","metadata":{},"source":["# Path to save cropped image"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.404497Z","iopub.status.busy":"2022-10-29T06:00:34.403952Z","iopub.status.idle":"2022-10-29T06:00:34.486417Z","shell.execute_reply":"2022-10-29T06:00:34.485281Z","shell.execute_reply.started":"2022-10-29T06:00:34.404459Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv(f'{BASE_PATH}/train_meta.csv')\n","mask_paths = []\n","cropped_paths = []\n","for i in range(0, len(df_train)):\n","    row = df_train.iloc[i]\n","    file_name = row['path'].split('/')[-1]\n","    mask_paths.append(f'{seg_inference_dir}/{file_name}')\n","    cropped_paths.append(f'{cropped_img_dir}/{file_name}')\n","df_train['cropped_path'] = cropped_paths\n","df_train['mask_path']    = mask_paths\n","df_train.tail()\n","\n","df_train.to_csv(f'{BASE_PATH}/train_meta.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_train"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.497353Z","iopub.status.busy":"2022-10-29T06:00:34.496904Z","iopub.status.idle":"2022-10-29T06:00:34.514198Z","shell.execute_reply":"2022-10-29T06:00:34.513162Z","shell.execute_reply.started":"2022-10-29T06:00:34.497317Z"},"trusted":true},"outputs":[],"source":["def compress(name, data):\n","    with gzip.open(name, 'wb') as f:\n","        pickle.dump(data, f)\n","\n","def decompress(name):\n","    with gzip.open(name, 'rb') as f:\n","        data = pickle.load(f)\n","    return data\n","\n","def compress_fast(name, data):\n","    with open(name, 'wb') as f:\n","        pickle.dump(data, f)\n","\n","def decompress_fast(name):\n","    with open(name, 'rb') as f:\n","        data = pickle.load(f)\n","    return data\n","\n","class SEGDataset(Dataset):\n","    def __init__(self, df, mode):\n","\n","        self.df = df.reset_index()\n","        self.mode = mode\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        \n","        image = decompress(row['path']).unsqueeze(0)\n","        #image = torch.from_numpy(image).to(torch.float32)\n","        save_path = row['mask_path']\n","\n","        return image, save_path\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.059903Z","iopub.status.busy":"2022-10-29T06:02:59.058663Z","iopub.status.idle":"2022-10-29T06:02:59.070787Z","shell.execute_reply":"2022-10-29T06:02:59.069705Z","shell.execute_reply.started":"2022-10-29T06:02:59.059860Z"},"trusted":true},"outputs":[],"source":["class TimmSegModel(nn.Module):\n","    def __init__(self, backbone, segtype='unet', pretrained=False):\n","        super(TimmSegModel, self).__init__()\n","\n","        self.encoder = timm.create_model(\n","            backbone,\n","            in_chans=1,\n","            features_only=True,\n","            pretrained=pretrained\n","        )\n","        g = self.encoder(torch.rand(1, 1, 64, 64))\n","        encoder_channels = [1] + [_.shape[1] for _ in g]\n","        decoder_channels = [256, 128, 64, 32, 16]\n","        if segtype == 'unet':\n","            self.decoder = smp.unet.decoder.UnetDecoder(\n","                encoder_channels=encoder_channels[:n_blocks+1],\n","                decoder_channels=decoder_channels[:n_blocks],\n","                n_blocks=n_blocks,\n","            )\n","\n","        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","    def forward(self,x):\n","        global_features = [0] + self.encoder(x)[:n_blocks]\n","        seg_features = self.decoder(*global_features)\n","        seg_features = self.segmentation_head(seg_features)\n","        return seg_features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.073051Z","iopub.status.busy":"2022-10-29T06:02:59.072642Z","iopub.status.idle":"2022-10-29T06:03:13.720715Z","shell.execute_reply":"2022-10-29T06:03:13.719595Z","shell.execute_reply.started":"2022-10-29T06:02:59.073017Z"},"trusted":true},"outputs":[],"source":["from timm.models.layers.conv2d_same import Conv2dSame\n","from conv3d_same import Conv3dSame\n","\n","\n","def convert_3d(module):\n","\n","    module_output = module\n","    if isinstance(module, torch.nn.BatchNorm2d):\n","        module_output = torch.nn.BatchNorm3d(\n","            module.num_features,\n","            module.eps,\n","            module.momentum,\n","            module.affine,\n","            module.track_running_stats,\n","        )\n","        if module.affine:\n","            with torch.no_grad():\n","                module_output.weight = module.weight\n","                module_output.bias = module.bias\n","        module_output.running_mean = module.running_mean\n","        module_output.running_var = module.running_var\n","        module_output.num_batches_tracked = module.num_batches_tracked\n","        if hasattr(module, \"qconfig\"):\n","            module_output.qconfig = module.qconfig\n","            \n","    elif isinstance(module, Conv2dSame):\n","        module_output = Conv3dSame(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.Conv2d):\n","        module_output = torch.nn.Conv3d(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","            padding_mode=module.padding_mode\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.MaxPool2d):\n","        module_output = torch.nn.MaxPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            dilation=module.dilation,\n","            ceil_mode=module.ceil_mode,\n","        )\n","    elif isinstance(module, torch.nn.AvgPool2d):\n","        module_output = torch.nn.AvgPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            ceil_mode=module.ceil_mode,\n","        )\n","\n","    for name, child in module.named_children():\n","        module_output.add_module(\n","            name, convert_3d(child)\n","        )\n","    del module\n","\n","    return module_output\n","\n","m = TimmSegModel(backbone)\n","m = convert_3d(m)\n","m(torch.rand(1, 1, 128,128,128)).shape"]},{"cell_type":"markdown","metadata":{},"source":["# Inference mask"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.738907Z","iopub.status.busy":"2022-10-29T06:03:13.738511Z","iopub.status.idle":"2022-10-29T06:03:13.755925Z","shell.execute_reply":"2022-10-29T06:03:13.755044Z","shell.execute_reply.started":"2022-10-29T06:03:13.738871Z"},"trusted":true},"outputs":[],"source":["def infer_func(model, loader_valid):\n","    model.eval()\n","    valid_loss = []\n","    outputs = []\n","    th = 0.1\n","    batch_metrics = [[]]\n","    bar = tqdm(loader_valid)\n","    \n","    with torch.no_grad():\n","        with amp.autocast():\n","            for images, save_paths in bar:\n","                images = images.cuda()\n","                logits = model(images)\n","                preds = (logits.sigmoid() > th).float().detach().cpu().numpy()\n","                y_preds = (preds+0.1).astype(np.uint8)\n","                def save_mask(ind, preds = y_preds): \n","                    compress(save_paths[ind], preds[ind])\n","                \n","                Parallel(n_jobs = len(y_preds))(delayed(save_mask)(i) for i in range(len(y_preds)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.979257Z","iopub.status.busy":"2022-10-29T06:03:13.978679Z","iopub.status.idle":"2022-10-29T06:03:13.992437Z","shell.execute_reply":"2022-10-29T06:03:13.991378Z","shell.execute_reply.started":"2022-10-29T06:03:13.979202Z"},"trusted":true},"outputs":[],"source":["def run(fold):\n","    model_file = os.path.join(model_dir, f'{seg_weight_name}')\n","    dataset_train = SEGDataset(df_train, 'valid')\n","    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_MASK_PRED, shuffle=False, num_workers=BATCH_MASK_PRED)\n","\n","    model = TimmSegModel(backbone, pretrained=True)\n","    model = convert_3d(model)\n","\n","    model.load_state_dict(torch.load(model_file))\n","    model = model.to(device)\n","\n","    print(len(dataset_train))\n","    \n","    infer_func(model, loader_train)\n","\n","    del model\n","    torch.cuda.empty_cache()\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:33.006143Z","iopub.status.busy":"2022-10-29T06:03:33.005515Z"},"trusted":true},"outputs":[],"source":["run(0)"]},{"cell_type":"markdown","metadata":{},"source":["# Postprocss to get crop regions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Returns GPU array\n","def standardize_pixel_array(pixel_array, dcm_rows):\n","    \"\"\"\n","    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n","    \"\"\"\n","    # Correct DICOM pixel_array if PixelRepresentation == 1.\n","    for z in range(0, len(pixel_array)):\n","        if int(dcm_rows[z]['PixelRepresentation']) == 1:\n","            bit_shift = dcm_rows[z]['BitsAllocated'] - dcm_rows[z]['BitsStored']\n","            dtype = pixel_array[z].dtype \n","            pixel_array[z] = (pixel_array[z] << bit_shift).astype(dtype) >>  bit_shift\n","\n","    pixel_array = torch.from_numpy(pixel_array.astype(np.float16)).to(DEVICE).to(torch.float16)    \n","\n","    for z in range(0, len(pixel_array)):\n","        intercept = float(dcm_rows[z]['RescaleIntercept'])\n","        slope = float(dcm_rows[z]['RescaleSlope'])\n","        center = int(dcm_rows[z]['WindowCenter'])\n","        width = int(dcm_rows[z]['WindowWidth'])\n","        low = center - width / 2\n","        high = center + width / 2    \n","        \n","        pixel_array[z] = (pixel_array[z] * slope) + intercept\n","        pixel_array[z] = torch.clip(pixel_array[z], low, high)\n","        \n","    gc.collect()    \n","    return pixel_array\n","\n","#to make longest axis to z\n","def get_new_axis_order(max_ind, n_dims=3):\n","    origin_order = np.arange(0, 3)\n","    new_order = origin_order.copy()\n","    new_order[0] = max_ind\n","    new_order[max_ind]=0\n","    return new_order"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#The order of the crop region data format\n","#Z start/end, Y start/end, X start/end for each mask channels + total region for the extravasation prediction\n","def calc_crop_region(mask):\n","    crop_range = np.zeros((6, 6))\n","    crop_range[:,::2]=10000\n","    mask_z = np.max(mask, axis = (2, 3)).astype(bool)\n","    mask_y = np.max(mask, axis = (1, 3)).astype(bool)\n","    mask_x = np.max(mask, axis = (1, 2)).astype(bool)\n","    \n","    template_zrange = np.arange(0, RESOL[0])\n","    template_yrange = np.arange(0, RESOL[1])\n","    template_xrange = np.arange(0, RESOL[2])\n","        \n","    for mi in range(0, 5):\n","        zrange = template_zrange[mask_z[mi]]\n","        yrange = template_yrange[mask_y[mi]]\n","        xrange = template_xrange[mask_x[mi]]\n","        # For incomplete organ\n","        if(len(zrange)==0):\n","            zrange = template_zrange.copy()\n","            yrange = template_yrange.copy()\n","            xrange = template_xrange.copy()\n","\n","        crop_range[mi] = np.min(zrange), np.max(zrange)+1, np.min(yrange), np.max(yrange)+1, np.min(xrange), np.max(xrange)+1\n","\n","    crop_range[5] = np.min(crop_range[:5, 0]), np.max(crop_range[:5, 1]), np.min(crop_range[:5, 2]), \\\n","                    np.max(crop_range[:5, 3]), np.min(crop_range[:5,4]), np.max(crop_range[:5, 5])\n","    \n","    crop_range[:,:2]/=len(mask_z[0])\n","    crop_range[:,2:4]/=len(mask_y[0])\n","    crop_range[:,4:6]/=len(mask_x[0])\n","\n","    # Then make extravasation (# 5 mask) to reference one and convert other mask's crop respective to it\n","    # --> To minimize the loading size due to speed issue.\n","    zmin, rel_zrange = crop_range[5,0], crop_range[5,1]-crop_range[5,0]\n","    ymin, rel_yrange = crop_range[5,2], crop_range[5,3]-crop_range[5,2]\n","    xmin, rel_xrange = crop_range[5,4], crop_range[5,5]-crop_range[5,4]\n","\n","    crop_range[:5,:2] = (crop_range[:5,:2]-zmin)/rel_zrange\n","    crop_range[:5,2:4] = (crop_range[:5,2:4]-ymin)/rel_yrange\n","    crop_range[:5,4:6] = (crop_range[:5,4:6]-xmin)/rel_xrange\n","\n","    return crop_range\n","\n","def crop_resize_avg_and_std_3d(data, region, resize_shape, reorder_axes, is_norm = PREPROC_NORM_OR_STD):\n","    shapes = data.shape\n","    region[:2]*=shapes[0]\n","    region[2:4]*=shapes[1]\n","    region[4:6]*=shapes[2]\n","    region = region.astype(int)\n","\n","    cropped = torch.clone(data[region[0]:region[1], region[2]:region[3], region[4]:region[5]])    \n","    \n","    #axes reorder to make longest axis to z axis\n","    cropped = torch.permute(cropped, (reorder_axes[0], reorder_axes[1], reorder_axes[2]))\n","    #resize xy\n","    cropped = transforms.Resize((int(resize_shape[1]), int(resize_shape[2])), antialias = True)(cropped)\n","    \n","    #zyx to xzy\n","    cropped = torch.permute(cropped, (2, 0, 1))\n","    #Resize yz\n","    cropped = transforms.Resize((int(resize_shape[0]), int(resize_shape[1])), antialias = True)(cropped)\n","    #xzy to zyx\n","    cropped = torch.permute(cropped, (1, 2, 0))\n","\n","    if is_norm:\n","        bottom = torch.min(data)\n","        data -= bottom\n","        top    = torch.max(data)\n","        data/=top\n","        del top, bottom\n","    else:\n","        avg = torch.mean(data, (0, 1, 2))\n","        std = torch.std(data, (0, 1, 2))\n","        data = (data-avg)/std\n","        del avg, std\n","        \n","    del shapes, region\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    return cropped\n","\n","# Read each slice and stack them to make 3d data\n","def process_3d_crop(save_path, mask_path, resize_shapes, reorder_axes, data_path = TRAIN_PATH):\n","    tmp = save_path.split('/')[-1][:-4]\n","    tmp = tmp.split('_')\n","    patient, study = int(tmp[0]), int(tmp[1])\n","    \n","    mask = decompress(mask_path)\n","    crop_regions = calc_crop_region(mask)\n","    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n","\n","    del mask\n","    gc.collect()\n","    crop_regions[5] = 0, 1, 0, 1, 0, 1\n","\n","    imgs = {}    \n","    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):  \n","        pos_z = -int((f.split('/')[-1])[:-4])\n","        imgs[pos_z] = f\n","\n","    imgs_3d = []\n","    n_imgs = len(imgs)    \n","    z_crop_range= (absolute_crop[0:2]*n_imgs).astype(int)\n","    \n","    dcm_rows = []\n","    for i, k in enumerate(sorted(imgs.keys())):\n","        if(i >= z_crop_range[0] and i < z_crop_range[1]):\n","            IS_XY_CROP = False\n","            f = imgs[k]\n","            #Exception for the corrupted dicom file\n","            if (f=='/kaggle/input/rsna-2023-abdominal-trauma-detection/test_images/3124/5842/514.dcm'):\n","                continue\n","            opened_dicom = dicomsdl.open(f)\n","            img = opened_dicom.pixelData(storedvalue=True)\n","            params = opened_dicom.getPixelDataInfo()\n","\n","            if not IS_XY_CROP:\n","                img_shape = np.shape(img)\n","                xy_crop_range = absolute_crop[2:].copy()   \n","                xy_crop_range[0:2]*=img_shape[0]\n","                xy_crop_range[2:4]*=img_shape[1]            \n","                xy_crop_range = xy_crop_range.astype(int)                \n","                IS_XY_CROP = True\n","                \n","            img = img[xy_crop_range[0]:xy_crop_range[1], xy_crop_range[2]:xy_crop_range[3]]             \n","\n","            #dcm_row = pd.DataFrame.from_dict(params)                   \n","            dcm_rows.append(params)                  \n","            imgs_3d.append(img[None])\n","\n","    del opened_dicom\n","    gc.collect()\n","                \n","    imgs_3d = np.vstack(imgs_3d)\n","\n","    imgs_3d  = standardize_pixel_array(imgs_3d, dcm_rows)\n","\n","    min_imgs = torch.min(imgs_3d)\n","    max_imgs = torch.max(imgs_3d)\n","        \n","    imgs_3d = ((imgs_3d - min_imgs) / (max_imgs - min_imgs + 1e-6))\n","\n","    if str(dcm_rows[0]['PhotometricInterpretation']) == \"MONOCHROME1\":\n","        imgs_3d = 1.0 - imgs_3d\n","\n","    #Loaded original imgs_3d    \n","    origin_shape = imgs_3d.shape\n","    for i in range(0, 6):    \n","        #To deal with almost not detected slices\n","        try:   \n","            # To deal with possible noises\n","            if(((crop_regions[i,1]-crop_regions[i,0]) < 10/origin_shape[0]) or \n","                ((crop_regions[i,3]-crop_regions[i,2]) < 10/origin_shape[1]) or \n","                ((crop_regions[i,5]-crop_regions[i,4]) < 10/origin_shape[2])):\n","                dummy_failure_function()\n","            \n","            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, crop_regions[i], resize_shapes[i], reorder_axes[i])).to(torch.float16).to('cpu')\n","            compress_fast(f'{save_path}_{i}', processed_img_3d)      \n","\n","            del processed_img_3d\n","            gc.collect()\n","        except:\n","            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, np.array([0, 1, 0, 1, 0, 1]), resize_shapes[i], reorder_axes[i])).to(torch.float16).to('cpu')\n","            compress_fast(f'{save_path}_{i}', processed_img_3d)\n","            del processed_img_3d\n","            gc.collect()  \n","\n","    del imgs, img, imgs_3d\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    \n","def calc_size_3d_crop(save_path, mask_path, data_path = TRAIN_PATH):\n","    tmp = save_path.split('/')[-1][:-4]\n","    tmp = tmp.split('_')\n","    patient, study = int(tmp[0]), int(tmp[1])\n","    \n","    mask = decompress(mask_path)\n","    crop_regions = calc_crop_region(mask)\n","    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n","\n","    crop_regions[5] = 0, 1, 0, 1, 0, 1\n","\n","    imgs = {}    \n","    \n","    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):      \n","        try:            \n","            img = dicomsdl.open(f).pixelData(storedvalue=True)\n","            img_shape = np.shape(img)\n","            xy_crop_range = absolute_crop[2:].copy()   \n","            xy_crop_range[0:2]*=img_shape[0]\n","            xy_crop_range[2:4]*=img_shape[1]            \n","            xy_crop_range = xy_crop_range.astype(int)\n","            img = img.astype(float)\n","            break\n","        except:\n","            continue\n","    \n","    base_crop_shape = np.zeros(3, float)\n","    base_crop_shape[:] = absolute_crop[1::2] - absolute_crop[0::2]\n","    base_crop_shape[0]*= len(glob.glob(data_path + f'/{patient}/{study}/*.dcm'))\n","    base_crop_shape[1:3] = xy_crop_range[1::2] - xy_crop_range[0::2]\n","    \n","    all_crop_shape = crop_regions[:,1::2] - crop_regions[:,0::2]\n","    for i in range(0, 6):\n","        all_crop_shape[i] *= base_crop_shape\n","    del img\n","    gc.collect()\n","    return all_crop_shape"]},{"cell_type":"markdown","metadata":{},"source":["# Calculate average crop size"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocess dataset\n","rng_samples = np.linspace(0, len(df_train), N_PROCESS_CROP+1, dtype = int)\n","def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = df_train):\n","    partial_crop_shapes = []\n","    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n","        partial_crop_shapes.append(calc_size_3d_crop(df_train.iloc[i]['cropped_path'], df_train.iloc[i]['mask_path'])[None])\n","    partial_crop_shapes = np.vstack(partial_crop_shapes)\n","    return partial_crop_shapes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","all_crop_shapes = Parallel(n_jobs = N_PROCESS_CROP)(delayed(process_3d_wrapper)(i) for i in range(N_PROCESS_CROP))\n","all_crop_shapes = np.vstack(all_crop_shapes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["norm_all_crop_shapes = (all_crop_shapes[:,:,0]*all_crop_shapes[:,:,1]*all_crop_shapes[:,:,2])**(1/3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["normed_all_crop_shapes = all_crop_shapes.copy()\n","for i in range(0, len(df_train)):\n","    for j in range(0, 6):\n","        normed_all_crop_shapes[i,j]/=norm_all_crop_shapes[i, j]\n","\n","avg_normed_all_crop_shapes = np.average(normed_all_crop_shapes, axis = 0)\n","avg_normed_all_crop_shapes*=CROP_RESOL\n","avg_normed_all_crop_shapes = avg_normed_all_crop_shapes.astype(int)\n","avg_normed_all_crop_shapes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_axes = np.argmax(avg_normed_all_crop_shapes, axis = 1)\n","\n","reorder_axes = np.zeros((6, 3), int)\n","for i in range(0, 6):\n","    reorder_axes[i] = get_new_axis_order(max_axes[i])\n","\n","reordered_crop_shapes = avg_normed_all_crop_shapes\n","for i in range(0, len(reordered_crop_shapes)):\n","    reordered_crop_shapes[i] = reordered_crop_shapes[i,reorder_axes[i]]\n","reordered_crop_shapes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#make z same and roughly match the number of pixel to RESOL**3 case\n","avg_normed_all_crop_shapes = avg_normed_all_crop_shapes.astype(float)\n","avg_z = np.average(avg_normed_all_crop_shapes[:,0])\n","avg_pixels = 0\n","for i in range(0, len(avg_normed_all_crop_shapes)):\n","    rescale_factor = avg_z / avg_normed_all_crop_shapes[i,0]\n","    avg_normed_all_crop_shapes[i,:]*=rescale_factor\n","    avg_pixels+=(avg_normed_all_crop_shapes[i,0] * avg_normed_all_crop_shapes[i,1] * avg_normed_all_crop_shapes[i,2])\n","avg_pixels/=6    \n","\n","total_rescale_factor = (CROP_RESOL**3/avg_pixels)**(1/3)\n","avg_normed_all_crop_shapes*=total_rescale_factor\n","avg_normed_all_crop_shapes[:,0]+=0.1\n","\n","avg_normed_all_crop_shapes = avg_normed_all_crop_shapes.astype(int)\n","avg_normed_all_crop_shapes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocess dataset\n","rng_samples = np.linspace(0, len(df_train), N_PROCESS_CROP+1, dtype = int)\n","def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = df_train, resize_shapes = avg_normed_all_crop_shapes, reorder_axes = reorder_axes):\n","    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n","        if not os.path.isfile(f\"{df_train.iloc[i]['cropped_path']}_0\"):\n","            process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'], resize_shapes, reorder_axes)     \n","        \n","        #For error correction\n","        else:            \n","            try:\n","                data = decompress(f\"df_train.iloc[i]['cropped_path']_0\")\n","            except: \n","                process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'], resize_shapes, reorder_axes)  \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","Parallel(n_jobs = N_PROCESS_CROP)(delayed(process_3d_wrapper)(i) for i in range(N_PROCESS_CROP))"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["ind = 8\n","cropped = decompress_fast(f\"{df_train.iloc[ind]['cropped_path']}_1\")\n","mask    = decompress(df_train.iloc[ind]['mask_path'])\n","img     = decompress(df_train.iloc[ind]['path'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
