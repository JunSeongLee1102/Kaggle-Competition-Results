{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Modification of the original code\n","I just took the code of the winner of the RSNA 2022 cervical spine fracture detection competition.  \n","Link: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage1"]},{"cell_type":"markdown","metadata":{},"source":["# 1st Place Solution Training 3D Semantic Segmentation (Stage1)\n","\n","Hi all,\n","\n","I'm very exciting to writing this notebook and the summary of our solution here.\n","\n","This is FULL version of training my final models (stage1), using resnet18d as backbone, unet as decoder and using 128x128x128 as input.\n","\n","NOTE: **You need to run this code locally because the RAM is not enough here.**\n","\n","NOTE2: **It is highly recommended to pre-process the 3D semantic segmentation training data first and save it locally, which can greatly speed up the loading of the data.**\n","\n","My brief summary of winning solution: https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/362607\n","\n","* Train Stage1 Notebook: This notebook\n","* Train Stage2 (Type1) Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage2-type1\n","* Train Stage2 (Type2) Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage2-type2\n","* Inference Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-inference\n","\n","**If you find these notebooks helpful please upvote. Thanks! **"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T05:59:39.369783Z","iopub.status.busy":"2022-10-29T05:59:39.369067Z","iopub.status.idle":"2022-10-29T06:00:25.730464Z","shell.execute_reply":"2022-10-29T06:00:25.729271Z","shell.execute_reply.started":"2022-10-29T05:59:39.369678Z"},"trusted":true},"outputs":[],"source":["#!pip -q install monai\n","#!pip -q install segmentation-models-pytorch==0.2.1"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-10-29T06:00:25.733795Z","iopub.status.busy":"2022-10-29T06:00:25.733350Z","iopub.status.idle":"2022-10-29T06:00:25.741619Z","shell.execute_reply":"2022-10-29T06:00:25.740579Z","shell.execute_reply.started":"2022-10-29T06:00:25.733741Z"},"trusted":true},"outputs":[],"source":["DEBUG = False\n","\n","import os\n","import sys\n","#sys.path = [\n","#    '../input/covn3d-same',\n","#] + sys.path"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:25.743219Z","iopub.status.busy":"2022-10-29T06:00:25.742950Z","iopub.status.idle":"2022-10-29T06:00:34.162024Z","shell.execute_reply":"2022-10-29T06:00:34.160905Z","shell.execute_reply.started":"2022-10-29T06:00:25.743184Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import gc\n","import ast\n","import cv2\n","import time\n","import timm\n","import pickle\n","import random\n","import pydicom\n","import argparse\n","import warnings\n","import numpy as np\n","import pandas as pd\n","from glob import glob\n","import nibabel as nib\n","from PIL import Image\n","from tqdm import tqdm\n","import albumentations\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gzip\n","import pickle\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.cuda.amp as amp\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","\n","from monai.transforms import Resize\n","import  monai.transforms as transforms\n","\n","%matplotlib inline\n","rcParams['figure.figsize'] = 20, 8\n","device = torch.device('cuda')\n","torch.backends.cudnn.benchmark = True\n","\n","sys.path.append('./lib_models')"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:29.485211Z","iopub.status.busy":"2022-10-29T06:03:29.484820Z","iopub.status.idle":"2022-10-29T06:03:29.494317Z","shell.execute_reply":"2022-10-29T06:03:29.493294Z","shell.execute_reply.started":"2022-10-29T06:03:29.485168Z"},"trusted":true},"outputs":[],"source":["RESOL = 128\n","\n","BASE_PATH = '/home/junseonglee/01_codes/input/rsna-2023-abdominal-trauma-detection'\n","MASK_SAVE_PATH = f'{BASE_PATH}/mask_preprocessed'\n","MASK_VALID_PATH = f'{BASE_PATH}/mask_validation'\n","\n","kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n","load_kernel = None\n","load_last = True\n","n_blocks = 4\n","n_folds = 5\n","backbone = 'resnet18d'\n","\n","image_sizes = [128, 128, 128]\n","R = Resize(image_sizes)\n","\n","init_lr = 3e-3\n","batch_size = 4\n","drop_rate = 0.\n","drop_path_rate = 0.\n","loss_weights = [1, 1]\n","p_mixup = 0.1\n","\n","data_dir = '../input/rsna-2022-cervical-spine-fracture-detection'\n","use_amp = True\n","num_workers = 8\n","out_dim = 5\n","\n","n_epochs = 1000\n","\n","log_dir = f'{BASE_PATH}/seg_models_backup'\n","model_dir = f'{BASE_PATH}/seg_models_backup'\n","os.makedirs(log_dir, exist_ok=True)\n","os.makedirs(model_dir, exist_ok=True)\n","os.makedirs(MASK_VALID_PATH, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.390438Z","iopub.status.busy":"2022-10-29T06:00:34.389599Z","iopub.status.idle":"2022-10-29T06:00:34.402322Z","shell.execute_reply":"2022-10-29T06:00:34.401400Z","shell.execute_reply.started":"2022-10-29T06:00:34.390400Z"},"trusted":true},"outputs":[],"source":["transforms_valid = transforms.Compose([\n","])"]},{"cell_type":"markdown","metadata":{},"source":["# DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.404497Z","iopub.status.busy":"2022-10-29T06:00:34.403952Z","iopub.status.idle":"2022-10-29T06:00:34.486417Z","shell.execute_reply":"2022-10-29T06:00:34.485281Z","shell.execute_reply.started":"2022-10-29T06:00:34.404459Z"},"trusted":true},"outputs":[],"source":["df_seg = pd.read_csv(f'{BASE_PATH}/seg_info.csv')\n","\n","kf = KFold(5, shuffle = True, random_state = 0)\n","df_seg['fold'] = -1\n","for fold, (train_idx, valid_idx) in enumerate(kf.split(df_seg, df_seg)):\n","    df_seg.loc[valid_idx, 'fold'] = fold\n","\n","df_seg.tail()"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.488463Z","iopub.status.busy":"2022-10-29T06:00:34.488061Z","iopub.status.idle":"2022-10-29T06:00:34.494025Z","shell.execute_reply":"2022-10-29T06:00:34.492953Z","shell.execute_reply.started":"2022-10-29T06:00:34.488426Z"},"trusted":true},"outputs":[],"source":["revert_list = [\n","    '1.2.826.0.1.3680043.1363',\n","    '1.2.826.0.1.3680043.20120',\n","    '1.2.826.0.1.3680043.2243',\n","    '1.2.826.0.1.3680043.24606',\n","    '1.2.826.0.1.3680043.32071'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.497353Z","iopub.status.busy":"2022-10-29T06:00:34.496904Z","iopub.status.idle":"2022-10-29T06:00:34.514198Z","shell.execute_reply":"2022-10-29T06:00:34.513162Z","shell.execute_reply.started":"2022-10-29T06:00:34.497317Z"},"trusted":true},"outputs":[],"source":["def compress(name, data):\n","    with gzip.open(name, 'wb') as f:\n","        pickle.dump(data, f)\n","\n","def decompress(name):\n","    with gzip.open(name, 'rb') as f:\n","        data = pickle.load(f)\n","    return data\n","\n","def load_pickle(name):\n","    with open(name, 'rb') as f:\n","        data = pickle.load(f)\n","    return data    \n","\n","def load_sample(row, has_mask=True):\n","    image = decompress(row['img_path'])[None]\n","\n","    if has_mask:\n","        mask = load_pickle(row['mask_path'])\n","        \n","        return image, mask\n","    else:\n","        return image\n","\n","\n","\n","class SEGDataset(Dataset):\n","    def __init__(self, df, mode, transform):\n","\n","        self.df = df.reset_index()\n","        self.mode = mode\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        \n","        \n","        ### using local cache\n","#         image_file = os.path.join(data_dir, f'{row.StudyInstanceUID}.npy')\n","#         mask_file = os.path.join(data_dir, f'{row.StudyInstanceUID}_mask.npy')\n","#         image = np.load(image_file).astype(np.float32)\n","#         mask = np.load(mask_file).astype(np.float32)\n","\n","        image, mask = load_sample(row, has_mask=True)\n","        image = torch.from_numpy(image).to(torch.float32)\n","        mask  = torch.from_numpy(mask).to(torch.float32)\n","\n","        res = self.transform({'image':image, 'mask':mask})\n","        image = res['image']\n","        mask = res['mask']\n","\n","\n","        return image, mask\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.516237Z","iopub.status.busy":"2022-10-29T06:00:34.515883Z","iopub.status.idle":"2022-10-29T06:00:34.528793Z","shell.execute_reply":"2022-10-29T06:00:34.527571Z","shell.execute_reply.started":"2022-10-29T06:00:34.516189Z"},"trusted":true},"outputs":[],"source":["rcParams['figure.figsize'] = 20,8\n","\n","df_show = df_seg"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.059903Z","iopub.status.busy":"2022-10-29T06:02:59.058663Z","iopub.status.idle":"2022-10-29T06:02:59.070787Z","shell.execute_reply":"2022-10-29T06:02:59.069705Z","shell.execute_reply.started":"2022-10-29T06:02:59.059860Z"},"trusted":true},"outputs":[],"source":["class TimmSegModel(nn.Module):\n","    def __init__(self, backbone, segtype='unet', pretrained=False):\n","        super(TimmSegModel, self).__init__()\n","\n","        self.encoder = timm.create_model(\n","            backbone,\n","            in_chans=1,\n","            features_only=True,\n","            drop_rate=drop_rate,\n","            drop_path_rate=drop_path_rate,\n","            pretrained=pretrained\n","        )\n","        g = self.encoder(torch.rand(1, 1, 64, 64))\n","        encoder_channels = [1] + [_.shape[1] for _ in g]\n","        decoder_channels = [256, 128, 64, 32, 16]\n","        if segtype == 'unet':\n","            self.decoder = smp.unet.decoder.UnetDecoder(\n","                encoder_channels=encoder_channels[:n_blocks+1],\n","                decoder_channels=decoder_channels[:n_blocks],\n","                n_blocks=n_blocks,\n","            )\n","\n","        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","    def forward(self,x):\n","        global_features = [0] + self.encoder(x)[:n_blocks]\n","        seg_features = self.decoder(*global_features)\n","        seg_features = self.segmentation_head(seg_features)\n","        return seg_features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.073051Z","iopub.status.busy":"2022-10-29T06:02:59.072642Z","iopub.status.idle":"2022-10-29T06:03:13.720715Z","shell.execute_reply":"2022-10-29T06:03:13.719595Z","shell.execute_reply.started":"2022-10-29T06:02:59.073017Z"},"trusted":true},"outputs":[],"source":["from timm.models.layers.conv2d_same import Conv2dSame\n","from conv3d_same import Conv3dSame\n","\n","\n","def convert_3d(module):\n","\n","    module_output = module\n","    if isinstance(module, torch.nn.BatchNorm2d):\n","        module_output = torch.nn.BatchNorm3d(\n","            module.num_features,\n","            module.eps,\n","            module.momentum,\n","            module.affine,\n","            module.track_running_stats,\n","        )\n","        if module.affine:\n","            with torch.no_grad():\n","                module_output.weight = module.weight\n","                module_output.bias = module.bias\n","        module_output.running_mean = module.running_mean\n","        module_output.running_var = module.running_var\n","        module_output.num_batches_tracked = module.num_batches_tracked\n","        if hasattr(module, \"qconfig\"):\n","            module_output.qconfig = module.qconfig\n","            \n","    elif isinstance(module, Conv2dSame):\n","        module_output = Conv3dSame(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.Conv2d):\n","        module_output = torch.nn.Conv3d(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","            padding_mode=module.padding_mode\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.MaxPool2d):\n","        module_output = torch.nn.MaxPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            dilation=module.dilation,\n","            ceil_mode=module.ceil_mode,\n","        )\n","    elif isinstance(module, torch.nn.AvgPool2d):\n","        module_output = torch.nn.AvgPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            ceil_mode=module.ceil_mode,\n","        )\n","\n","    for name, child in module.named_children():\n","        module_output.add_module(\n","            name, convert_3d(child)\n","        )\n","    del module\n","\n","    return module_output\n","\n","\n","m = TimmSegModel(backbone)\n","m = convert_3d(m)\n","m(torch.rand(1, 1, 128,128,128)).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import Any, Dict, Optional\n","\n","\n","def binary_dice_score(\n","    y_pred: torch.Tensor,\n","    y_true: torch.Tensor,\n","    threshold: Optional[float] = None,\n","    nan_score_on_empty=False,\n","    eps: float = 1e-7,\n",") -> float:\n","\n","    if threshold is not None:\n","        y_pred = (y_pred > threshold).to(y_true.dtype)\n","\n","    intersection = torch.sum(y_pred * y_true).item()\n","    cardinality = (torch.sum(y_pred) + torch.sum(y_true)).item()\n","\n","    score = (2.0 * intersection) / (cardinality + eps)\n","\n","    has_targets = torch.sum(y_true) > 0\n","    has_predicted = torch.sum(y_pred) > 0\n","\n","    if not has_targets:\n","        if nan_score_on_empty:\n","            score = np.nan\n","        else:\n","            score = float(not has_predicted)\n","    return score\n","\n","\n","def multilabel_dice_score(\n","    y_true: torch.Tensor,\n","    y_pred: torch.Tensor,\n","    threshold=None,\n","    eps=1e-7,\n","    nan_score_on_empty=False,\n","):\n","    ious = []\n","    num_classes = y_pred.size(0)\n","    for class_index in range(num_classes):\n","        iou = binary_dice_score(\n","            y_pred=y_pred[class_index],\n","            y_true=y_true[class_index],\n","            threshold=threshold,\n","            nan_score_on_empty=nan_score_on_empty,\n","            eps=eps,\n","        )\n","        ious.append(iou)\n","\n","    return ious\n","\n","\n","def dice_loss(input, target):\n","    input = torch.sigmoid(input)\n","    smooth = 1.0\n","    iflat = input.view(-1)\n","    tflat = target.view(-1)\n","    intersection = (iflat * tflat).sum()\n","    return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n","\n","\n","def bce_dice(input, target, loss_weights=loss_weights):\n","    loss1 = loss_weights[0] * nn.BCEWithLogitsLoss()(input, target)\n","    loss2 = loss_weights[1] * dice_loss(input, target)\n","    return (loss1 + loss2) / sum(loss_weights)\n","\n","criterion = bce_dice"]},{"cell_type":"markdown","metadata":{},"source":["# Valid func"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.738907Z","iopub.status.busy":"2022-10-29T06:03:13.738511Z","iopub.status.idle":"2022-10-29T06:03:13.755925Z","shell.execute_reply":"2022-10-29T06:03:13.755044Z","shell.execute_reply.started":"2022-10-29T06:03:13.738871Z"},"trusted":true},"outputs":[],"source":["def test_func(model, loader_valid):\n","    model.eval()\n","    valid_loss = []\n","    outputs = []\n","    ths = [0.1]\n","    batch_metrics = [[]]\n","    bar = tqdm(loader_valid)\n","\n","    counter = 0\n","    \n","    with torch.no_grad():\n","        for images, gt_masks in bar:\n","            images = images.cuda()\n","            gt_masks = gt_masks.cuda()\n","\n","            logits = model(images)\n","            images = images.cpu().numpy()\n","            loss = criterion(logits, gt_masks)\n","            valid_loss.append(loss.item())\n","            for thi, th in enumerate(ths):\n","                pred = (logits.sigmoid() > th).float().detach()\n","                for i in range(logits.shape[0]):\n","                    image = images[i]\n","                    y_pred = logits[i].sigmoid().cpu()\n","                    y_true = gt_masks[i].cpu()\n","                    tmp = multilabel_dice_score(\n","                        y_pred=y_pred,\n","                        y_true=y_true,\n","                        threshold=0.5,\n","                    )\n","                    y_pred = y_pred.numpy()\n","                    y_true = y_true.numpy()\n","                    compress(f'{MASK_VALID_PATH}/image_{counter}.pkl', image)                    \n","                    compress(f'{MASK_VALID_PATH}/pred_{counter}.pkl', y_pred)\n","                    compress(f'{MASK_VALID_PATH}/true_{counter}.pkl', y_true)\n","                    counter +=1\n","                    batch_metrics[thi].extend(tmp)\n","            bar.set_description(f'smth:{np.mean(valid_loss[-30:]):.4f}')\n","            \n","    metrics = [np.mean(this_metric) for this_metric in batch_metrics]\n","    print('best th:', ths[np.argmax(metrics)], 'best dc:', np.max(metrics))\n","    best_th = ths[np.argmax(metrics)]\n","    \n","\n","    return np.mean(valid_loss), np.max(metrics)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.757646Z","iopub.status.busy":"2022-10-29T06:03:13.757188Z","iopub.status.idle":"2022-10-29T06:03:13.977069Z","shell.execute_reply":"2022-10-29T06:03:13.976109Z","shell.execute_reply.started":"2022-10-29T06:03:13.757610Z"},"trusted":true},"outputs":[],"source":["rcParams['figure.figsize'] = 20, 2\n","optimizer = optim.AdamW(m.parameters(), lr=init_lr)\n","scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000)\n","lrs = []\n","for epoch in range(1, 1000+1):\n","    scheduler_cosine.step(epoch-1)\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","plt.plot(range(len(lrs)), lrs)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.979257Z","iopub.status.busy":"2022-10-29T06:03:13.978679Z","iopub.status.idle":"2022-10-29T06:03:13.992437Z","shell.execute_reply":"2022-10-29T06:03:13.991378Z","shell.execute_reply.started":"2022-10-29T06:03:13.979202Z"},"trusted":true},"outputs":[],"source":["def run(fold):\n","\n","    log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n","    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n","\n","    valid_ = df_seg[df_seg['fold'] == fold].reset_index(drop=True)\n","\n","    dataset_valid = SEGDataset(valid_, 'valid', transform=transforms_valid)\n","    loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","    model = TimmSegModel(backbone, pretrained=True)\n","    model = convert_3d(model)\n","\n","    model.load_state_dict(torch.load(model_file))\n","    model = model.to(device)\n","    \n","    from_epoch = 0\n","\n","    print(len(dataset_valid))\n","\n","    valid_loss, metric = test_func(model, loader_valid)\n","\n","    del model\n","    torch.cuda.empty_cache()\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:33.006143Z","iopub.status.busy":"2022-10-29T06:03:33.005515Z"},"trusted":true},"outputs":[],"source":["run(0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(0, 42):\n","    origin_img = decompress(f'{MASK_VALID_PATH}/image_{i}.pkl')\n","    true_img = decompress(f'{MASK_VALID_PATH}/true_{i}.pkl')\n","    pred_img = decompress(f'{MASK_VALID_PATH}/pred_{i}.pkl')\n","\n","    f, axs = plt.subplots(3, 5, figsize=(15, 9))\n","    try:\n","        pred_img += 0.1\n","    except:\n","        asdf = 0\n","    pred_img = pred_img.astype(np.uint8)\n","    axs[0,0].imshow(origin_img[0,:,64,:])  \n","    for j in range(0, 5):              \n","        axs[1,j].imshow(true_img[j,:,64,:])\n","        axs[2,j].imshow(pred_img[j,:,64,:])\n","    del true_img, pred_img\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
