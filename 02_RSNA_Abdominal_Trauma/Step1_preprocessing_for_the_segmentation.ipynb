{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This is the code for the preprocessing of the 3d images with mask labels.  \n",
    "So only apply preprocessing for the images that have labels (206 samples among about 3700 total samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "from glob import glob\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import pydicom\n",
    "import dicomsdl\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import gzip\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from multiprocessing import Pool\n",
    "import nibabel as nib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_PATH  = '/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection'\n",
    "TRAIN_PATH = f'{BASE_PATH}/train_images'\n",
    "DATA_PATH = f'{BASE_PATH}/3d_preprocessed'\n",
    "\n",
    "if not os.path.isdir(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "\n",
    "RESOL = [160, 160, 160]\n",
    "N_FOLDS  = 5\n",
    "N_PREPROCESS_CHUNKS = 12\n",
    "\n",
    "PREPROC_NORM_OR_STD = False # True: normalization, False: standardization\n",
    "\n",
    "train_df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "train_df = train_df.sort_values(by=['patient_id'])\n",
    "\n",
    "# Mask related parameters\n",
    "# Order 1: Bowel, 2: left kidney, 3: right kidney, 4: liver, 5: spleen\n",
    "MASK_ORDER = [5, 3, 4, 1, 2]\n",
    "\n",
    "BASE_PATH = '/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection'\n",
    "MASK_SAVE_PATH = f'{BASE_PATH}/mask_preprocessed'\n",
    "\n",
    "if not os.path.isdir(MASK_SAVE_PATH):\n",
    "    os.mkdir(MASK_SAVE_PATH)\n",
    "    \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path definition\n",
    "## For the 3D images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "train_meta = pd.read_csv(f'{BASE_PATH}/train_series_meta.csv')\n",
    "train_df = train_df.sort_values(by=['patient_id'])\n",
    "train_df\n",
    "\n",
    "TRAIN_PATH = BASE_PATH + \"/train_images/\"\n",
    "n_chunk = 8\n",
    "patients = os.listdir(TRAIN_PATH)\n",
    "n_patients = len(patients)\n",
    "rng_patients = np.linspace(0, n_patients+1, n_chunk+1, dtype = int)\n",
    "patients_cts = glob(f'{TRAIN_PATH}/*/*')\n",
    "n_cts = len(patients_cts)\n",
    "patients_cts_arr = np.zeros((n_cts, 2), int)\n",
    "data_paths=[]\n",
    "for i in range(0, n_cts):\n",
    "    patient, ct = patients_cts[i].split('/')[-2:]\n",
    "    patients_cts_arr[i] = patient, ct\n",
    "    data_paths.append(f'{BASE_PATH}/3d_preprocessed/{patients_cts_arr[i,0]}_{patients_cts_arr[i,1]}.pkl')\n",
    "TRAIN_IMG_PATH = BASE_PATH + '/processed' \n",
    "\n",
    "#Generate tables for training\n",
    "train_meta_df = pd.DataFrame(patients_cts_arr, columns = ['patient_id', 'series'])\n",
    "\n",
    "#5-fold splitting\n",
    "train_df['fold'] = 0\n",
    "labels = train_df[['bowel_healthy','bowel_injury',\n",
    "                    'extravasation_healthy','extravasation_injury',\n",
    "                    'kidney_healthy','kidney_low','kidney_high',\n",
    "                    'liver_healthy','liver_low','liver_high',\n",
    "                    'spleen_healthy','spleen_low','spleen_high',\n",
    "                    'any_injury']].to_numpy()\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=0)\n",
    "counter = 0\n",
    "for train_index, test_index in mskf.split(np.ones(len(train_df)), labels):\n",
    "    for i in range(0, len(test_index)):\n",
    "        train_df['fold'][test_index[i]] = counter\n",
    "    counter+=1\n",
    "\n",
    "train_meta_df = train_meta_df.join(train_df.set_index('patient_id'), on='patient_id')\n",
    "train_meta_df['path']=data_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:11<00:00, 18.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#Segmentation mask part\n",
    "img_list = glob(f'{BASE_PATH}/train_images/*/*')\n",
    "\n",
    "#To know which mask belongs to which patients\n",
    "series_to_patient_dict = {}\n",
    "for i in range(0, len(img_list)):\n",
    "    tmp = img_list[i].split('/')\n",
    "    series_to_patient_dict[int(tmp[-1])] = int(tmp[-2])\n",
    "\n",
    "seg_path_list = glob(f'{BASE_PATH}/segmentations/*')\n",
    "seg_info_arr = np.zeros((len(seg_path_list), 2), int)\n",
    "for i in range(0, len(seg_path_list)):\n",
    "    series  = int(seg_path_list[i].split('/')[-1][:-4])\n",
    "    patient = series_to_patient_dict[series]\n",
    "    seg_info_arr[i,0] = patient\n",
    "    seg_info_arr[i,1] = series\n",
    "\n",
    "seg_info_df = pd.DataFrame(seg_info_arr, columns = ['patient_id', 'series'])\n",
    "seg_info_df['mask_path'] = ''\n",
    "mask_paths = []\n",
    "for i in range(0, len(seg_info_df)):\n",
    "    row = seg_info_df.iloc[i]\n",
    "    patient_id = row['patient_id']\n",
    "    series = row['series']\n",
    "    mask_paths.append(f'{MASK_SAVE_PATH}/{patient_id}_{series}.pkl')\n",
    "seg_info_df['mask_path'] = mask_paths\n",
    "\n",
    "#train_meta_df = pd.read_csv(f'{BASE_PATH}/train_meta.csv')\n",
    "seg_info_df['img_path'] = ''\n",
    "img_paths = []\n",
    "for i in tqdm(range(0, len(seg_info_df))):\n",
    "    row = seg_info_df.iloc[i]\n",
    "    patient_id = row['patient_id']\n",
    "    series     = row['series']\n",
    "    train_img_path = train_meta_df.loc[(train_meta_df['patient_id']==patient_id)&(train_meta_df['series']==series), 'path']\n",
    "    img_paths.append(train_img_path.iloc[0])\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "seg_info_df['img_path'] = img_paths\n",
    "seg_info_df.to_csv(f'{BASE_PATH}/seg_info.csv', index = False)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess input dcm slices to 3d images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(name, data):\n",
    "    with gzip.open(name, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def decompress(name):\n",
    "    with gzip.open(name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns GPU array\n",
    "def standardize_pixel_array(pixel_array, dcm_rows):\n",
    "    \"\"\"\n",
    "    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n",
    "    \"\"\"\n",
    "    # Correct DICOM pixel_array if PixelRepresentation == 1.\n",
    "    for z in range(0, len(pixel_array)):\n",
    "        if int(dcm_rows[z]['PixelRepresentation']) == 1:\n",
    "            bit_shift = dcm_rows[z]['BitsAllocated'] - dcm_rows[z]['BitsStored']\n",
    "            dtype = pixel_array[z].dtype \n",
    "            pixel_array[z] = (pixel_array[z] << bit_shift).astype(dtype) >>  bit_shift\n",
    "\n",
    "    pixel_array = torch.from_numpy(pixel_array.astype(np.float16)).to(DEVICE).to(torch.float16)    \n",
    "\n",
    "    for z in range(0, len(pixel_array)):\n",
    "        intercept = float(dcm_rows[z]['RescaleIntercept'])\n",
    "        slope = float(dcm_rows[z]['RescaleSlope'])\n",
    "        center = int(dcm_rows[z]['WindowCenter'])\n",
    "        width = int(dcm_rows[z]['WindowWidth'])\n",
    "        low = center - width / 2\n",
    "        high = center + width / 2    \n",
    "        \n",
    "        pixel_array[z] = (pixel_array[z] * slope) + intercept\n",
    "        pixel_array[z] = torch.clip(pixel_array[z], low, high)\n",
    "        \n",
    "    gc.collect()    \n",
    "    return pixel_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_norm_or_std(data, resize_shape, is_norm = PREPROC_NORM_OR_STD):  \n",
    "    #resize xy\n",
    "    data = transforms.Resize((int(resize_shape[1]), int(resize_shape[2])), antialias = True)(data)\n",
    "    \n",
    "    #zyx to xzy\n",
    "    data = torch.permute(data, (2, 0, 1))\n",
    "    #Resize yz\n",
    "    data = transforms.Resize((int(resize_shape[0]), int(resize_shape[1])), antialias = True)(data)\n",
    "    #xzy to zyx\n",
    "    data = torch.permute(data, (1, 2, 0))\n",
    "\n",
    "    if is_norm:\n",
    "        bottom = torch.min(data)\n",
    "        data -= bottom\n",
    "        top    = torch.max(data)\n",
    "        data/=top\n",
    "        del top, bottom\n",
    "    else:\n",
    "        avg = torch.mean(data, (0, 1, 2))\n",
    "        std = torch.std(data, (0, 1, 2))\n",
    "        data = (data-avg)/std\n",
    "        del avg, std\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return data\n",
    "\n",
    "# Read each slice and stack them to make 3d data\n",
    "def process_3d(save_path, data_path = TRAIN_PATH):\n",
    "    tmp = save_path.split('/')[-1][:-4]\n",
    "    tmp = tmp.split('_')\n",
    "    patient, study = int(tmp[0]), int(tmp[1])\n",
    "    imgs = {}    \n",
    "    \n",
    "    # To load only needed slices\n",
    "    imgs = {}    \n",
    "    for f in sorted(glob(data_path + f'/{patient}/{study}/*.dcm')):  \n",
    "        pos_z = -int((f.split('/')[-1])[:-4])\n",
    "        imgs[pos_z] = f\n",
    "        \n",
    "    sample_z = np.linspace(0, len(imgs)-1, RESOL, dtype=int)\n",
    "    dcm_rows = []\n",
    "    imgs_3d  = []\n",
    "    for i, k in enumerate(sorted(imgs.keys())):\n",
    "        if not np.isin([i], sample_z)[0]:\n",
    "            continue        \n",
    "        f= imgs[k]\n",
    "        opened_dicom = dicomsdl.open(f)\n",
    "        img = opened_dicom.pixelData(storedvalue=True)\n",
    "        params = opened_dicom.getPixelDataInfo()\n",
    "        \n",
    "        imgs_3d.append(img[None])\n",
    "        dcm_rows.append(params)\n",
    "\n",
    "    imgs_3d = np.vstack(imgs_3d)\n",
    "    imgs_3d = standardize_pixel_array(imgs_3d, dcm_rows)\n",
    "    \n",
    "    min_imgs = torch.min(imgs_3d)\n",
    "    max_imgs = torch.max(imgs_3d)\n",
    "        \n",
    "    imgs_3d = ((imgs_3d - min_imgs) / (max_imgs - min_imgs + 1e-6))\n",
    "\n",
    "    if str(dcm_rows[0]['PhotometricInterpretation']) == \"MONOCHROME1\":\n",
    "        imgs_3d = 1.0 - imgs_3d\n",
    "\n",
    "    imgs_3d = resize_norm_or_std(imgs_3d, opt_shape).to('cpu')\n",
    "\n",
    "    #Save the image\n",
    "    compress(save_path, imgs_3d)                      \n",
    "\n",
    "    del imgs, img, imgs_3d\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "rng_samples = np.linspace(0, len(train_meta_df), N_PREPROCESS_CHUNKS+1, dtype = int)\n",
    "def process_3d_wrapper(process_ind, rng_samples = rng_samples, df = train_meta_df):\n",
    "    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n",
    "        process_3d(df.iloc[i]['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4711 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb Cell 17\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_3d_wrapper\u001b[39m(process_ind, rng_samples \u001b[39m=\u001b[39m rng_samples, df \u001b[39m=\u001b[39m train_meta_df):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(rng_samples[process_ind], rng_samples[process_ind\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         process_3d(df\u001b[39m.\u001b[39miloc[i][\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb Cell 17\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     dcm_rows\u001b[39m.\u001b[39mappend(params)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m imgs_3d \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack(imgs_3d)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m imgs_3d \u001b[39m=\u001b[39m standardize_pixel_array(imgs_3d, dcm_rows)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m min_imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmin(imgs_3d)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m max_imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(imgs_3d)\n",
      "\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     low \u001b[39m=\u001b[39m center \u001b[39m-\u001b[39m width \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     high \u001b[39m=\u001b[39m center \u001b[39m+\u001b[39m width \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m    \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     pixel_array[z] \u001b[39m=\u001b[39m (pixel_array[z] \u001b[39m*\u001b[39m slope) \u001b[39m+\u001b[39m intercept\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     pixel_array[z] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclip(pixel_array[z], low, high)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_preprocessing_for_the_segmentation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()    \n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Parallel(n_jobs = N_PREPROCESS_CHUNKS)(delayed(process_3d_wrapper)(i) for i in range(N_PREPROCESS_CHUNKS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess masks to 3d dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3D_segmentations(filepath, downsample_rate=2):\n",
    "    img = nib.load(filepath).get_fdata()\n",
    "    img = np.transpose(img, [1, 0, 2])\n",
    "    img = np.rot90(img, 1, (1,2))\n",
    "    img = img[::-1,:,:]\n",
    "    img = np.transpose(img, [1, 0, 2])\n",
    "    img = np.flip(img, 0)\n",
    "\n",
    "    if(len(img)>256):\n",
    "        img = img[::downsample_rate, ::downsample_rate, ::downsample_rate]\n",
    "    else:\n",
    "        img = img[:,::downsample_rate, ::downsample_rate]\n",
    "    return img\n",
    "\n",
    "def img_to_masks(img, mask_order  = MASK_ORDER):\n",
    "    imgs_stack = []    \n",
    "    for i in range(0, len(mask_order)):\n",
    "        one_mask = (img==mask_order[i]).astype(np.uint8)\n",
    "        imgs_stack.append(one_mask[None])\n",
    "\n",
    "    imgs_stack = np.vstack(imgs_stack)    \n",
    "    return imgs_stack\n",
    "\n",
    "def resize_3d(img, resize_shape = RESOL):\n",
    "    imgs_stack = []\n",
    "    for i in range(0, len(img)):\n",
    "        imgs_stack.append(cv2.resize(img[i], (resize_shape[2], resize_shape[1]))[None])\n",
    "    imgs_stack = np.vstack(imgs_stack)\n",
    "\n",
    "    resized_img = np.zeros(resize_shape, np.uint8)\n",
    "    for i in range(0, len(imgs_stack[0,0])):\n",
    "        resized_img[:,:,i] = cv2.resize(imgs_stack[:,:,i], (resize_shape[1], resize_shape[0]))\n",
    "    del imgs_stack\n",
    "    gc.collect()\n",
    "    return resized_img\n",
    "\n",
    "def process_mask(path, resize_shape = RESOL):\n",
    "    series = path.split('/')[-1].split('_')[-1][:-4]\n",
    "    origin_mask_path = f'{BASE_PATH}/segmentations/{series}.nii'\n",
    "    img_3d = create_3D_segmentations(origin_mask_path)\n",
    "    mask_3d = img_to_masks(img_3d)    \n",
    "    resized_mask_3d = np.zeros((len(mask_3d), resize_shape[0], resize_shape[1], resize_shape[2]), np.uint8)\n",
    "    for i in range(0, len(MASK_ORDER)):\n",
    "        resized_mask_3d[i] = resize_3d(mask_3d[i])\n",
    "    compress(path, resized_mask_3d)\n",
    "    del img_3d, mask_3d, resized_mask_3d\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "rng_samples = np.linspace(0, len(seg_info_df), N_PREPROCESS_CHUNKS+1, dtype = int)\n",
    "def process_3d_wrapper(process_ind, rng_samples = rng_samples, seg_info_df = seg_info_df):\n",
    "    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n",
    "        process_mask(seg_info_df.iloc[i]['mask_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Parallel(n_jobs = N_PREPROCESS_CHUNKS)(delayed(process_3d_wrapper)(i) for i in range(N_PREPROCESS_CHUNKS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_info_df = pd.read_csv(f'{BASE_PATH}/seg_info.csv')\n",
    "\n",
    "for i in range(0, len(seg_info_df)):\n",
    "    if (i !=100):\n",
    "        continue\n",
    "    row = seg_info_df.iloc[i]\n",
    "    img_3d = decompress(row['img_path'])\n",
    "    mask   = decompress(row['mask_path'])\n",
    "    f, axs = plt.subplots(1, 6, figsize=(18, 3))\n",
    "    plt.title(f'Ind {i}')\n",
    "    axs[0].imshow(img_3d[:,64,:])\n",
    "    for j in range(0, 5):\n",
    "        axs[j+1].imshow(mask[j,:,64,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.06",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
