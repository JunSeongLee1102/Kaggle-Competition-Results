{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Modification of the original code\n","I just took the code of the winner of the RSNA 2022 cervical spine fracture detection competition.  \n","Link: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage1"]},{"cell_type":"markdown","metadata":{},"source":["# 1st Place Solution Training 3D Semantic Segmentation (Stage1)\n","\n","Hi all,\n","\n","I'm very exciting to writing this notebook and the summary of our solution here.\n","\n","This is FULL version of training my final models (stage1), using resnet18d as backbone, unet as decoder and using 128x128x128 as input.\n","\n","NOTE: **You need to run this code locally because the RAM is not enough here.**\n","\n","NOTE2: **It is highly recommended to pre-process the 3D semantic segmentation training data first and save it locally, which can greatly speed up the loading of the data.**\n","\n","My brief summary of winning solution: https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/362607\n","\n","* Train Stage1 Notebook: This notebook\n","* Train Stage2 (Type1) Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage2-type1\n","* Train Stage2 (Type2) Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage2-type2\n","* Inference Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-inference\n","\n","**If you find these notebooks helpful please upvote. Thanks! **"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T05:59:39.369783Z","iopub.status.busy":"2022-10-29T05:59:39.369067Z","iopub.status.idle":"2022-10-29T06:00:25.730464Z","shell.execute_reply":"2022-10-29T06:00:25.729271Z","shell.execute_reply.started":"2022-10-29T05:59:39.369678Z"},"trusted":true},"outputs":[],"source":["#!pip -q install monai\n","#!pip -q install segmentation-models-pytorch==0.2.1"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-10-29T06:00:25.733795Z","iopub.status.busy":"2022-10-29T06:00:25.733350Z","iopub.status.idle":"2022-10-29T06:00:25.741619Z","shell.execute_reply":"2022-10-29T06:00:25.740579Z","shell.execute_reply.started":"2022-10-29T06:00:25.733741Z"},"trusted":true},"outputs":[],"source":["DEBUG = False\n","\n","import os\n","import sys\n","#sys.path = [\n","#    '../input/covn3d-same',\n","#] + sys.path"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:25.743219Z","iopub.status.busy":"2022-10-29T06:00:25.742950Z","iopub.status.idle":"2022-10-29T06:00:34.162024Z","shell.execute_reply":"2022-10-29T06:00:34.160905Z","shell.execute_reply.started":"2022-10-29T06:00:25.743184Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import gc\n","import ast\n","import cv2\n","import time\n","import timm\n","import pickle\n","import random\n","import pydicom\n","import dicomsdl\n","import argparse\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import glob\n","import nibabel as nib\n","from PIL import Image\n","from tqdm import tqdm\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gzip\n","from compress import Compressor\n","import pickle\n","from joblib import Parallel, delayed\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.cuda.amp as amp\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","\n","from monai.transforms import Resize\n","import  monai.transforms as transforms\n","\n","%matplotlib inline\n","rcParams['figure.figsize'] = 20, 8\n","device = torch.device('cuda')\n","torch.backends.cudnn.benchmark = True\n","\n","sys.path.append('./lib_models')"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:29.485211Z","iopub.status.busy":"2022-10-29T06:03:29.484820Z","iopub.status.idle":"2022-10-29T06:03:29.494317Z","shell.execute_reply":"2022-10-29T06:03:29.493294Z","shell.execute_reply.started":"2022-10-29T06:03:29.485168Z"},"trusted":true},"outputs":[],"source":["RESOL = 128\n","\n","BASE_PATH = '/home/junseonglee/01_codes/input/rsna-2023-abdominal-trauma-detection'\n","MASK_SAVE_PATH = f'{BASE_PATH}/mask_preprocessed'\n","MASK_VALID_PATH = f'{BASE_PATH}/mask_validation'\n","TRAIN_PATH = f'{BASE_PATH}/train_images'\n","N_PREPROCESS_CHUNKS = 24\n","\n","kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n","load_kernel = None\n","load_last = True\n","n_blocks = 4\n","n_folds = 5\n","backbone = 'resnet18d'\n","\n","image_sizes = [128, 128, 128]\n","R = Resize(image_sizes)\n","\n","init_lr = 3e-3\n","batch_size = 4\n","drop_rate = 0.\n","drop_path_rate = 0.\n","loss_weights = [1, 1]\n","p_mixup = 0.1\n","\n","data_dir = '../input/rsna-2022-cervical-spine-fracture-detection'\n","use_amp = True\n","num_workers = 8\n","out_dim = 5\n","\n","n_epochs = 1000\n","\n","log_dir = f'{BASE_PATH}/seg_models_backup'\n","model_dir = f'{BASE_PATH}/seg_models_backup'\n","seg_inference_dir = f'{BASE_PATH}/seg_infer_results'\n","cropped_img_dir   = f'{BASE_PATH}/3d_preprocessed_crop'\n","os.makedirs(log_dir, exist_ok=True)\n","os.makedirs(model_dir, exist_ok=True)\n","os.makedirs(MASK_VALID_PATH, exist_ok=True)\n","os.makedirs(seg_inference_dir, exist_ok = True)\n","os.makedirs(cropped_img_dir, exist_ok = True)\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.390438Z","iopub.status.busy":"2022-10-29T06:00:34.389599Z","iopub.status.idle":"2022-10-29T06:00:34.402322Z","shell.execute_reply":"2022-10-29T06:00:34.401400Z","shell.execute_reply.started":"2022-10-29T06:00:34.390400Z"},"trusted":true},"outputs":[],"source":["transforms_valid = transforms.Compose([\n","])"]},{"cell_type":"markdown","metadata":{},"source":["# DataFrame"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.404497Z","iopub.status.busy":"2022-10-29T06:00:34.403952Z","iopub.status.idle":"2022-10-29T06:00:34.486417Z","shell.execute_reply":"2022-10-29T06:00:34.485281Z","shell.execute_reply.started":"2022-10-29T06:00:34.404459Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv(f'{BASE_PATH}/train_meta.csv')\n","mask_paths = []\n","cropped_paths = []\n","for i in range(0, len(df_train)):\n","    row = df_train.iloc[i]\n","    file_name = row['path'].split('/')[-1]\n","    mask_paths.append(f'{seg_inference_dir}/{file_name}')\n","    cropped_paths.append(f'{cropped_img_dir}/{file_name}')\n","df_train['mask_path'] = mask_paths\n","df_train['cropped_path'] = cropped_paths\n","df_train.tail()\n","\n","df_train.to_csv(f'{BASE_PATH}/train_meta.csv', index = False)"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.497353Z","iopub.status.busy":"2022-10-29T06:00:34.496904Z","iopub.status.idle":"2022-10-29T06:00:34.514198Z","shell.execute_reply":"2022-10-29T06:00:34.513162Z","shell.execute_reply.started":"2022-10-29T06:00:34.497317Z"},"trusted":true},"outputs":[],"source":["def compress(name, data):\n","    with gzip.open(name, 'wb') as f:\n","        pickle.dump(data, f)\n","\n","def decompress(name):\n","    with gzip.open(name, 'rb') as f:\n","        data = pickle.load(f)\n","    return data\n","\n","def compress_fast(name, data):\n","    np.savez_compressed(name, a=data)\n","\n","\n","def decompress_fast(name):\n","    data = np.load(f'{name}.npz')['a']    \n","    return data\n","\n","def save_pickle(name, data):\n","    with open(name, 'wb') as f:\n","        pickle.dump(data, f)\n","\n","def load_pickle(name):\n","    with open(name, 'rb') as f:\n","        data = pickle.load(f)\n","    return data    \n","\n","def load_sample(row, has_mask=True):\n","    image = decompress(row['img_path'])[None]\n","\n","    if has_mask:\n","        mask = load_pickle(row['mask_path'])\n","        \n","        return image, mask\n","    else:\n","        return image\n","\n","\n","\n","class SEGDataset(Dataset):\n","    def __init__(self, df, mode, transform):\n","\n","        self.df = df.reset_index()\n","        self.mode = mode\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        \n","        image = decompress(row['path'])[None]\n","        image = torch.from_numpy(image).to(torch.float32)\n","        #file_name = row['path'].split('/')[-1]\n","        #save_path = f'{seg_inference_dir}/{file_name}'\n","        save_train = row['mask_path']\n","\n","        return image, save_path\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.059903Z","iopub.status.busy":"2022-10-29T06:02:59.058663Z","iopub.status.idle":"2022-10-29T06:02:59.070787Z","shell.execute_reply":"2022-10-29T06:02:59.069705Z","shell.execute_reply.started":"2022-10-29T06:02:59.059860Z"},"trusted":true},"outputs":[],"source":["class TimmSegModel(nn.Module):\n","    def __init__(self, backbone, segtype='unet', pretrained=False):\n","        super(TimmSegModel, self).__init__()\n","\n","        self.encoder = timm.create_model(\n","            backbone,\n","            in_chans=1,\n","            features_only=True,\n","            drop_rate=drop_rate,\n","            drop_path_rate=drop_path_rate,\n","            pretrained=pretrained\n","        )\n","        g = self.encoder(torch.rand(1, 1, 64, 64))\n","        encoder_channels = [1] + [_.shape[1] for _ in g]\n","        decoder_channels = [256, 128, 64, 32, 16]\n","        if segtype == 'unet':\n","            self.decoder = smp.unet.decoder.UnetDecoder(\n","                encoder_channels=encoder_channels[:n_blocks+1],\n","                decoder_channels=decoder_channels[:n_blocks],\n","                n_blocks=n_blocks,\n","            )\n","\n","        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","    def forward(self,x):\n","        global_features = [0] + self.encoder(x)[:n_blocks]\n","        seg_features = self.decoder(*global_features)\n","        seg_features = self.segmentation_head(seg_features)\n","        return seg_features"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.073051Z","iopub.status.busy":"2022-10-29T06:02:59.072642Z","iopub.status.idle":"2022-10-29T06:03:13.720715Z","shell.execute_reply":"2022-10-29T06:03:13.719595Z","shell.execute_reply.started":"2022-10-29T06:02:59.073017Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 5, 128, 128, 128])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from timm.models.layers.conv2d_same import Conv2dSame\n","from conv3d_same import Conv3dSame\n","\n","\n","def convert_3d(module):\n","\n","    module_output = module\n","    if isinstance(module, torch.nn.BatchNorm2d):\n","        module_output = torch.nn.BatchNorm3d(\n","            module.num_features,\n","            module.eps,\n","            module.momentum,\n","            module.affine,\n","            module.track_running_stats,\n","        )\n","        if module.affine:\n","            with torch.no_grad():\n","                module_output.weight = module.weight\n","                module_output.bias = module.bias\n","        module_output.running_mean = module.running_mean\n","        module_output.running_var = module.running_var\n","        module_output.num_batches_tracked = module.num_batches_tracked\n","        if hasattr(module, \"qconfig\"):\n","            module_output.qconfig = module.qconfig\n","            \n","    elif isinstance(module, Conv2dSame):\n","        module_output = Conv3dSame(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.Conv2d):\n","        module_output = torch.nn.Conv3d(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","            padding_mode=module.padding_mode\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.MaxPool2d):\n","        module_output = torch.nn.MaxPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            dilation=module.dilation,\n","            ceil_mode=module.ceil_mode,\n","        )\n","    elif isinstance(module, torch.nn.AvgPool2d):\n","        module_output = torch.nn.AvgPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            ceil_mode=module.ceil_mode,\n","        )\n","\n","    for name, child in module.named_children():\n","        module_output.add_module(\n","            name, convert_3d(child)\n","        )\n","    del module\n","\n","    return module_output\n","\n","\n","m = TimmSegModel(backbone)\n","m = convert_3d(m)\n","m(torch.rand(1, 1, 128,128,128)).shape"]},{"cell_type":"markdown","metadata":{},"source":["# Valid func"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.738907Z","iopub.status.busy":"2022-10-29T06:03:13.738511Z","iopub.status.idle":"2022-10-29T06:03:13.755925Z","shell.execute_reply":"2022-10-29T06:03:13.755044Z","shell.execute_reply.started":"2022-10-29T06:03:13.738871Z"},"trusted":true},"outputs":[],"source":["def infer_func(model, loader_valid):\n","    model.eval()\n","    valid_loss = []\n","    outputs = []\n","    ths = [0.2]\n","    batch_metrics = [[]]\n","    bar = tqdm(loader_valid)\n","\n","    counter = 0\n","    \n","    with torch.no_grad():\n","        for images, save_paths in bar:\n","            images = images.cuda()\n","\n","            logits = model(images)\n","\n","            for thi, th in enumerate(ths):\n","                pred = (logits.sigmoid() > th).float().detach()\n","                for i in range(logits.shape[0]):                    \n","                    y_pred = (logits[i].sigmoid().cpu().numpy()+0.1).astype(np.uint8)\n","                    compress(save_paths[i], y_pred)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.979257Z","iopub.status.busy":"2022-10-29T06:03:13.978679Z","iopub.status.idle":"2022-10-29T06:03:13.992437Z","shell.execute_reply":"2022-10-29T06:03:13.991378Z","shell.execute_reply.started":"2022-10-29T06:03:13.979202Z"},"trusted":true},"outputs":[],"source":["def run(fold):\n","\n","    log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n","    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n","\n","\n","    dataset_train = SEGDataset(df_train, 'valid', transform=transforms_valid)\n","    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","    model = TimmSegModel(backbone, pretrained=True)\n","    model = convert_3d(model)\n","\n","    model.load_state_dict(torch.load(model_file))\n","    model = model.to(device)\n","\n","    print(len(dataset_train))\n","\n","    infer_func(model, loader_train)\n","\n","    del model\n","    torch.cuda.empty_cache()\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:33.006143Z","iopub.status.busy":"2022-10-29T06:03:33.005515Z"},"trusted":true},"outputs":[],"source":["#run(0)"]},{"cell_type":"markdown","metadata":{},"source":["# Postprocss to get crop regions"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["#The order of the crop region data format\n","#Z start/end, Y start/end, X start/end for each mask channels + total region for the extravasation prediction\n","def calc_crop_region(mask):\n","    crop_range = np.zeros((6, 6))\n","    crop_range[:,::2]=10000\n","    mask_z = np.max(mask, axis = (2, 3)).astype(bool)\n","    mask_y = np.max(mask, axis = (1, 3)).astype(bool)\n","    mask_x = np.max(mask, axis = (1, 2)).astype(bool)\n","    \n","    template_range = np.arange(0, RESOL)\n","\n","    for mi in range(0, 5):\n","        zrange = template_range[mask_z[mi]]\n","        yrange = template_range[mask_y[mi]]\n","        xrange = template_range[mask_x[mi]]\n","        # For incomplete organ\n","        if(len(zrange)==0):\n","            zrange = template_range.copy()\n","            yrange = template_range.copy()\n","            xrange = template_range.copy()\n","\n","        crop_range[mi] = np.min(zrange), np.max(zrange)+1, np.min(yrange), np.max(yrange)+1, np.min(xrange), np.max(xrange)+1\n","\n","    crop_range[5] = np.min(crop_range[:5, 0]), np.max(crop_range[:5, 1]), np.min(crop_range[:5, 2]), \\\n","                    np.max(crop_range[:5, 3]), np.min(crop_range[:5,4]), np.max(crop_range[:5, 5])\n","    \n","    crop_range[:,:2]/=len(mask_z[0])\n","    crop_range[:,2:4]/=len(mask_y[0])\n","    crop_range[:,4:6]/=len(mask_x[0])\n","\n","    # Then make extravasation (# 5 mask) to reference one and convert other mask's crop respective to it\n","    # --> To minimize the loading size due to speed issue.\n","    zmin, rel_zrange = crop_range[5,0], crop_range[5,1]-crop_range[5,0]\n","    ymin, rel_yrange = crop_range[5,2], crop_range[5,3]-crop_range[5,2]\n","    xmin, rel_xrange = crop_range[5,4], crop_range[5,5]-crop_range[5,4]\n","\n","    crop_range[:5,:2] = (crop_range[:5,:2]-zmin)/rel_zrange\n","    crop_range[:5,2:4] = (crop_range[:5,2:4]-ymin)/rel_yrange\n","    crop_range[:5,4:6] = (crop_range[:5,4:6]-xmin)/rel_xrange\n","\n","    return crop_range\n","\n","def crop_resize_avg_and_std_3d(data, region):\n","    shapes = np.shape(data)\n","    region[:2]*=shapes[0]\n","    region[2:4]*=shapes[1]\n","    region[4:6]*=shapes[2]\n","    region = region.astype(int)\n","\n","    cropped = data[region[0]:region[1], region[2]:region[3], region[4]:region[5]]\n","    slices = []\n","    for i in range(0, len(cropped)):\n","        slices.append(cv2.resize(cropped[i], (RESOL, RESOL))[None])\n","    \n","    slices = np.vstack(slices)\n","    \n","    resized_cropped = np.zeros((RESOL, RESOL, RESOL))\n","    for i in range(0, len(slices[0,0])):\n","        resized_cropped[:,:,i] = cv2.resize(slices[:,:,i], (RESOL, RESOL))\n","    \n","    std = np.std(resized_cropped)\n","    avg = np.average(resized_cropped)\n","    resized_cropped = (resized_cropped-avg)/std\n","    resized_cropped = resized_cropped.astype(np.float32)\n","\n","    del cropped, slices\n","    gc.collect()\n","    return resized_cropped\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.         1.         0.         0.92857143 0.08910891 0.94059406]\n"," [0.46728972 0.72897196 0.64285714 0.94285714 0.57425743 0.78217822]\n"," [0.4953271  0.71962617 0.51428571 0.87142857 0.16831683 0.38613861]\n"," [0.55140187 0.95327103 0.01428571 0.94285714 0.         0.69306931]\n"," [0.58878505 0.86915888 0.47142857 1.         0.64356436 1.        ]\n"," [0.1640625  1.         0.2109375  0.7578125  0.1328125  0.921875  ]]\n"]}],"source":["crop_regions = np.zeros((len(df_train), 6))\n","for i in range(0, len(df_train)):\n","    row = df_train.iloc[i]\n","    #file_name = row['path'].split('/')[-1]\n","    #save_path = f'{seg_inference_dir}/{file_name}'\n","    mask = decompress(row['mask_path'])\n","    crop_region = calc_crop_region(mask)\n","    print(crop_region)\n","    #plt.imshow(mask_file[4,:,60,:])\n","    if (i==0):\n","        break\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n","    \"\"\"\n","    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n","    \"\"\"\n","    # Correct DICOM pixel_array if PixelRepresentation == 1.\n","    pixel_array = dcm.pixel_array\n","    pixel_rep = dcm.PixelRepresentation\n","\n","    return pixel_array"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Read each slice and stack them to make 3d data\n","def process_3d_crop(save_path, mask_path, data_path = TRAIN_PATH):\n","    tmp = save_path.split('/')[-1][:-4]\n","    tmp = tmp.split('_')\n","    patient, study = int(tmp[0]), int(tmp[1])\n","    \n","    mask = decompress(mask_path)\n","    crop_regions = calc_crop_region(mask)\n","    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n"," \n","    crop_regions[5] = 0, 1, 0, 1, 0, 1\n","\n","    imgs = {}    \n","    \n","    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):      \n","        pixel_rep = 0\n","        bit_shift = 0\n","        dtype = 0\n","        try:            \n","            dicom = pydicom.dcmread(f)        \n","            img = standardize_pixel_array(dicom)\n","            img_shape = np.shape(img)\n","            xy_crop_range = absolute_crop[2:].copy()   \n","            xy_crop_range[0:2]*=img_shape[0]\n","            xy_crop_range[2:4]*=img_shape[1]            \n","            xy_crop_range = xy_crop_range.astype(int)\n","            img = img.astype(float)\n","            break\n","        except:\n","            continue\n","            \n","    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):\n","        #For the case that some of the image can't be read -> error without this though don't know why  \n","        img = dicomsdl.open(f).pixelData(storedvalue=True)[xy_crop_range[0]:xy_crop_range[1], xy_crop_range[2]:xy_crop_range[3]]\n","        img = img.astype(float)\n","        \n","        #dicom = pydicom.dcmread(f)\n","        #img = standardize_pixel_array(dicom).astype(float)\n","        #ind = int((f.split('/')[-1])[:-4])\n","        pos_z = -int((f.split('/')[-1])[:-4])\n","        imgs[pos_z] = img\n","\n","\n","    #sample_z = np.linspace(0, len(imgs)-1, RESOL, dtype=int)\n","\n","    imgs_3d = []\n","    n_imgs = len(imgs)    \n","    z_crop_range= (absolute_crop[0:2]*n_imgs).astype(int)\n","\n","    #print(z_crop_range)\n","    for i, k in enumerate(sorted(imgs.keys())):\n","        #if i in sample_z:\n","        if(i >= z_crop_range[0] and i < z_crop_range[1]):\n","            img = imgs[k]\n","            imgs_3d.append(img[None])\n","        \n","    imgs_3d = np.vstack(imgs_3d)\n","    imgs_3d = ((imgs_3d - imgs_3d.min()) / (imgs_3d.max() - imgs_3d.min()))\n","\n","    if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n","        imgs_3d = 1.0 - imgs_3d\n","\n","    #Loaded original imgs_3d    \n","    processed_img_3d = np.zeros((6, RESOL, RESOL, RESOL))\n","\n","    for i in range(0, 6):     \n","        #To deal with almost not detected slices\n","        try:   \n","            processed_img_3d[i] = crop_resize_avg_and_std_3d(imgs_3d, crop_regions[i])\n","        except:\n","            processed_img_3d[i] = crop_resize_avg_and_std_3d(imgs_3d, np.array([0, 1, 0, 1, 0, 1]))\n","\n","    processed_img_3d = processed_img_3d.reshape(6*RESOL*RESOL*RESOL)\n","    #here to\n","    #gzip too slow maybe I should divide the inference process to chunks or do not save in the inference notebooks\n","    compress_fast(save_path, processed_img_3d)                      \n","\n","    del imgs, img\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Preprocess dataset\n","rng_samples = np.linspace(0, len(df_train), N_PREPROCESS_CHUNKS+1, dtype = int)\n","def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = df_train):\n","    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n","        if not os.path.isfile(f\"{df_train.iloc[i]['cropped_path']}.npz\"):\n","            process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'])"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [00:00<00:00, 26171.00it/s]\n","100%|██████████| 196/196 [00:00<00:00, 22757.27it/s]\n","100%|██████████| 196/196 [00:00<00:00, 27880.47it/s]\n","100%|██████████| 196/196 [00:00<00:00, 29056.08it/s]\n","100%|██████████| 196/196 [00:00<00:00, 27891.82it/s]\n","100%|██████████| 196/196 [00:00<00:00, 48292.52it/s]\n","100%|██████████| 197/197 [00:00<00:00, 22827.25it/s]\n","100%|██████████| 196/196 [00:00<00:00, 23046.27it/s]\n","100%|██████████| 196/196 [00:00<00:00, 47140.52it/s]\n","100%|██████████| 197/197 [00:00<00:00, 30729.22it/s]\n","100%|██████████| 197/197 [00:00<00:00, 46132.43it/s]\n","100%|██████████| 196/196 [00:00<00:00, 46775.74it/s]\n","100%|██████████| 196/196 [00:00<00:00, 22443.52it/s]\n","100%|██████████| 196/196 [00:00<00:00, 22866.78it/s]\n","100%|██████████| 197/197 [00:00<00:00, 22889.85it/s]\n","100%|██████████| 196/196 [00:00<00:00, 45737.38it/s]\n","100%|██████████| 197/197 [00:00<00:00, 23043.70it/s]\n","100%|██████████| 196/196 [00:00<00:00, 40891.54it/s]\n","100%|██████████| 196/196 [00:00<00:00, 45333.83it/s]\n","100%|██████████| 197/197 [00:05<00:00, 33.02it/s]\n","100%|██████████| 196/196 [00:11<00:00, 17.00it/s]\n","100%|██████████| 196/196 [00:24<00:00,  8.10it/s]\n","100%|██████████| 196/196 [00:30<00:00,  6.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 170 ms, sys: 333 ms, total: 503 ms\n","Wall time: 40.3 s\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 197/197 [00:39<00:00,  4.97it/s]\n"]},{"data":{"text/plain":["[None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None,\n"," None]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","#For debugging\n","#for i in range(0, N_PREPROCESS_CHUNKS):\n"," #   process_3d_wrapper(i)\n","Parallel(n_jobs = N_PREPROCESS_CHUNKS)(delayed(process_3d_wrapper)(i) for i in range(N_PREPROCESS_CHUNKS))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
