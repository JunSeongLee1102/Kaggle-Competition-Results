{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Modification of the original code\n","I just took the code of the winner of the RSNA 2022 cervical spine fracture detection competition.  \n","Link: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:25.743219Z","iopub.status.busy":"2022-10-29T06:00:25.742950Z","iopub.status.idle":"2022-10-29T06:00:34.162024Z","shell.execute_reply":"2022-10-29T06:00:34.160905Z","shell.execute_reply.started":"2022-10-29T06:00:25.743184Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import gc\n","import ast\n","import cv2\n","import imageio\n","import time\n","import timm\n","import pickle\n","import random\n","import pydicom\n","import dicomsdl\n","import argparse\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import glob\n","import nibabel as nib\n","from PIL import Image\n","from tqdm import tqdm\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gzip\n","import pickle\n","from joblib import Parallel, delayed\n","import lz4.frame\n","import mgzip\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.cuda.amp as amp\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","\n","\n","%matplotlib inline\n","rcParams['figure.figsize'] = 20, 8\n","device = torch.device('cuda')\n","torch.backends.cudnn.benchmark = True\n","\n","os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n","sys.path.append('./lib_models')"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:29.485211Z","iopub.status.busy":"2022-10-29T06:03:29.484820Z","iopub.status.idle":"2022-10-29T06:03:29.494317Z","shell.execute_reply":"2022-10-29T06:03:29.493294Z","shell.execute_reply.started":"2022-10-29T06:03:29.485168Z"},"trusted":true},"outputs":[],"source":["RESOL = 128\n","\n","BASE_PATH = '/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection'\n","MASK_SAVE_PATH = f'{BASE_PATH}/mask_preprocessed'\n","MASK_VALID_PATH = f'{BASE_PATH}/mask_validation'\n","TRAIN_PATH = f'{BASE_PATH}/train_images'\n","\n","BATCH_MASK_PRED = 8\n","N_PROCESS_CROP = 12\n","\n","kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n","load_kernel = None\n","load_last = True\n","n_blocks = 4\n","backbone = 'resnet18d'\n","\n","image_sizes = [128, 128, 128]\n","data_dir = '../input/rsna-2022-cervical-spine-fracture-detection'\n","use_amp = True\n","\n","\n","num_workers = 24\n","out_dim = 5\n","\n","model_dir = f'{BASE_PATH}/seg_models_backup'\n","seg_inference_dir = f'{BASE_PATH}/seg_infer_results'\n","cropped_img_dir   = f'{BASE_PATH}/3d_preprocessed_crop_ratio'\n","os.makedirs(model_dir, exist_ok=True)\n","os.makedirs(MASK_VALID_PATH, exist_ok=True)\n","os.makedirs(seg_inference_dir, exist_ok = True)\n","os.makedirs(cropped_img_dir, exist_ok = True)"]},{"cell_type":"markdown","metadata":{},"source":["# Path to save cropped image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.404497Z","iopub.status.busy":"2022-10-29T06:00:34.403952Z","iopub.status.idle":"2022-10-29T06:00:34.486417Z","shell.execute_reply":"2022-10-29T06:00:34.485281Z","shell.execute_reply.started":"2022-10-29T06:00:34.404459Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv(f'{BASE_PATH}/train_meta.csv')\n","mask_paths = []\n","cropped_paths = []\n","for i in range(0, len(df_train)):\n","    row = df_train.iloc[i]\n","    file_name = row['path'].split('/')[-1]\n","    mask_paths.append(f'{seg_inference_dir}/{file_name}')\n","    cropped_paths.append(f'{cropped_img_dir}/{file_name}')\n","df_train['cropped_path'] = cropped_paths\n","df_train['mask_path']    = mask_paths\n","df_train.tail()\n","\n","df_train.to_csv(f'{BASE_PATH}/train_meta.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_train"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.497353Z","iopub.status.busy":"2022-10-29T06:00:34.496904Z","iopub.status.idle":"2022-10-29T06:00:34.514198Z","shell.execute_reply":"2022-10-29T06:00:34.513162Z","shell.execute_reply.started":"2022-10-29T06:00:34.497317Z"},"trusted":true},"outputs":[],"source":["def compress(name, data):\n","    with gzip.open(name, 'wb') as f:\n","        pickle.dump(data, f)\n","\n","def decompress(name):\n","    with gzip.open(name, 'rb') as f:\n","        data = pickle.load(f)\n","    return data\n","\n","def compress_fast(name, data):  \n","    np.save(name, data)\n","\n","def decompress_fast(name):\n","    data = np.load(f'{name}.npy')\n","    return data\n","\n","\n","class SEGDataset(Dataset):\n","    def __init__(self, df, mode):\n","\n","        self.df = df.reset_index()\n","        self.mode = mode\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        \n","        image = decompress(row['path'])[None]\n","        image = torch.from_numpy(image).to(torch.float32)\n","        save_path = row['mask_path']\n","\n","        return image, save_path\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.059903Z","iopub.status.busy":"2022-10-29T06:02:59.058663Z","iopub.status.idle":"2022-10-29T06:02:59.070787Z","shell.execute_reply":"2022-10-29T06:02:59.069705Z","shell.execute_reply.started":"2022-10-29T06:02:59.059860Z"},"trusted":true},"outputs":[],"source":["class TimmSegModel(nn.Module):\n","    def __init__(self, backbone, segtype='unet', pretrained=False):\n","        super(TimmSegModel, self).__init__()\n","\n","        self.encoder = timm.create_model(\n","            backbone,\n","            in_chans=1,\n","            features_only=True,\n","            pretrained=pretrained\n","        )\n","        g = self.encoder(torch.rand(1, 1, 64, 64))\n","        encoder_channels = [1] + [_.shape[1] for _ in g]\n","        decoder_channels = [256, 128, 64, 32, 16]\n","        if segtype == 'unet':\n","            self.decoder = smp.unet.decoder.UnetDecoder(\n","                encoder_channels=encoder_channels[:n_blocks+1],\n","                decoder_channels=decoder_channels[:n_blocks],\n","                n_blocks=n_blocks,\n","            )\n","\n","        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","    def forward(self,x):\n","        global_features = [0] + self.encoder(x)[:n_blocks]\n","        seg_features = self.decoder(*global_features)\n","        seg_features = self.segmentation_head(seg_features)\n","        return seg_features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.073051Z","iopub.status.busy":"2022-10-29T06:02:59.072642Z","iopub.status.idle":"2022-10-29T06:03:13.720715Z","shell.execute_reply":"2022-10-29T06:03:13.719595Z","shell.execute_reply.started":"2022-10-29T06:02:59.073017Z"},"trusted":true},"outputs":[],"source":["from timm.models.layers.conv2d_same import Conv2dSame\n","from conv3d_same import Conv3dSame\n","\n","\n","def convert_3d(module):\n","\n","    module_output = module\n","    if isinstance(module, torch.nn.BatchNorm2d):\n","        module_output = torch.nn.BatchNorm3d(\n","            module.num_features,\n","            module.eps,\n","            module.momentum,\n","            module.affine,\n","            module.track_running_stats,\n","        )\n","        if module.affine:\n","            with torch.no_grad():\n","                module_output.weight = module.weight\n","                module_output.bias = module.bias\n","        module_output.running_mean = module.running_mean\n","        module_output.running_var = module.running_var\n","        module_output.num_batches_tracked = module.num_batches_tracked\n","        if hasattr(module, \"qconfig\"):\n","            module_output.qconfig = module.qconfig\n","            \n","    elif isinstance(module, Conv2dSame):\n","        module_output = Conv3dSame(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.Conv2d):\n","        module_output = torch.nn.Conv3d(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","            padding_mode=module.padding_mode\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.MaxPool2d):\n","        module_output = torch.nn.MaxPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            dilation=module.dilation,\n","            ceil_mode=module.ceil_mode,\n","        )\n","    elif isinstance(module, torch.nn.AvgPool2d):\n","        module_output = torch.nn.AvgPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            ceil_mode=module.ceil_mode,\n","        )\n","\n","    for name, child in module.named_children():\n","        module_output.add_module(\n","            name, convert_3d(child)\n","        )\n","    del module\n","\n","    return module_output\n","\n","m = TimmSegModel(backbone)\n","m = convert_3d(m)\n","m(torch.rand(1, 1, 128,128,128)).shape"]},{"cell_type":"markdown","metadata":{},"source":["# Inference mask"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.738907Z","iopub.status.busy":"2022-10-29T06:03:13.738511Z","iopub.status.idle":"2022-10-29T06:03:13.755925Z","shell.execute_reply":"2022-10-29T06:03:13.755044Z","shell.execute_reply.started":"2022-10-29T06:03:13.738871Z"},"trusted":true},"outputs":[],"source":["def infer_func(model, loader_valid):\n","    model.eval()\n","    valid_loss = []\n","    outputs = []\n","    th = 0.2\n","    batch_metrics = [[]]\n","    bar = tqdm(loader_valid)\n","    \n","    with torch.no_grad():\n","        for images, save_paths in bar:\n","            images = images.cuda()\n","            logits = model(images)\n","            preds = (logits.sigmoid() > 0.2).float().detach().cpu().numpy()\n","            y_preds = (preds+0.1).astype(np.uint8)\n","        \n","            def save_mask(ind, preds = y_preds): \n","                compress(save_paths[ind], preds[ind])\n","            \n","            Parallel(n_jobs = len(y_preds))(delayed(save_mask)(i) for i in range(len(y_preds)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.979257Z","iopub.status.busy":"2022-10-29T06:03:13.978679Z","iopub.status.idle":"2022-10-29T06:03:13.992437Z","shell.execute_reply":"2022-10-29T06:03:13.991378Z","shell.execute_reply.started":"2022-10-29T06:03:13.979202Z"},"trusted":true},"outputs":[],"source":["def run(fold):\n","    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n","    dataset_train = SEGDataset(df_train, 'valid')\n","    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_MASK_PRED, shuffle=False, num_workers=BATCH_MASK_PRED)\n","\n","    model = TimmSegModel(backbone, pretrained=True)\n","    model = convert_3d(model)\n","\n","    model.load_state_dict(torch.load(model_file))\n","    model = model.to(device)\n","\n","    print(len(dataset_train))\n","    \n","    infer_func(model, loader_train)\n","\n","    del model\n","    torch.cuda.empty_cache()\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:33.006143Z","iopub.status.busy":"2022-10-29T06:03:33.005515Z"},"trusted":true},"outputs":[],"source":["#run(0)"]},{"cell_type":"markdown","metadata":{},"source":["# Postprocss to get crop regions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#The order of the crop region data format\n","#Z start/end, Y start/end, X start/end for each mask channels + total region for the extravasation prediction\n","def calc_crop_region(mask):\n","    crop_range = np.zeros((6, 6))\n","    crop_range[:,::2]=10000\n","    mask_z = np.max(mask, axis = (2, 3)).astype(bool)\n","    mask_y = np.max(mask, axis = (1, 3)).astype(bool)\n","    mask_x = np.max(mask, axis = (1, 2)).astype(bool)\n","    \n","    template_range = np.arange(0, RESOL)\n","\n","    for mi in range(0, 5):\n","        zrange = template_range[mask_z[mi]]\n","        yrange = template_range[mask_y[mi]]\n","        xrange = template_range[mask_x[mi]]\n","        # For incomplete organ\n","        if(len(zrange)==0):\n","            zrange = template_range.copy()\n","            yrange = template_range.copy()\n","            xrange = template_range.copy()\n","\n","        crop_range[mi] = np.min(zrange), np.max(zrange)+1, np.min(yrange), np.max(yrange)+1, np.min(xrange), np.max(xrange)+1\n","\n","    crop_range[5] = np.min(crop_range[:5, 0]), np.max(crop_range[:5, 1]), np.min(crop_range[:5, 2]), \\\n","                    np.max(crop_range[:5, 3]), np.min(crop_range[:5,4]), np.max(crop_range[:5, 5])\n","    \n","    crop_range[:,:2]/=len(mask_z[0])\n","    crop_range[:,2:4]/=len(mask_y[0])\n","    crop_range[:,4:6]/=len(mask_x[0])\n","\n","    # Then make extravasation (# 5 mask) to reference one and convert other mask's crop respective to it\n","    # --> To minimize the loading size due to speed issue.\n","    zmin, rel_zrange = crop_range[5,0], crop_range[5,1]-crop_range[5,0]\n","    ymin, rel_yrange = crop_range[5,2], crop_range[5,3]-crop_range[5,2]\n","    xmin, rel_xrange = crop_range[5,4], crop_range[5,5]-crop_range[5,4]\n","\n","    crop_range[:5,:2] = (crop_range[:5,:2]-zmin)/rel_zrange\n","    crop_range[:5,2:4] = (crop_range[:5,2:4]-ymin)/rel_yrange\n","    crop_range[:5,4:6] = (crop_range[:5,4:6]-xmin)/rel_xrange\n","\n","    return crop_range\n","\n","def crop_resize_avg_and_std_3d(data, region, resize_shape):\n","    shapes = np.shape(data)\n","    region[:2]*=shapes[0]\n","    region[2:4]*=shapes[1]\n","    region[4:6]*=shapes[2]\n","    region = region.astype(int)\n","\n","    cropped = data[region[0]:region[1], region[2]:region[3], region[4]:region[5]].copy()   \n","    cropped_shape = np.array(np.shape(cropped)).astype(float)\n","    n_cropped_pixels = cropped_shape[0]*cropped_shape[1]*cropped_shape[2]    \n","    \n","\n","    slices = []\n","    for i in range(0, len(cropped)):\n","        slices.append(cv2.resize(cropped[i], (resize_shape[2], resize_shape[1]))[None])\n","    \n","    slices = np.vstack(slices)\n","    resized_cropped = np.zeros(resize_shape)\n","    for i in range(0, len(slices[0,0])):\n","        resized_cropped[:,:,i] = cv2.resize(slices[:,:,i], (resize_shape[1], resize_shape[0]))\n","\n","        \n","    std = np.std(resized_cropped)\n","    avg = np.average(resized_cropped)\n","    resized_cropped = (resized_cropped-avg)/std\n","    resized_cropped = resized_cropped.astype(np.float32)\n","    \n","    del cropped, slices\n","    gc.collect()\n","    return resized_cropped\n","\n","def standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n","    \"\"\"\n","    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n","    \"\"\"\n","    # Correct DICOM pixel_array if PixelRepresentation == 1.\n","    pixel_array = dcm.pixel_array\n","    if dcm.PixelRepresentation == 1:\n","        bit_shift = dcm.BitsAllocated - dcm.BitsStored\n","        dtype = pixel_array.dtype \n","        pixel_array = (pixel_array << bit_shift).astype(dtype) >>  bit_shift\n","#         pixel_array = pydicom.pixel_data_handlers.util.apply_modality_lut(new_array, dcm)\n","\n","    intercept = float(dcm.RescaleIntercept)\n","    slope = float(dcm.RescaleSlope)\n","    center = int(dcm.WindowCenter)\n","    width = int(dcm.WindowWidth)\n","    low = center - width / 2\n","    high = center + width / 2    \n","    \n","    pixel_array = (pixel_array * slope) + intercept\n","    pixel_array = np.clip(pixel_array, low, high)\n","\n","    return pixel_array\n","\n","# Read each slice and stack them to make 3d data\n","def process_3d_crop(save_path, mask_path, resize_shapes, data_path = TRAIN_PATH):\n","    tmp = save_path.split('/')[-1][:-4]\n","    tmp = tmp.split('_')\n","    patient, study = int(tmp[0]), int(tmp[1])\n","    \n","    mask = decompress(mask_path)\n","    crop_regions = calc_crop_region(mask)\n","    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n","\n","    crop_regions[5] = 0, 1, 0, 1, 0, 1\n","\n","    imgs = {}    \n","    \n","    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):      \n","        try:            \n","            dicom = pydicom.dcmread(f)        \n","            img = standardize_pixel_array(dicom)\n","            img_shape = np.shape(img)\n","            xy_crop_range = absolute_crop[2:].copy()   \n","            xy_crop_range[0:2]*=img_shape[0]\n","            xy_crop_range[2:4]*=img_shape[1]            \n","            xy_crop_range = xy_crop_range.astype(int)\n","            img = img.astype(float)\n","            pos_z = -int((f.split('/')[-1])[:-4])\n","            imgs[pos_z] = img\n","            break\n","        except:\n","            continue\n","            \n","    imgs_3d = []\n","    n_imgs = len(imgs)    \n","    z_crop_range= (absolute_crop[0:2]*n_imgs).astype(int)\n","\n","    #print(z_crop_range)\n","    for i, k in enumerate(sorted(imgs.keys())):\n","        #if i in sample_z:\n","        if(i >= z_crop_range[0] and i < z_crop_range[1]):\n","            img = imgs[k]\n","            imgs_3d.append(img[None])\n","        \n","    imgs_3d = np.vstack(imgs_3d)\n","    imgs_3d = ((imgs_3d - imgs_3d.min()) / (imgs_3d.max() - imgs_3d.min()))\n","\n","    if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n","        imgs_3d = 1.0 - imgs_3d\n","\n","    #Loaded original imgs_3d    \n","    #processed_img_3d = np.zeros((6, RESOL, RESOL, RESOL))\n","    \n","    origin_shape = np.shape(imgs_3d)\n","    for i in range(0, 6):    \n","        #To deal with almost not detected slices\n","        try:   \n","            # To deal with possible noises\n","            if(((crop_regions[i,1]-crop_regions[i,0]) < 5/origin_shape[0]) or \n","                ((crop_regions[i,3]-crop_regions[i,2]) < 5/origin_shape[1]) or \n","                ((crop_regions[i,5]-crop_regions[i,4]) < 5/origin_shape[2])):\n","                dummy_failure_function()\n","            \n","            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, crop_regions[i], resize_shapes[i]).astype(np.float16))\n","            compress_fast(f'{save_path}_{i}', processed_img_3d)      \n","            del processed_img_3d\n","            gc.collect()\n","        except:\n","            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, np.array([0, 1, 0, 1, 0, 1]), resize_shapes[i]).astype(np.float16))\n","            compress_fast(f'{save_path}_{i}', processed_img_3d) \n","            del processed_img_3d\n","            gc.collect()  \n","\n","    del imgs, img\n","    gc.collect()\n","    \n","def calc_size_3d_crop(save_path, mask_path, data_path = TRAIN_PATH):\n","    tmp = save_path.split('/')[-1][:-4]\n","    tmp = tmp.split('_')\n","    patient, study = int(tmp[0]), int(tmp[1])\n","    \n","    mask = decompress(mask_path)\n","    crop_regions = calc_crop_region(mask)\n","    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n","\n","    crop_regions[5] = 0, 1, 0, 1, 0, 1\n","\n","    imgs = {}    \n","    \n","    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):      \n","        try:            \n","            dicom = pydicom.dcmread(f)        \n","            img = standardize_pixel_array(dicom)\n","            img_shape = np.shape(img)\n","            xy_crop_range = absolute_crop[2:].copy()   \n","            xy_crop_range[0:2]*=img_shape[0]\n","            xy_crop_range[2:4]*=img_shape[1]            \n","            xy_crop_range = xy_crop_range.astype(int)\n","            img = img.astype(float)\n","            break\n","        except:\n","            continue\n","    \n","    base_crop_shape = np.zeros(3, float)\n","    base_crop_shape[:] = absolute_crop[1::2] - absolute_crop[0::2]\n","    base_crop_shape[0]*= len(glob.glob(data_path + f'/{patient}/{study}/*.dcm'))\n","    base_crop_shape[1:3] = xy_crop_range[1::2] - xy_crop_range[0::2]\n","    \n","    all_crop_shape = crop_regions[:,1::2] - crop_regions[:,0::2]\n","    for i in range(0, 6):\n","        all_crop_shape[i] *= base_crop_shape\n","    del img\n","    gc.collect()\n","    return all_crop_shape"]},{"cell_type":"markdown","metadata":{},"source":["# Calculate average crop size"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocess dataset\n","rng_samples = np.linspace(0, len(df_train), N_PROCESS_CROP+1, dtype = int)\n","def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = df_train):\n","    partial_crop_shapes = []\n","    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n","        partial_crop_shapes.append(calc_size_3d_crop(df_train.iloc[i]['cropped_path'], df_train.iloc[i]['mask_path'])[None])\n","    partial_crop_shapes = np.vstack(partial_crop_shapes)\n","    return partial_crop_shapes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","all_crop_shapes = Parallel(n_jobs = N_PROCESS_CROP)(delayed(process_3d_wrapper)(i) for i in range(N_PROCESS_CROP))\n","all_crop_shapes = np.vstack(all_crop_shapes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["norm_all_crop_shapes = (all_crop_shapes[:,:,0]*all_crop_shapes[:,:,1]*all_crop_shapes[:,:,2])**(1/3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["normed_all_crop_shapes = all_crop_shapes.copy()\n","for i in range(0, len(df_train)):\n","    for j in range(0, 6):\n","        normed_all_crop_shapes[i,j]/=norm_all_crop_shapes[i, j]\n","\n","avg_normed_all_crop_shapes = np.average(normed_all_crop_shapes, axis = 0)\n","avg_normed_all_crop_shapes*=RESOL\n","avg_normed_all_crop_shapes = avg_normed_all_crop_shapes.astype(int)\n","avg_normed_all_crop_shapes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocess dataset\n","rng_samples = np.linspace(0, len(df_train), N_PROCESS_CROP+1, dtype = int)\n","def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = df_train, resize_shapes = avg_normed_all_crop_shapes):\n","    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n","        if not os.path.isfile(f\"{df_train.iloc[i]['cropped_path']}_0.npy\"):\n","            process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'], resize_shapes)     \n","        \n","        #For error correction\n","        else:            \n","            try:\n","                data = decompress_fast(df_train.iloc[i]['cropped_path'])\n","            except: \n","                process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'], resize_shapes)  \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","Parallel(n_jobs = N_PROCESS_CROP)(delayed(process_3d_wrapper)(i) for i in range(N_PROCESS_CROP))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
