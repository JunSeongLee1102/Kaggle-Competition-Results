{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Modification of the original code\n","I just took the code of the winner of the RSNA 2022 cervical spine fracture detection competition.  \n","Link: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage1"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:25.743219Z","iopub.status.busy":"2022-10-29T06:00:25.742950Z","iopub.status.idle":"2022-10-29T06:00:34.162024Z","shell.execute_reply":"2022-10-29T06:00:34.160905Z","shell.execute_reply.started":"2022-10-29T06:00:25.743184Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/junseonglee/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"data":{"text/plain":["device(type='cuda')"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import sys\n","import gc\n","import ast\n","import cv2\n","import imageio\n","import time\n","import timm\n","import pickle\n","import random\n","import pydicom\n","import dicomsdl\n","import argparse\n","import warnings\n","import numpy as np\n","import cupy as cp\n","import pandas as pd\n","import glob\n","import nibabel as nib\n","from PIL import Image\n","from tqdm import tqdm\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gzip\n","import pickle\n","from joblib import Parallel, delayed\n","import lz4.frame\n","import mgzip\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.cuda.amp as amp\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","\n","\n","%matplotlib inline\n","rcParams['figure.figsize'] = 20, 8\n","device = torch.device('cuda')\n","torch.backends.cudnn.benchmark = True\n","\n","os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n","sys.path.append('./lib_models')\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","DEVICE"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:29.485211Z","iopub.status.busy":"2022-10-29T06:03:29.484820Z","iopub.status.idle":"2022-10-29T06:03:29.494317Z","shell.execute_reply":"2022-10-29T06:03:29.493294Z","shell.execute_reply.started":"2022-10-29T06:03:29.485168Z"},"trusted":true},"outputs":[],"source":["RESOL = 128\n","\n","BASE_PATH = '/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection'\n","MASK_SAVE_PATH = f'{BASE_PATH}/mask_preprocessed'\n","MASK_VALID_PATH = f'{BASE_PATH}/mask_validation'\n","TRAIN_PATH = f'{BASE_PATH}/train_images'\n","\n","BATCH_MASK_PRED = 8\n","N_PROCESS_CROP = 12\n","\n","kernel_type = 'timm3d_res50d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n","load_kernel = None\n","load_last = True\n","n_blocks = 4\n","backbone = 'resnet50d'\n","\n","image_sizes = [128, 128, 128]\n","data_dir = '../input/rsna-2022-cervical-spine-fracture-detection'\n","use_amp = True\n","\n","\n","num_workers = 24\n","out_dim = 5\n","\n","model_dir = f'{BASE_PATH}/seg_models_backup'\n","seg_inference_dir = f'{BASE_PATH}/seg_infer_results'\n","cropped_img_dir   = f'{BASE_PATH}/3d_preprocessed_crop_ratio'\n","os.makedirs(model_dir, exist_ok=True)\n","os.makedirs(MASK_VALID_PATH, exist_ok=True)\n","os.makedirs(seg_inference_dir, exist_ok = True)\n","os.makedirs(cropped_img_dir, exist_ok = True)"]},{"cell_type":"markdown","metadata":{},"source":["# Path to save cropped image"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.404497Z","iopub.status.busy":"2022-10-29T06:00:34.403952Z","iopub.status.idle":"2022-10-29T06:00:34.486417Z","shell.execute_reply":"2022-10-29T06:00:34.485281Z","shell.execute_reply.started":"2022-10-29T06:00:34.404459Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv(f'{BASE_PATH}/train_meta.csv')\n","mask_paths = []\n","cropped_paths = []\n","for i in range(0, len(df_train)):\n","    row = df_train.iloc[i]\n","    file_name = row['path'].split('/')[-1]\n","    mask_paths.append(f'{seg_inference_dir}/{file_name}')\n","    cropped_paths.append(f'{cropped_img_dir}/{file_name}')\n","df_train['cropped_path'] = cropped_paths\n","df_train['mask_path']    = mask_paths\n","df_train.tail()\n","\n","df_train.to_csv(f'{BASE_PATH}/train_meta.csv', index = False)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>patient_id</th>\n","      <th>series</th>\n","      <th>bowel_healthy</th>\n","      <th>bowel_injury</th>\n","      <th>extravasation_healthy</th>\n","      <th>extravasation_injury</th>\n","      <th>kidney_healthy</th>\n","      <th>kidney_low</th>\n","      <th>kidney_high</th>\n","      <th>liver_healthy</th>\n","      <th>liver_low</th>\n","      <th>liver_high</th>\n","      <th>spleen_healthy</th>\n","      <th>spleen_low</th>\n","      <th>spleen_high</th>\n","      <th>any_injury</th>\n","      <th>fold</th>\n","      <th>path</th>\n","      <th>mask_path</th>\n","      <th>cropped_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>26473</td>\n","      <td>11365</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>35348</td>\n","      <td>41274</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>37260</td>\n","      <td>21973</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37260</td>\n","      <td>19788</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>18301</td>\n","      <td>1407</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4706</th>\n","      <td>8076</td>\n","      <td>41087</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4707</th>\n","      <td>20087</td>\n","      <td>30769</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4708</th>\n","      <td>20087</td>\n","      <td>45305</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4709</th>\n","      <td>43639</td>\n","      <td>64933</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4710</th>\n","      <td>43639</td>\n","      <td>59734</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4711 rows × 20 columns</p>\n","</div>"],"text/plain":["      patient_id  series  bowel_healthy  bowel_injury  extravasation_healthy  \\\n","0          26473   11365              1             0                      1   \n","1          35348   41274              1             0                      1   \n","2          37260   21973              1             0                      1   \n","3          37260   19788              1             0                      1   \n","4          18301    1407              1             0                      1   \n","...          ...     ...            ...           ...                    ...   \n","4706        8076   41087              1             0                      1   \n","4707       20087   30769              1             0                      1   \n","4708       20087   45305              1             0                      1   \n","4709       43639   64933              1             0                      1   \n","4710       43639   59734              1             0                      1   \n","\n","      extravasation_injury  kidney_healthy  kidney_low  kidney_high  \\\n","0                        0               1           0            0   \n","1                        0               1           0            0   \n","2                        0               1           0            0   \n","3                        0               1           0            0   \n","4                        0               1           0            0   \n","...                    ...             ...         ...          ...   \n","4706                     0               1           0            0   \n","4707                     0               0           1            0   \n","4708                     0               0           1            0   \n","4709                     0               1           0            0   \n","4710                     0               1           0            0   \n","\n","      liver_healthy  liver_low  liver_high  spleen_healthy  spleen_low  \\\n","0                 1          0           0               1           0   \n","1                 1          0           0               1           0   \n","2                 1          0           0               1           0   \n","3                 1          0           0               1           0   \n","4                 1          0           0               1           0   \n","...             ...        ...         ...             ...         ...   \n","4706              1          0           0               0           1   \n","4707              1          0           0               1           0   \n","4708              1          0           0               1           0   \n","4709              1          0           0               1           0   \n","4710              1          0           0               1           0   \n","\n","      spleen_high  any_injury  fold  \\\n","0               0           0     3   \n","1               0           0     0   \n","2               0           0     0   \n","3               0           0     0   \n","4               0           0     4   \n","...           ...         ...   ...   \n","4706            0           1     1   \n","4707            0           1     2   \n","4708            0           1     2   \n","4709            0           0     2   \n","4710            0           0     2   \n","\n","                                                   path  \\\n","0     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","1     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","2     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","3     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","...                                                 ...   \n","4706  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4707  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4708  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4709  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4710  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","\n","                                              mask_path  \\\n","0     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","1     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","2     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","3     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","...                                                 ...   \n","4706  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4707  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4708  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4709  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4710  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","\n","                                           cropped_path  \n","0     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","1     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","2     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","3     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","...                                                 ...  \n","4706  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4707  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4708  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4709  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4710  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","\n","[4711 rows x 20 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df_train"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.497353Z","iopub.status.busy":"2022-10-29T06:00:34.496904Z","iopub.status.idle":"2022-10-29T06:00:34.514198Z","shell.execute_reply":"2022-10-29T06:00:34.513162Z","shell.execute_reply.started":"2022-10-29T06:00:34.497317Z"},"trusted":true},"outputs":[],"source":["def compress(name, data):\n","    with gzip.open(name, 'wb') as f:\n","        pickle.dump(data, f)\n","\n","def decompress(name):\n","    with gzip.open(name, 'rb') as f:\n","        data = pickle.load(f)\n","    return data\n","\n","def compress_fast(name, data):  \n","    np.save(name, data)\n","\n","def decompress_fast(name):\n","    data = np.load(f'{name}.npy')\n","    return data\n","\n","\n","class SEGDataset(Dataset):\n","    def __init__(self, df, mode):\n","\n","        self.df = df.reset_index()\n","        self.mode = mode\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        \n","        image = decompress(row['path'])[None]\n","        image = torch.from_numpy(image).to(torch.float32)\n","        save_path = row['mask_path']\n","\n","        return image, save_path\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.059903Z","iopub.status.busy":"2022-10-29T06:02:59.058663Z","iopub.status.idle":"2022-10-29T06:02:59.070787Z","shell.execute_reply":"2022-10-29T06:02:59.069705Z","shell.execute_reply.started":"2022-10-29T06:02:59.059860Z"},"trusted":true},"outputs":[],"source":["class TimmSegModel(nn.Module):\n","    def __init__(self, backbone, segtype='unet', pretrained=False):\n","        super(TimmSegModel, self).__init__()\n","\n","        self.encoder = timm.create_model(\n","            backbone,\n","            in_chans=1,\n","            features_only=True,\n","            pretrained=pretrained\n","        )\n","        g = self.encoder(torch.rand(1, 1, 64, 64))\n","        encoder_channels = [1] + [_.shape[1] for _ in g]\n","        decoder_channels = [256, 128, 64, 32, 16]\n","        if segtype == 'unet':\n","            self.decoder = smp.unet.decoder.UnetDecoder(\n","                encoder_channels=encoder_channels[:n_blocks+1],\n","                decoder_channels=decoder_channels[:n_blocks],\n","                n_blocks=n_blocks,\n","            )\n","\n","        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","    def forward(self,x):\n","        global_features = [0] + self.encoder(x)[:n_blocks]\n","        seg_features = self.decoder(*global_features)\n","        seg_features = self.segmentation_head(seg_features)\n","        return seg_features"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.073051Z","iopub.status.busy":"2022-10-29T06:02:59.072642Z","iopub.status.idle":"2022-10-29T06:03:13.720715Z","shell.execute_reply":"2022-10-29T06:03:13.719595Z","shell.execute_reply.started":"2022-10-29T06:02:59.073017Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 5, 128, 128, 128])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from timm.models.layers.conv2d_same import Conv2dSame\n","from conv3d_same import Conv3dSame\n","\n","\n","def convert_3d(module):\n","\n","    module_output = module\n","    if isinstance(module, torch.nn.BatchNorm2d):\n","        module_output = torch.nn.BatchNorm3d(\n","            module.num_features,\n","            module.eps,\n","            module.momentum,\n","            module.affine,\n","            module.track_running_stats,\n","        )\n","        if module.affine:\n","            with torch.no_grad():\n","                module_output.weight = module.weight\n","                module_output.bias = module.bias\n","        module_output.running_mean = module.running_mean\n","        module_output.running_var = module.running_var\n","        module_output.num_batches_tracked = module.num_batches_tracked\n","        if hasattr(module, \"qconfig\"):\n","            module_output.qconfig = module.qconfig\n","            \n","    elif isinstance(module, Conv2dSame):\n","        module_output = Conv3dSame(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.Conv2d):\n","        module_output = torch.nn.Conv3d(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","            padding_mode=module.padding_mode\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.MaxPool2d):\n","        module_output = torch.nn.MaxPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            dilation=module.dilation,\n","            ceil_mode=module.ceil_mode,\n","        )\n","    elif isinstance(module, torch.nn.AvgPool2d):\n","        module_output = torch.nn.AvgPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            ceil_mode=module.ceil_mode,\n","        )\n","\n","    for name, child in module.named_children():\n","        module_output.add_module(\n","            name, convert_3d(child)\n","        )\n","    del module\n","\n","    return module_output\n","\n","m = TimmSegModel(backbone)\n","m = convert_3d(m)\n","m(torch.rand(1, 1, 128,128,128)).shape"]},{"cell_type":"markdown","metadata":{},"source":["# Inference mask"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.738907Z","iopub.status.busy":"2022-10-29T06:03:13.738511Z","iopub.status.idle":"2022-10-29T06:03:13.755925Z","shell.execute_reply":"2022-10-29T06:03:13.755044Z","shell.execute_reply.started":"2022-10-29T06:03:13.738871Z"},"trusted":true},"outputs":[],"source":["def infer_func(model, loader_valid):\n","    model.eval()\n","    valid_loss = []\n","    outputs = []\n","    th = 0.1\n","    batch_metrics = [[]]\n","    bar = tqdm(loader_valid)\n","    \n","    with torch.no_grad():\n","        for images, save_paths in bar:\n","            images = images.cuda()\n","            logits = model(images)\n","            preds = (logits.sigmoid() > 0.1).float().detach().cpu().numpy()\n","            y_preds = (preds+0.1).astype(np.uint8)\n","        \n","            def save_mask(ind, preds = y_preds): \n","                compress(save_paths[ind], preds[ind])\n","            \n","            Parallel(n_jobs = len(y_preds))(delayed(save_mask)(i) for i in range(len(y_preds)))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.979257Z","iopub.status.busy":"2022-10-29T06:03:13.978679Z","iopub.status.idle":"2022-10-29T06:03:13.992437Z","shell.execute_reply":"2022-10-29T06:03:13.991378Z","shell.execute_reply.started":"2022-10-29T06:03:13.979202Z"},"trusted":true},"outputs":[],"source":["def run(fold):\n","    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n","    dataset_train = SEGDataset(df_train, 'valid')\n","    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_MASK_PRED, shuffle=False, num_workers=BATCH_MASK_PRED)\n","\n","    model = TimmSegModel(backbone, pretrained=True)\n","    model = convert_3d(model)\n","\n","    model.load_state_dict(torch.load(model_file))\n","    model = model.to(device)\n","\n","    print(len(dataset_train))\n","    \n","    infer_func(model, loader_train)\n","\n","    del model\n","    torch.cuda.empty_cache()\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:33.006143Z","iopub.status.busy":"2022-10-29T06:03:33.005515Z"},"trusted":true},"outputs":[],"source":["#run(0)"]},{"cell_type":"markdown","metadata":{},"source":["# Postprocss to get crop regions"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["'/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection/train_images/35348/41274'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["glob.glob(f'{BASE_PATH}/train_images/*/*')[1]"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["BitsAllocated                                                                16\n","BitsStored                                                                   16\n","Columns                                                                     512\n","ContentDate                                                            20230722\n","ContentTime                                                       003306.881589\n","FrameOfReferenceUID           1.2.826.0.1.3680043.8.498.28466454913303094199...\n","HighBit                                                                      15\n","ImageOrientationPatient       [1.00000, 0.00000, 0.00000, 0.00000, 1.00000, ...\n","ImagePositionPatient                            [-235.8665, -235.8665, 1459.20]\n","InstanceNumber                                                              532\n","KVP                                                                       120.0\n","PatientID                                                                 49954\n","PatientPosition                                                             FFS\n","PhotometricInterpretation                                           MONOCHROME2\n","PixelRepresentation                                                           1\n","PixelSpacing                                                     [0.923, 0.923]\n","RescaleIntercept                                                            0.0\n","RescaleSlope                                                                1.0\n","Rows                                                                        512\n","SOPInstanceUID                                  1.2.123.12345.1.2.3.49954.1.532\n","SamplesPerPixel                                                               1\n","SeriesInstanceUID                               1.2.123.12345.1.2.3.49954.41479\n","SeriesNumber                                                                  6\n","SliceThickness                                                              1.0\n","StudyInstanceUID                                      1.2.123.12345.1.2.3.49954\n","WindowCenter                                                               50.0\n","WindowWidth                                                               400.0\n","FileMetaInformationVersion                                                   \u0000\u0001\n","ImplementationClassUID                             1.2.3.123456.4.5.1234.1.12.0\n","ImplementationVersionName                                         PYDICOM 2.4.0\n","MediaStorageSOPClassUID                               1.2.840.10008.5.1.4.1.1.2\n","MediaStorageSOPInstanceUID                      1.2.123.12345.1.2.3.49954.1.532\n","TransferSyntaxUID                                           1.2.840.10008.1.2.5\n","path                                           train_images/49954/41479/532.dcm\n","RescaleType                                                                 nan\n","Name: 0, dtype: object"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train_dcm_tags = pd.read_parquet(f'{BASE_PATH}/train_dicom_tags.parquet')\n","train_dcm_tags.keys()\n","train_dcm_tags.iloc[0]"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["train_dcm_tags = train_dcm_tags.groupby(['PatientID','SeriesInstanceUID'])\n","\n","patient_series_info = {}\n","for (patient, series), group in train_dcm_tags:\n","    series = series.split('.')[-1]\n","    name = f'{patient}_{series}'\n","    patient_series_info[name] = group.reset_index()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["#Returns GPU array\n","def standardize_pixel_array(pixel_array, dcm_rows):\n","    \"\"\"\n","    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n","    \"\"\"\n","    # Correct DICOM pixel_array if PixelRepresentation == 1.\n","    #pixel_array = dcm.pixel_array\n","    #pixel_array = cp.array(pixel_array)\n","    for z in range(0, len(pixel_array)):\n","        if dcm_rows[z].PixelRepresentation == 1:\n","            bit_shift = dcm_rows[z].BitsAllocated - dcm_rows[z].BitsStored\n","            dtype = pixel_array[z].dtype \n","            pixel_array[z] = (pixel_array[z] << bit_shift).astype(dtype) >>  bit_shift\n","    #         pixel_array = pydicom.pixel_data_handlers.util.apply_modality_lut(new_array, dcm)\n","\n","    pixel_array = torch.from_numpy(pixel_array.astype(float)).to(DEVICE).to(torch.float16)    \n","    for z in range(0, len(pixel_array)):\n","        intercept = float(dcm_rows[z].RescaleIntercept)\n","        slope = float(dcm_rows[z].RescaleSlope)\n","        center = int(dcm_rows[z].WindowCenter)\n","        width = int(dcm_rows[z].WindowWidth)\n","        low = center - width / 2\n","        high = center + width / 2    \n","        \n","        pixel_array[z] = (pixel_array[z] * slope) + intercept\n","        pixel_array[z] = torch.clip(pixel_array[z], low, high)\n","        \n","    #pixel_array = cp.asnumpy(pixel_array)    \n","    #del cu_pixel_array\n","    #cp._default_memory_pool.free_all_blocks()\n","    gc.collect()\n","    \n","    return pixel_array\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["list_keys = list(patient_series_info.keys())\n","for key in list_keys:\n","    len_h = len(np.unique(patient_series_info[key]['RescaleIntercept']))\n","    if(len_h!=1):\n","        print(len_h)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["#The order of the crop region data format\n","#Z start/end, Y start/end, X start/end for each mask channels + total region for the extravasation prediction\n","def calc_crop_region(mask):\n","    crop_range = np.zeros((6, 6))\n","    crop_range[:,::2]=10000\n","    mask_z = np.max(mask, axis = (2, 3)).astype(bool)\n","    mask_y = np.max(mask, axis = (1, 3)).astype(bool)\n","    mask_x = np.max(mask, axis = (1, 2)).astype(bool)\n","    \n","    template_range = np.arange(0, RESOL)\n","\n","    for mi in range(0, 5):\n","        zrange = template_range[mask_z[mi]]\n","        yrange = template_range[mask_y[mi]]\n","        xrange = template_range[mask_x[mi]]\n","        # For incomplete organ\n","        if(len(zrange)==0):\n","            zrange = template_range.copy()\n","            yrange = template_range.copy()\n","            xrange = template_range.copy()\n","\n","        crop_range[mi] = np.min(zrange), np.max(zrange)+1, np.min(yrange), np.max(yrange)+1, np.min(xrange), np.max(xrange)+1\n","\n","    crop_range[5] = np.min(crop_range[:5, 0]), np.max(crop_range[:5, 1]), np.min(crop_range[:5, 2]), \\\n","                    np.max(crop_range[:5, 3]), np.min(crop_range[:5,4]), np.max(crop_range[:5, 5])\n","    \n","    crop_range[:,:2]/=len(mask_z[0])\n","    crop_range[:,2:4]/=len(mask_y[0])\n","    crop_range[:,4:6]/=len(mask_x[0])\n","\n","    # Then make extravasation (# 5 mask) to reference one and convert other mask's crop respective to it\n","    # --> To minimize the loading size due to speed issue.\n","    zmin, rel_zrange = crop_range[5,0], crop_range[5,1]-crop_range[5,0]\n","    ymin, rel_yrange = crop_range[5,2], crop_range[5,3]-crop_range[5,2]\n","    xmin, rel_xrange = crop_range[5,4], crop_range[5,5]-crop_range[5,4]\n","\n","    crop_range[:5,:2] = (crop_range[:5,:2]-zmin)/rel_zrange\n","    crop_range[:5,2:4] = (crop_range[:5,2:4]-ymin)/rel_yrange\n","    crop_range[:5,4:6] = (crop_range[:5,4:6]-xmin)/rel_xrange\n","\n","    return crop_range\n","\n","def crop_resize_avg_and_std_3d(data, region, resize_shape):  \n","    shapes = data.shape\n","    region[:2]*=shapes[0]\n","    region[2:4]*=shapes[1]\n","    region[4:6]*=shapes[2]\n","    region = region.astype(int)\n","\n","    cropped = torch.clone(data[region[0]:region[1], region[2]:region[3], region[4]:region[5]])    \n","\n","    #resize xy\n","    cropped = transforms.Resize((int(resize_shape[1]), int(resize_shape[2])), antialias = True)(cropped)\n","    #slices = []\n","    #for i in range(0, len(cropped)):\n","    #    slices.append(cv2.resize(cropped[i], (resize_shape[2], resize_shape[1]))[None])\n","    \n","    #slices = np.vstack(slices)\n","    #resized_cropped = np.zeros(resize_shape)\n","    \n","    #zyx to xzy\n","    cropped = torch.permute(cropped, (2, 0, 1))\n","    cropped = transforms.Resize((int(resize_shape[0]), int(resize_shape[1])), antialias = True)(cropped)\n","    #xzy to zyx\n","    cropped = torch.permute(cropped, (1, 2, 0))\n","    #for i in range(0, len(slices[0,0])):\n","    #    resized_cropped[:,:,i] = cv2.resize(slices[:,:,i], (resize_shape[1], resize_shape[0]))\n","\n","        \n","    std = torch.std(cropped, (0, 1, 2))\n","    avg = torch.mean(cropped, (0, 1, 2))\n","    cropped = (cropped-avg)/std\n","    \n","\n","    gc.collect()\n","    return cropped\n","\n","\n","# Read each slice and stack them to make 3d data\n","def process_3d_crop(save_path, mask_path, resize_shapes, data_path = TRAIN_PATH, patient_series_info = patient_series_info):\n","    tmp = save_path.split('/')[-1][:-4]\n","    tmp = tmp.split('_')\n","    patient, study = int(tmp[0]), int(tmp[1])\n","    \n","    sel_df = patient_series_info[f'{patient}_{study}']\n","    sel_df = sel_df.sort_values('InstanceNumber')\n","    #print(sel_df)\n","    \n","    mask = decompress(mask_path)\n","    crop_regions = calc_crop_region(mask)\n","    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n","\n","    crop_regions[5] = 0, 1, 0, 1, 0, 1\n","\n","    imgs = {}    \n","    \n","    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):  \n","        pos_z = -int((f.split('/')[-1])[:-4])\n","        imgs[pos_z] = f\n","\n","    imgs_3d = []\n","    n_imgs = len(imgs)    \n","    z_crop_range= (absolute_crop[0:2]*n_imgs).astype(int)\n","    #print(z_crop_range)\n","    \n","    dcm_rows = []\n","    for i, k in enumerate(sorted(imgs.keys())):\n","        #if i in sample_z:\n","        if(i >= z_crop_range[0] and i < z_crop_range[1]):\n","            IS_XY_CROP = False\n","            f = imgs[k]\n","            #try:            \n","            img = dicomsdl.open(f).pixelData(storedvalue=True)\n","            if not IS_XY_CROP:\n","                img_shape = np.shape(img)\n","                xy_crop_range = absolute_crop[2:].copy()   \n","                xy_crop_range[0:2]*=img_shape[0]\n","                xy_crop_range[2:4]*=img_shape[1]            \n","                xy_crop_range = xy_crop_range.astype(int)\n","                img = img[xy_crop_range[0]:xy_crop_range[1], xy_crop_range[2]:xy_crop_range[3]]\n","                IS_XY_CROP = True\n","            else:\n","                img = dicomsdl.open(f).pixelData(storedvalue=True)[xy_crop_range[0]:xy_crop_range[1], xy_crop_range[2]:xy_crop_range[3]]                \n","\n","            try:\n","                dcm_row = sel_df.loc[sel_df['InstanceNumber']<=-k].iloc[-1]     \n","            except:\n","                dcm_row = sel_df.loc[sel_df['InstanceNumber']>-k].iloc[0]          \n","            dcm_rows.append(dcm_row)                  \n","            imgs_3d.append(img[None])\n","            #except:\n","            #    continue\n","                \n","    imgs_3d = np.vstack(imgs_3d)\n","\n","    imgs_3d  = standardize_pixel_array(imgs_3d, dcm_rows)\n","    min_imgs = torch.min(imgs_3d)\n","    max_imgs = torch.max(imgs_3d)\n","    imgs_3d = ((imgs_3d - min_imgs) / (max_imgs - min_imgs + 1e-6))\n","\n","    if dcm_rows[0].PhotometricInterpretation == \"MONOCHROME1\":\n","        imgs_3d = 1.0 - imgs_3d\n","\n","    #Loaded original imgs_3d    \n","    #processed_img_3d = np.zeros((6, RESOL, RESOL, RESOL))\n","    \n","    origin_shape = imgs_3d.shape\n","    for i in range(0, 6):    \n","        #To deal with almost not detected slices\n","        try:   \n","            # To deal with possible noises\n","            if(((crop_regions[i,1]-crop_regions[i,0]) < 5/origin_shape[0]) or \n","                ((crop_regions[i,3]-crop_regions[i,2]) < 5/origin_shape[1]) or \n","                ((crop_regions[i,5]-crop_regions[i,4]) < 5/origin_shape[2])):\n","                dummy_failure_function()\n","            \n","            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, crop_regions[i], resize_shapes[i])).to('cpu')\n","            compress(f'{save_path}_{i}', processed_img_3d)      \n","\n","            del processed_img_3d\n","            gc.collect()\n","        except:\n","            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, np.array([0, 1, 0, 1, 0, 1]), resize_shapes[i]))\n","            compress(f'{save_path}_{i}', processed_img_3d).to('cpu')\n","            del processed_img_3d\n","            gc.collect()  \n","\n","    del imgs, img\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    \n","def calc_size_3d_crop(save_path, mask_path, data_path = TRAIN_PATH):\n","    tmp = save_path.split('/')[-1][:-4]\n","    tmp = tmp.split('_')\n","    patient, study = int(tmp[0]), int(tmp[1])\n","    \n","    mask = decompress(mask_path)\n","    crop_regions = calc_crop_region(mask)\n","    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n","\n","    crop_regions[5] = 0, 1, 0, 1, 0, 1\n","\n","    imgs = {}    \n","    \n","    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):      \n","        try:            \n","            img = dicomsdl.open(f).pixelData(storedvalue=True)\n","            img_shape = np.shape(img)\n","            xy_crop_range = absolute_crop[2:].copy()   \n","            xy_crop_range[0:2]*=img_shape[0]\n","            xy_crop_range[2:4]*=img_shape[1]            \n","            xy_crop_range = xy_crop_range.astype(int)\n","            img = img.astype(float)\n","            break\n","        except:\n","            continue\n","    \n","    base_crop_shape = np.zeros(3, float)\n","    base_crop_shape[:] = absolute_crop[1::2] - absolute_crop[0::2]\n","    base_crop_shape[0]*= len(glob.glob(data_path + f'/{patient}/{study}/*.dcm'))\n","    base_crop_shape[1:3] = xy_crop_range[1::2] - xy_crop_range[0::2]\n","    \n","    all_crop_shape = crop_regions[:,1::2] - crop_regions[:,0::2]\n","    for i in range(0, 6):\n","        all_crop_shape[i] *= base_crop_shape\n","    del img\n","    gc.collect()\n","    return all_crop_shape"]},{"cell_type":"markdown","metadata":{},"source":["# Calculate average crop size"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Preprocess dataset\n","rng_samples = np.linspace(0, len(df_train), N_PROCESS_CROP+1, dtype = int)\n","def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = df_train):\n","    partial_crop_shapes = []\n","    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n","        partial_crop_shapes.append(calc_size_3d_crop(df_train.iloc[i]['cropped_path'], df_train.iloc[i]['mask_path'])[None])\n","    partial_crop_shapes = np.vstack(partial_crop_shapes)\n","    return partial_crop_shapes"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 392/392 [00:13<00:00, 28.14it/s]\n","100%|██████████| 392/392 [00:13<00:00, 28.09it/s]\n","100%|██████████| 393/393 [00:13<00:00, 28.16it/s]\n","100%|██████████| 393/393 [00:14<00:00, 27.51it/s]\n","100%|██████████| 393/393 [00:14<00:00, 27.40it/s]\n","100%|██████████| 393/393 [00:14<00:00, 27.49it/s]\n","100%|██████████| 392/392 [00:14<00:00, 26.63it/s]\n","100%|██████████| 393/393 [00:15<00:00, 25.57it/s]\n","100%|██████████| 392/392 [00:15<00:00, 25.07it/s]\n"," 99%|█████████▉| 390/393 [00:18<00:00, 41.46it/s]"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 153 ms, sys: 570 ms, total: 723 ms\n","Wall time: 19.6 s\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 392/392 [00:18<00:00, 20.83it/s]\n","100%|██████████| 393/393 [00:18<00:00, 20.99it/s]\n","100%|██████████| 393/393 [00:18<00:00, 20.80it/s]\n"]}],"source":["%%time\n","all_crop_shapes = Parallel(n_jobs = N_PROCESS_CROP)(delayed(process_3d_wrapper)(i) for i in range(N_PROCESS_CROP))\n","all_crop_shapes = np.vstack(all_crop_shapes)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["norm_all_crop_shapes = (all_crop_shapes[:,:,0]*all_crop_shapes[:,:,1]*all_crop_shapes[:,:,2])**(1/3)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["array([[124, 120, 166],\n","       [103, 146, 167],\n","       [101, 156, 157],\n","       [ 73, 167, 199],\n","       [ 81, 172, 180],\n","       [116, 122, 175]])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["normed_all_crop_shapes = all_crop_shapes.copy()\n","for i in range(0, len(df_train)):\n","    for j in range(0, 6):\n","        normed_all_crop_shapes[i,j]/=norm_all_crop_shapes[i, j]\n","\n","avg_normed_all_crop_shapes = np.average(normed_all_crop_shapes, axis = 0)\n","avg_normed_all_crop_shapes*=RESOL\n","avg_normed_all_crop_shapes = avg_normed_all_crop_shapes.astype(int)\n","avg_normed_all_crop_shapes"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# Preprocess dataset\n","rng_samples = np.linspace(0, len(df_train), N_PROCESS_CROP+1, dtype = int)\n","def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = df_train, resize_shapes = avg_normed_all_crop_shapes):\n","    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n","        if not os.path.isfile(f\"{df_train.iloc[i]['cropped_path']}_0\"):\n","            process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'], resize_shapes)     \n","        \n","        #For error correction\n","        else:            \n","            try:\n","                data = decompress_fast(df_train.iloc[i]['cropped_path'])\n","            except: \n","                process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'], resize_shapes)  \n","        "]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/392 [00:00<?, ?it/s]/home/junseonglee/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","  0%|          | 1/392 [00:05<36:06,  5.54s/it]\n"]},{"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'to'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     data \u001b[39m=\u001b[39m decompress_fast(df_train\u001b[39m.\u001b[39miloc[i][\u001b[39m'\u001b[39m\u001b[39mcropped_path\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mexcept\u001b[39;00m: \n","\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecompress_fast\u001b[39m(name):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39m(os_fspath(file), \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection/3d_preprocessed_crop_ratio/35348_41274.pkl.npy'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=156'>157</a>\u001b[0m processed_img_3d \u001b[39m=\u001b[39m (crop_resize_avg_and_std_3d(imgs_3d, crop_regions[i], resize_shapes[i]))\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=157'>158</a>\u001b[0m compress(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msave_path\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, processed_img_3d)      \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39mdel\u001b[39;00m processed_img_3d\n","\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb Cell 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m gzip\u001b[39m.\u001b[39mopen(name, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(data, f)\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/gzip.py:289\u001b[0m, in \u001b[0;36mGzipFile.write\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m length \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfileobj\u001b[39m.\u001b[39mwrite(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompress\u001b[39m.\u001b[39mcompress(data))\n\u001b[1;32m    290\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m length\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","File \u001b[0;32m<timed eval>:1\u001b[0m\n","\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     data \u001b[39m=\u001b[39m decompress_fast(df_train\u001b[39m.\u001b[39miloc[i][\u001b[39m'\u001b[39m\u001b[39mcropped_path\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mexcept\u001b[39;00m: \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     process_3d_crop(train_meta_df\u001b[39m.\u001b[39miloc[i][\u001b[39m'\u001b[39m\u001b[39mcropped_path\u001b[39m\u001b[39m'\u001b[39m], train_meta_df\u001b[39m.\u001b[39miloc[i][\u001b[39m'\u001b[39m\u001b[39mmask_path\u001b[39m\u001b[39m'\u001b[39m], resize_shapes)\n","\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=161'>162</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=162'>163</a>\u001b[0m     processed_img_3d \u001b[39m=\u001b[39m (crop_resize_avg_and_std_3d(imgs_3d, np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]), resize_shapes[i]))\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m     compress(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msave_path\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, processed_img_3d)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=164'>165</a>\u001b[0m     \u001b[39mdel\u001b[39;00m processed_img_3d\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step2b_generate_masks_and_avgoptcrop_for_the_classification.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m     gc\u001b[39m.\u001b[39mcollect()  \n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to'"]}],"source":["%%time\n","#process_3d_wrapper(0)\n","Parallel(n_jobs = N_PROCESS_CROP)(delayed(process_3d_wrapper)(i) for i in range(N_PROCESS_CROP))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
