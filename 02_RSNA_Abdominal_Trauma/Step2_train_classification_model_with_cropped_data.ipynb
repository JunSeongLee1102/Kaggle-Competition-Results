{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junseonglee/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjunseonglee\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/junseonglee/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "sys.path.append('./lib_models')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.metrics\n",
    "import warnings\n",
    "import pydicom\n",
    "import dicomsdl\n",
    "from joblib import Parallel, delayed\n",
    "#import h5py\n",
    "import bz2\n",
    "import pickle\n",
    "import gzip\n",
    "import mgzip\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from multiprocessing import Pool\n",
    "import lz4.frame\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch import nn\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import timm\n",
    "from timm.utils import AverageMeter\n",
    "from timm.models import resnet\n",
    "\n",
    "#sys.path.append('/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection')\n",
    "import timm_new\n",
    "\n",
    "from monai.transforms import Resize\n",
    "import  monai.transforms as transforms\n",
    "\n",
    "import wandb\n",
    "sys.path.append('./lib_models')\n",
    "\n",
    "wandb.login(key = '585f58f321685308f7933861d9dde7488de0970b')\n",
    "#warnings.filterwarnings('ignore', category=UserWarning)\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING']='1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm_new.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone = 'seresnext50_32x4d.racm_in1k'\n",
    "\n",
    "IS_WANDB = True\n",
    "PROJECT_NAME = 'RSNA_ABTD'\n",
    "GROUP_NAME= 'augmentation_test'\n",
    "RUN_NAME=   f'{backbone}_mixup0.1'\n",
    "\n",
    "if not IS_WANDB:\n",
    "    PROJECT_NAME = 'Dummy_Project'\n",
    "\n",
    "BASE_PATH  = '/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection'\n",
    "TRAIN_PATH = f'{BASE_PATH}/train_images'\n",
    "DATA_PATH = f'{BASE_PATH}/3d_preprocessed'\n",
    "\n",
    "seg_inference_dir = f'{BASE_PATH}/seg_infer_results'\n",
    "cropped_img_dir   = f'{BASE_PATH}/3d_preprocessed_crop'\n",
    "\n",
    "if not os.path.isdir(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "\n",
    "RESOL = 128\n",
    "UP_RESOL = 128\n",
    "N_CHANNELS = 6\n",
    "BATCH_SIZE = 16\n",
    "ACCUM_STEPS = 1\n",
    "N_WORKERS  = 8\n",
    "LR = 0.001\n",
    "N_EPOCHS = 100\n",
    "EARLY_STOP_COUNT = 20\n",
    "N_FOLDS  = 5\n",
    "N_PREPROCESS_CHUNKS = 12\n",
    "train_df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "train_df = train_df.sort_values(by=['patient_id'])\n",
    "n_blocks = 4\n",
    "drop_rate = 0.2\n",
    "drop_path_rate = 0.2\n",
    "p_mixup = 0.1\n",
    "\n",
    "\n",
    "#backbone = 'efficientnet_b1'\n",
    "\n",
    "\n",
    "\n",
    "wandb_config = {\n",
    "    'RESOL': RESOL,\n",
    "    'BACKBONE': backbone,\n",
    "    'N_CHANNELS': N_CHANNELS,\n",
    "    'N_EPOCHS': N_EPOCHS,\n",
    "    'N_FOLDS': N_FOLDS,\n",
    "    'EARLY_STOP_COUNT': EARLY_STOP_COUNT,\n",
    "    'BATCH_SIZE': BATCH_SIZE,    \n",
    "    'LR': LR,\n",
    "    'N_EPOCHS': N_EPOCHS,\n",
    "    'DROP_RATE': drop_rate,\n",
    "    'DROP_PATH_RATE': drop_path_rate,\n",
    "    'MIXUP_RATE': p_mixup\n",
    "}\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE\n",
    "#DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4]), array([630, 629, 630, 629, 629]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "train_meta = pd.read_csv(f'{BASE_PATH}/train_series_meta.csv')\n",
    "train_df = train_df.sort_values(by=['patient_id'])\n",
    "train_df\n",
    "\n",
    "TRAIN_PATH = BASE_PATH + \"/train_images/\"\n",
    "n_chunk = 8\n",
    "patients = os.listdir(TRAIN_PATH)\n",
    "n_patients = len(patients)\n",
    "rng_patients = np.linspace(0, n_patients+1, n_chunk+1, dtype = int)\n",
    "patients_cts = glob.glob(f'{TRAIN_PATH}/*/*')\n",
    "n_cts = len(patients_cts)\n",
    "patients_cts_arr = np.zeros((n_cts, 2), int)\n",
    "data_paths=[]\n",
    "for i in range(0, n_cts):\n",
    "    patient, ct = patients_cts[i].split('/')[-2:]\n",
    "    patients_cts_arr[i] = patient, ct\n",
    "    data_paths.append(f'{BASE_PATH}/3d_preprocessed/{patients_cts_arr[i,0]}_{patients_cts_arr[i,1]}.pkl')\n",
    "TRAIN_IMG_PATH = BASE_PATH + '/processed' \n",
    "\n",
    "#Generate tables for training\n",
    "train_meta_df = pd.DataFrame(patients_cts_arr, columns = ['patient_id', 'series'])\n",
    "\n",
    "#5-fold splitting\n",
    "train_df['fold'] = 0\n",
    "labels = train_df[['bowel_healthy','bowel_injury',\n",
    "                    'extravasation_healthy','extravasation_injury',\n",
    "                    'kidney_healthy','kidney_low','kidney_high',\n",
    "                    'liver_healthy','liver_low','liver_high',\n",
    "                    'spleen_healthy','spleen_low','spleen_high',\n",
    "                    'any_injury']].to_numpy()\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=0)\n",
    "counter = 0\n",
    "for train_index, test_index in mskf.split(np.ones(len(train_df)), labels):\n",
    "    for i in range(0, len(test_index)):\n",
    "        train_df['fold'][test_index[i]] = counter\n",
    "    counter+=1\n",
    "\n",
    "train_meta_df = train_meta_df.join(train_df.set_index('patient_id'), on='patient_id')\n",
    "train_meta_df['path']=data_paths\n",
    "\n",
    "#For mask paths\n",
    "mask_paths = []\n",
    "cropped_paths = []\n",
    "for i in range(0, len(train_meta_df)):\n",
    "    row = train_meta_df.iloc[i]\n",
    "    file_name = row['path'].split('/')[-1]\n",
    "    mask_paths.append(f'{seg_inference_dir}/{file_name}')\n",
    "    cropped_paths.append(f'{cropped_img_dir}/{file_name}')\n",
    "train_meta_df['mask_path'] = mask_paths\n",
    "train_meta_df['cropped_path'] = cropped_paths\n",
    "\n",
    "train_meta_df.to_csv(f'{BASE_PATH}/train_meta.csv', index = False)\n",
    "np.unique(train_df['fold'].to_numpy(), return_counts = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(name, data):\n",
    "    with gzip.open(name, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def decompress(name):\n",
    "    with gzip.open(name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def compress_fast(name, data):  \n",
    "    np.save(name, data)\n",
    "\n",
    "def decompress_fast(name):\n",
    "    data = np.load(f'{name}.npy')\n",
    "    return data\n",
    "\n",
    "def save_png(name, data):\n",
    "    cv2.imwrite(f'{name}.png', data,  [cv2.IMWRITE_PNG_COMPRESSION, 9])\n",
    "\n",
    "\n",
    "def load_png(name):\n",
    "    #data = cv2.imread(f'{name}.png', cv2.IMREAD_ANYDEPTH)\n",
    "    data = np.array(Image.open(f'{name}.png'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n",
    "    \"\"\"\n",
    "    # Correct DICOM pixel_array if PixelRepresentation == 1.\n",
    "    pixel_array = dcm.pixel_array\n",
    "    pixel_rep = dcm.PixelRepresentation\n",
    "\n",
    "    return pixel_array\n",
    "\n",
    "#The order of the crop region data format\n",
    "#Z start/end, Y start/end, X start/end for each mask channels + total region for the extravasation prediction\n",
    "def calc_crop_region(mask):\n",
    "    crop_range = np.zeros((6, 6))\n",
    "    crop_range[:,::2]=10000\n",
    "    mask_z = np.max(mask, axis = (2, 3)).astype(bool)\n",
    "    mask_y = np.max(mask, axis = (1, 3)).astype(bool)\n",
    "    mask_x = np.max(mask, axis = (1, 2)).astype(bool)\n",
    "    \n",
    "    template_range = np.arange(0, RESOL)\n",
    "\n",
    "    for mi in range(0, 5):\n",
    "        zrange = template_range[mask_z[mi]]\n",
    "        yrange = template_range[mask_y[mi]]\n",
    "        xrange = template_range[mask_x[mi]]\n",
    "        # For incomplete organ\n",
    "        if(len(zrange)==0):\n",
    "            zrange = template_range.copy()\n",
    "            yrange = template_range.copy()\n",
    "            xrange = template_range.copy()\n",
    "\n",
    "        crop_range[mi] = np.min(zrange), np.max(zrange)+1, np.min(yrange), np.max(yrange)+1, np.min(xrange), np.max(xrange)+1\n",
    "\n",
    "    crop_range[5] = np.min(crop_range[:5, 0]), np.max(crop_range[:5, 1]), np.min(crop_range[:5, 2]), \\\n",
    "                    np.max(crop_range[:5, 3]), np.min(crop_range[:5,4]), np.max(crop_range[:5, 5])\n",
    "    \n",
    "    crop_range[:,:2]/=len(mask_z[0])\n",
    "    crop_range[:,2:4]/=len(mask_y[0])\n",
    "    crop_range[:,4:6]/=len(mask_x[0])\n",
    "\n",
    "    # Then make extravasation (# 5 mask) to reference one and convert other mask's crop respective to it\n",
    "    # --> To minimize the loading size due to speed issue.\n",
    "    zmin, rel_zrange = crop_range[5,0], crop_range[5,1]-crop_range[5,0]\n",
    "    ymin, rel_yrange = crop_range[5,2], crop_range[5,3]-crop_range[5,2]\n",
    "    xmin, rel_xrange = crop_range[5,4], crop_range[5,5]-crop_range[5,4]\n",
    "\n",
    "    crop_range[:5,:2] = (crop_range[:5,:2]-zmin)/rel_zrange\n",
    "    crop_range[:5,2:4] = (crop_range[:5,2:4]-ymin)/rel_yrange\n",
    "    crop_range[:5,4:6] = (crop_range[:5,4:6]-xmin)/rel_xrange\n",
    "\n",
    "    return crop_range\n",
    "\n",
    "def crop_resize_avg_and_std_3d(data, region):\n",
    "    shapes = np.shape(data)\n",
    "    region[:2]*=shapes[0]\n",
    "    region[2:4]*=shapes[1]\n",
    "    region[4:6]*=shapes[2]\n",
    "    region = region.astype(int)\n",
    "\n",
    "    cropped = data[region[0]:region[1], region[2]:region[3], region[4]:region[5]]\n",
    "    slices = []\n",
    "    for i in range(0, len(cropped)):\n",
    "        slices.append(cv2.resize(cropped[i], (RESOL, RESOL))[None])\n",
    "    \n",
    "    slices = np.vstack(slices)\n",
    "    \n",
    "    resized_cropped = np.zeros((RESOL, RESOL, RESOL))\n",
    "    for i in range(0, len(slices[0,0])):\n",
    "        resized_cropped[:,:,i] = cv2.resize(slices[:,:,i], (RESOL, RESOL))\n",
    "    \n",
    "    std = np.std(resized_cropped)\n",
    "    avg = np.average(resized_cropped)\n",
    "    resized_cropped = (resized_cropped-avg)/std\n",
    "    resized_cropped = resized_cropped.astype(np.float32)\n",
    "    \n",
    "\n",
    "    del cropped, slices\n",
    "    gc.collect()\n",
    "    return resized_cropped\n",
    "\n",
    "def standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n",
    "    \"\"\"\n",
    "    # Correct DICOM pixel_array if PixelRepresentation == 1.\n",
    "    pixel_array = dcm.pixel_array\n",
    "    pixel_rep = dcm.PixelRepresentation\n",
    "\n",
    "    return pixel_array\n",
    "\n",
    "# Read each slice and stack them to make 3d data\n",
    "def process_3d_crop(save_path, mask_path, data_path = TRAIN_PATH):\n",
    "    tmp = save_path.split('/')[-1][:-4]\n",
    "    tmp = tmp.split('_')\n",
    "    patient, study = int(tmp[0]), int(tmp[1])\n",
    "    \n",
    "    mask = decompress(mask_path)\n",
    "    crop_regions = calc_crop_region(mask)\n",
    "    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n",
    "\n",
    "    crop_regions[5] = 0, 1, 0, 1, 0, 1\n",
    "\n",
    "    imgs = {}    \n",
    "    \n",
    "    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):      \n",
    "        pixel_rep = 0\n",
    "        bit_shift = 0\n",
    "        dtype = 0\n",
    "        try:            \n",
    "            dicom = pydicom.dcmread(f)        \n",
    "            img = standardize_pixel_array(dicom)\n",
    "            img_shape = np.shape(img)\n",
    "            xy_crop_range = absolute_crop[2:].copy()   \n",
    "            xy_crop_range[0:2]*=img_shape[0]\n",
    "            xy_crop_range[2:4]*=img_shape[1]            \n",
    "            xy_crop_range = xy_crop_range.astype(int)\n",
    "            img = img.astype(float)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):\n",
    "        #For the case that some of the image can't be read -> error without this though don't know why  \n",
    "        img = dicomsdl.open(f).pixelData(storedvalue=True)[xy_crop_range[0]:xy_crop_range[1], xy_crop_range[2]:xy_crop_range[3]]\n",
    "        img = img.astype(float)\n",
    "        \n",
    "        #dicom = pydicom.dcmread(f)\n",
    "        #img = standardize_pixel_array(dicom).astype(float)\n",
    "        #ind = int((f.split('/')[-1])[:-4])\n",
    "        pos_z = -int((f.split('/')[-1])[:-4])\n",
    "        imgs[pos_z] = img\n",
    "\n",
    "\n",
    "    #sample_z = np.linspace(0, len(imgs)-1, RESOL, dtype=int)\n",
    "\n",
    "    imgs_3d = []\n",
    "    n_imgs = len(imgs)    \n",
    "    z_crop_range= (absolute_crop[0:2]*n_imgs).astype(int)\n",
    "\n",
    "    #print(z_crop_range)\n",
    "    for i, k in enumerate(sorted(imgs.keys())):\n",
    "        #if i in sample_z:\n",
    "        if(i >= z_crop_range[0] and i < z_crop_range[1]):\n",
    "            img = imgs[k]\n",
    "            imgs_3d.append(img[None])\n",
    "        \n",
    "    imgs_3d = np.vstack(imgs_3d)\n",
    "    imgs_3d = ((imgs_3d - imgs_3d.min()) / (imgs_3d.max() - imgs_3d.min()))\n",
    "\n",
    "    if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        imgs_3d = 1.0 - imgs_3d\n",
    "\n",
    "    #Loaded original imgs_3d    \n",
    "    processed_img_3d = np.zeros((6, RESOL, RESOL, RESOL))\n",
    "\n",
    "    for i in range(0, 6):     \n",
    "        #To deal with almost not detected slices\n",
    "        try:   \n",
    "            processed_img_3d[i] = crop_resize_avg_and_std_3d(imgs_3d, crop_regions[i])\n",
    "        except:\n",
    "            processed_img_3d[i] = crop_resize_avg_and_std_3d(imgs_3d, np.array([0, 1, 0, 1, 0, 1]))\n",
    "\n",
    "    #here to\n",
    "    #gzip too slow maybe I should divide the inference process to chunks or do not save in the inference notebooks\\\n",
    "    \n",
    "    \n",
    "    processed_img_3d = (processed_img_3d).astype(np.float32)\n",
    "    \n",
    "    processed_img_3d = processed_img_3d.reshape(6*RESOL, RESOL*RESOL)*65535\n",
    "    processed_img_3d = processed_img_3d.astype(np.uint16)\n",
    "    #save_png(save_path, processed_img_3d)\n",
    "    compress_fast(save_path, processed_img_3d)                      \n",
    "    #save_pickle(save_path, processed_img_3d)\n",
    "\n",
    "    del imgs, img, processed_img_3d\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "rng_samples = np.linspace(0, len(train_meta_df), N_PREPROCESS_CHUNKS+1, dtype = int)\n",
    "def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = train_meta_df):\n",
    "    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n",
    "        if not os.path.isfile(train_meta_df.iloc[i]['path']):\n",
    "            process_3d(train_meta_df.iloc[i]['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AbdominalCTDataset(Dataset):\n",
    "    def __init__(self, meta_df, is_train = True, transform_set = None):\n",
    "        self.meta_df = meta_df\n",
    "        self.is_train = is_train\n",
    "        self.transform_set = transform_set\n",
    "        self.data_3ds = []\n",
    "        for i in tqdm(range(0, len(self.meta_df))):\n",
    "            tmp_data_3d = decompress_fast(self.meta_df.iloc[i]['cropped_path'])[None]\n",
    "            tmp_data_3d = torch.from_numpy(tmp_data_3d)\n",
    "            self.data_3ds.append(tmp_data_3d)\n",
    "        self.data_3ds = torch.cat(self.data_3ds, dim = 0)        \n",
    "    def __len__(self):\n",
    "        return len(self.meta_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.meta_df.iloc[idx]\n",
    "        label = row[['bowel_healthy','bowel_injury',\n",
    "                    'extravasation_healthy','extravasation_injury',\n",
    "                    'kidney_healthy','kidney_low','kidney_high',\n",
    "                    'liver_healthy','liver_low','liver_high',\n",
    "                    'spleen_healthy','spleen_low','spleen_high', 'any_injury']]\n",
    "\n",
    "        #data_3d = decompress_fast(row['cropped_path'])\n",
    "        #data_3d = torch.from_numpy(data_3d)\n",
    "        #data_3d = torch.clone(self.data_3ds[idx])\n",
    "        data_3d = self.data_3ds[idx]\n",
    "        if self.transform_set is not None:\n",
    "            data_3d = self.transform_set({'image':data_3d})\n",
    "            data_3d = data_3d['image']\n",
    "        \n",
    "\n",
    "        label = label.to_numpy().astype(np.float32)                    \n",
    "        label = torch.from_numpy(label)\n",
    "                \n",
    "        \n",
    "        return data_3d, label        \n",
    "\n",
    "#train_dataset = AbdominalCTDataset(train_meta_df)\n",
    "#data_3d, label = train_dataset[0]\n",
    "#print(label)\n",
    "\n",
    "#del train_dataset, data_3d, label\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbdominalCTIndexDataset(Dataset):\n",
    "    def __init__(self, meta_df, is_train = True, transform_set = None):\n",
    "        self.meta_df = meta_df\n",
    "        self.indices = np.arange(len(self.meta_df))\n",
    "    def __len__(self):\n",
    "        return len(self.meta_df)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        return self.indices[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm.models.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.layers.conv2d_same import Conv2dSame\n",
    "from conv3d_same import Conv3dSame\n",
    "\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "\n",
    "#m = TimmSegModel(backbone)\n",
    "#m = convert_3d(m)\n",
    "#out = m(torch.rand(1, 1, 128,128,128))\n",
    "#for i in range(0, len(out)):\n",
    "#    print(out[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm_new.create_model(\n",
    "            backbone,\n",
    "            in_chans=N_CHANNELS,\n",
    "            features_only=True,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, N_CHANNELS, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        \n",
    "        #if segtype == 'unet':\n",
    "        #    self.decoder = smp.unet.decoder.UnetDecoder(\n",
    "        #        encoder_channels=encoder_channels[:n_blocks+1],\n",
    "        #        decoder_channels=decoder_channels[:n_blocks],\n",
    "        #        n_blocks=n_blocks,\n",
    "        #    )\n",
    "        self.avgpool = nn.AvgPool2d(5, 4, 2)\n",
    "        \n",
    "        [_.shape[1] for _ in g]\n",
    "        self.convs1x1 = nn.ModuleList()    \n",
    "        self.batchnorms = nn.ModuleList()    \n",
    "        self.batchnorms13 = nn.ModuleList()\n",
    "        for i in range(0, len(g)):\n",
    "            self.convs1x1.append(nn.Conv2d(g[i].shape[1], 13, 1))\n",
    "            self.batchnorms.append(nn.BatchNorm2d(g[i].shape[1]))\n",
    "            self.batchnorms13.append(nn.BatchNorm2d(13))\n",
    "\n",
    "        del g\n",
    "        gc.collect()\n",
    "    def forward(self,x):\n",
    "        global_features = self.encoder(x)[:n_blocks]        \n",
    "        for i in range(0, len(global_features)):\n",
    "            #global_features[i] = self.batchnorms[i](global_features[i])\n",
    "            global_features[i] = self.convs1x1[i](global_features[i])\n",
    "            #global_features[i] = self.batchnorms13[i](global_features[i])\n",
    "            \n",
    "            #global_features[i] = self.avgpool(global_features[i])\n",
    "        return global_features\n",
    "        #seg_features = self.decoder(*global_features)\n",
    "        #seg_features = self.segmentation_head(seg_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbdominalClassifier(nn.Module):\n",
    "    def __init__(self, device = DEVICE):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.upsample = torch.nn.Upsample(size = [UP_RESOL, UP_RESOL, UP_RESOL])\n",
    "        self.resnet3d = TimmSegModel(backbone)\n",
    "        self.resnet3d = convert_3d(self.resnet3d)\n",
    "        #self.resnet3d.load_state_dict(torch.load(f'{BASE_PATH}/seg_models_backup/timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep_fold0_best.pth'), strict=False)\n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.dropout  = nn.Dropout(p=0.5)\n",
    "        self.softmax  = nn.Softmax(dim=1)        \n",
    "        self.maxpool  = nn.MaxPool1d(5, 1)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.upsample(x)\n",
    "        x = self.resnet3d(x)\n",
    "        pooled_features = []\n",
    "        for i in range(0, len(x)):        \n",
    "            #pooled_features.append(self.flatten(torch.sum(x[i], dim = 1)))\n",
    "            pooled_features.append(torch.reshape(torch.mean(x[i], dim = (2, 3, 4)), (batch_size, 13, 1)))\n",
    "            \n",
    "        x = torch.cat(pooled_features, dim=2)\n",
    "        labels = torch.mean(x, dim=2)\n",
    "        \n",
    "        bowel_soft = self.softmax(labels[:,0:2])\n",
    "        extrav_soft = self.softmax(labels[:,2:4])\n",
    "        kidney_soft = self.softmax(labels[:,4:7])\n",
    "        liver_soft = self.softmax(labels[:,7:10])\n",
    "        spleen_soft = self.softmax(labels[:,10:13])\n",
    "\n",
    "        any_in = torch.cat([1-bowel_soft[:,0:1], 1-extrav_soft[:,0:1], \n",
    "                            1-kidney_soft[:,0:1], 1-liver_soft[:,0:1], 1-spleen_soft[:,0:1]], dim = 1) \n",
    "        any_in = self.maxpool(any_in)\n",
    "        any_not_in = 1-any_in\n",
    "        any_in = torch.cat([any_not_in, any_in], dim = 1)\n",
    "\n",
    "        return labels, any_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28521267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AbdominalClassifier()\n",
    "\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "print(get_n_params(model))\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbdominalClassifier()\n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=ttl_iters, eta_min=1e-6)\n",
    "\n",
    "\n",
    "weights = np.ones(2)\n",
    "weights[1] = 2\n",
    "crit_bowel  = nn.CrossEntropyLoss(weight = torch.from_numpy(weights).to(DEVICE))\n",
    "weights[1] = 6\n",
    "crit_extrav = nn.CrossEntropyLoss(weight = torch.from_numpy(weights).to(DEVICE))\n",
    "crit_any = nn.CrossEntropyLoss(weight = torch.from_numpy(weights).to(DEVICE))\n",
    "\n",
    "weights = np.ones((3))\n",
    "weights[1] = 2\n",
    "weights[2] = 4\n",
    "crit_kidney = nn.CrossEntropyLoss(weight = torch.from_numpy(weights).to(DEVICE))\n",
    "crit_liver  = nn.CrossEntropyLoss(weight = torch.from_numpy(weights).to(DEVICE))\n",
    "crit_spleen = nn.CrossEntropyLoss(weight = torch.from_numpy(weights).to(DEVICE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_one(tensor):\n",
    "    norm = torch.sum(tensor, 1)\n",
    "    for i in range(0, tensor.shape[1]):\n",
    "        tensor[:,i]/=norm\n",
    "    return tensor\n",
    "\n",
    "def apply_softmax_to_labels(X_out):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    X_out[:,:2]    = normalize_to_one(softmax(X_out[:,:2]))\n",
    "    X_out[:,2:4]   = normalize_to_one(softmax(X_out[:,2:4]))\n",
    "    X_out[:,4:7]   = normalize_to_one(softmax(X_out[:,4:7]))\n",
    "    X_out[:,7:10]  = normalize_to_one(softmax(X_out[:,7:10]))\n",
    "    X_out[:,10:13] = normalize_to_one(softmax(X_out[:,10:13]))\n",
    "\n",
    "    return X_out\n",
    "\n",
    "def calculate_score(X_outs, ys, step = 'train'):\n",
    "    X_outs = X_outs.astype(np.float64)\n",
    "    ys     = ys.astype(np.float64)\n",
    "\n",
    "    bowel_weights  =  ys[:,0] + 2*ys[:,1]\n",
    "    extrav_weights = ys[:,2] + 6*ys[:,3]\n",
    "    kidney_weights = ys[:,4] + 2*ys[:,5] + 4*ys[:,6]\n",
    "    liver_weights  = ys[:,7] + 2*ys[:,8] + 4*ys[:,9]\n",
    "    spleen_weights = ys[:,10] + 2*ys[:,11] + 4*ys[:,12]\n",
    "    any_in_weights = ys[:,13] + 6*ys[:,14]\n",
    "    \n",
    "\n",
    "    bowel_loss  = sklearn.metrics.log_loss(ys[:,:2], X_outs[:,:2], sample_weight = bowel_weights)\n",
    "    extrav_loss = sklearn.metrics.log_loss(ys[:,2:4], X_outs[:,2:4], sample_weight = extrav_weights)\n",
    "    kidney_loss = sklearn.metrics.log_loss(ys[:,4:7], X_outs[:,4:7], sample_weight = kidney_weights)\n",
    "    liver_loss  = sklearn.metrics.log_loss(ys[:,7:10], X_outs[:,7:10], sample_weight = liver_weights)\n",
    "    spleen_loss = sklearn.metrics.log_loss(ys[:,10:13], X_outs[:,10:13], sample_weight = spleen_weights)\n",
    "    any_in_loss = sklearn.metrics.log_loss(ys[:,13:15], X_outs[:,13:15], sample_weight =  any_in_weights)\n",
    "    \n",
    "    avg_loss = (bowel_loss + extrav_loss + kidney_loss + liver_loss + spleen_loss + any_in_loss)/6\n",
    "\n",
    "    losses= {f'{step}_bowel_metric': bowel_loss, f'{step}_extrav_metric': extrav_loss, f'{step}_kidney_metric': kidney_loss,\n",
    "             f'{step}_liver_metric': liver_loss, f'{step}_spleen_metric': spleen_loss, f'{step}_any_in_metric': any_in_loss,\n",
    "             f'{step}_avg_metric': avg_loss}\n",
    "\n",
    "    wandb.log(losses)\n",
    "    return avg_loss\n",
    "\n",
    "def calculate_loss(X_out, X_any, y):\n",
    "    batch_size = X_out.shape[0]\n",
    "    bowel_loss  = crit_bowel(X_out[:,:2], y[:,:2])\n",
    "    extrav_loss = crit_extrav(X_out[:,2:4], y[:,2:4])\n",
    "    kidney_loss = crit_kidney(X_out[:,4:7], y[:,4:7])\n",
    "    liver_loss  = crit_liver(X_out[:,7:10], y[:,7:10])\n",
    "    spleen_loss = crit_spleen(X_out[:,10:13], y[:,10:13])\n",
    "    any_in_loss = crit_any(X_any,  torch.cat([torch.ones(batch_size, 1).to(DEVICE)- y[:,13:14],y[:,13:14]], dim = 1))\n",
    "    \n",
    "    avg_loss = (bowel_loss + extrav_loss + kidney_loss + liver_loss + spleen_loss + any_in_loss)/6\n",
    "    return bowel_loss, extrav_loss, kidney_loss, liver_loss, spleen_loss, any_in_loss, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(inputs, truth, clip=[0, 1]):\n",
    "    indices = torch.randperm(inputs.size(0))\n",
    "    shuffled_input = inputs[indices]\n",
    "    shuffled_labels = truth[indices]\n",
    "\n",
    "    lam = np.random.uniform(clip[0], clip[1])\n",
    "    inputs = inputs * lam + shuffled_input * (1 - lam)\n",
    "    return inputs, truth, shuffled_labels, lam\n",
    "\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandFlipd(keys=[\"image\"], prob=0.5, spatial_axis=1),\n",
    "    transforms.RandFlipd(keys=[\"image\"], prob=0.5, spatial_axis=2),\n",
    "    transforms.RandAffined(keys=[\"image\"], translate_range=[int(x*y) for x, y in zip([RESOL, RESOL, RESOL], [0.3, 0.3, 0.3])], padding_mode='zeros', prob=0.7),\n",
    "    transforms.RandGridDistortiond(keys=(\"image\"), prob=0.5, distort_limit=(-0.01, 0.01), mode=\"nearest\"),    \n",
    "    #monai.transforms.RandGibbsNoise(prob=0.1, alpha=(0.0, 1.0)),\n",
    "])\n",
    "\n",
    "transforms_valid = transforms.Compose([\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rng_selected_inds = np.linspace(0, len(train_meta_df), tmp_n_workers+1, dtype = int)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection/wandb/run-20230917_185453-s46id6fx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/junseonglee/RSNA_ABTD/runs/s46id6fx' target=\"_blank\">seresnext50_32x4d.racm_in1k_mixup0.1</a></strong> to <a href='https://wandb.ai/junseonglee/RSNA_ABTD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/junseonglee/RSNA_ABTD' target=\"_blank\">https://wandb.ai/junseonglee/RSNA_ABTD</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/junseonglee/RSNA_ABTD/runs/s46id6fx' target=\"_blank\">https://wandb.ai/junseonglee/RSNA_ABTD/runs/s46id6fx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3782/3782 [01:42<00:00, 37.04it/s]\n",
      "100%|██████████| 929/929 [00:16<00:00, 56.88it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / trn/loss=0.9182\n",
      "Epoch 1 / train/metric=0.6640\n",
      "Epoch 1 / val/metric=0.5884\n",
      "Best val_metric 0.5883762900394015 at epoch 1!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [04:05<6:44:32, 245.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / trn/loss=0.8467\n",
      "Epoch 2 / train/metric=0.5989\n",
      "Epoch 2 / val/metric=0.5817\n",
      "Best val_metric 0.5816546881689556 at epoch 2!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [07:42<6:13:24, 228.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 / trn/loss=0.8418\n",
      "Epoch 3 / train/metric=0.5944\n",
      "Epoch 3 / val/metric=0.5734\n",
      "Best val_metric 0.5734003045978874 at epoch 3!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [11:16<5:58:57, 222.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / trn/loss=0.8343\n",
      "Epoch 4 / train/metric=0.5886\n",
      "Epoch 4 / val/metric=0.5646\n",
      "Best val_metric 0.564556318771054 at epoch 4!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [14:51<5:50:58, 219.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 / trn/loss=0.8287\n",
      "Epoch 5 / train/metric=0.5837\n",
      "Epoch 5 / val/metric=0.5587\n",
      "Best val_metric 0.5586953679724718 at epoch 5!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [18:22<5:42:37, 216.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 / trn/loss=0.8200\n",
      "Epoch 6 / train/metric=0.5754\n",
      "Epoch 6 / val/metric=0.5526\n",
      "Best val_metric 0.5525669021385523 at epoch 6!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [21:57<5:38:05, 215.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 / trn/loss=0.8095\n",
      "Epoch 7 / train/metric=0.5651\n",
      "Epoch 7 / val/metric=0.5458\n",
      "Best val_metric 0.5457564477992639 at epoch 7!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [25:31<5:33:30, 215.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 / trn/loss=0.8013\n",
      "Epoch 8 / train/metric=0.5577\n",
      "Epoch 8 / val/metric=0.5431\n",
      "Best val_metric 0.5430945142922997 at epoch 8!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [29:01<5:27:34, 213.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 / trn/loss=0.7977\n",
      "Epoch 9 / train/metric=0.5579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [32:30<5:21:46, 212.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 / val/metric=0.5518\n",
      "Epoch 10 / trn/loss=0.8010\n",
      "Epoch 10 / train/metric=0.5576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [36:03<5:18:48, 212.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / val/metric=0.5628\n",
      "Epoch 11 / trn/loss=0.7933\n",
      "Epoch 11 / train/metric=0.5538\n",
      "Epoch 11 / val/metric=0.5420\n",
      "Best val_metric 0.5419760230997055 at epoch 11!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [39:41<5:17:44, 214.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 / trn/loss=0.7918\n",
      "Epoch 12 / train/metric=0.5505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [43:16<5:14:13, 214.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 / val/metric=0.5466\n",
      "Epoch 13 / trn/loss=0.7925\n",
      "Epoch 13 / train/metric=0.5513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [46:50<5:10:25, 214.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 / val/metric=0.5463\n",
      "Epoch 14 / trn/loss=0.7909\n",
      "Epoch 14 / train/metric=0.5541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [50:27<5:08:16, 215.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 / val/metric=0.6102\n",
      "Epoch 15 / trn/loss=0.7829\n",
      "Epoch 15 / train/metric=0.5470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [53:57<5:02:28, 213.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 / val/metric=0.6081\n",
      "Epoch 16 / trn/loss=0.7860\n",
      "Epoch 16 / train/metric=0.5455\n",
      "Epoch 16 / val/metric=0.5341\n",
      "Best val_metric 0.5341110681405637 at epoch 16!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [57:28<4:57:48, 212.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 / trn/loss=0.7833\n",
      "Epoch 17 / train/metric=0.5465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [1:01:05<4:56:02, 214.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 / val/metric=0.5342\n",
      "Epoch 18 / trn/loss=0.7897\n",
      "Epoch 18 / train/metric=0.5486\n",
      "Epoch 18 / val/metric=0.5302\n",
      "Best val_metric 0.5301994923380444 at epoch 18!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [1:04:40<4:53:07, 214.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 / trn/loss=0.7757\n",
      "Epoch 19 / train/metric=0.5389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [1:08:12<4:48:36, 213.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 / val/metric=0.5436\n",
      "Epoch 20 / trn/loss=0.7799\n",
      "Epoch 20 / train/metric=0.5416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [1:11:49<4:46:01, 214.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 / val/metric=0.5670\n",
      "Epoch 21 / trn/loss=0.7725\n",
      "Epoch 21 / train/metric=0.5354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [1:15:25<4:43:10, 215.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / val/metric=0.5431\n",
      "Epoch 22 / trn/loss=0.7791\n",
      "Epoch 22 / train/metric=0.5393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [1:19:01<4:40:07, 215.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 / val/metric=0.5728\n",
      "Epoch 23 / trn/loss=0.7724\n",
      "Epoch 23 / train/metric=0.5367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [1:22:34<4:35:34, 214.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 / val/metric=0.5603\n",
      "Epoch 24 / trn/loss=0.7734\n",
      "Epoch 24 / train/metric=0.5308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [1:26:21<4:36:29, 218.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 / val/metric=0.5311\n",
      "Epoch 25 / trn/loss=0.7638\n",
      "Epoch 25 / train/metric=0.5282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [1:29:55<4:31:23, 217.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 / val/metric=0.5331\n",
      "Epoch 26 / trn/loss=0.7611\n",
      "Epoch 26 / train/metric=0.5258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [1:33:29<4:26:27, 216.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 / val/metric=0.5458\n",
      "Epoch 27 / trn/loss=0.7552\n",
      "Epoch 27 / train/metric=0.5203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [1:37:00<4:21:11, 214.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 / val/metric=0.5382\n",
      "Epoch 28 / trn/loss=0.7596\n",
      "Epoch 28 / train/metric=0.5208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [1:40:38<4:18:49, 215.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 / val/metric=0.5305\n",
      "Epoch 29 / trn/loss=0.7597\n",
      "Epoch 29 / train/metric=0.5171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [1:44:19<4:17:02, 217.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 / val/metric=0.5308\n",
      "Epoch 30 / trn/loss=0.7360\n",
      "Epoch 30 / train/metric=0.5058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [1:47:53<4:12:11, 216.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 / val/metric=0.5881\n",
      "Epoch 31 / trn/loss=0.7299\n",
      "Epoch 31 / train/metric=0.5044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [1:51:32<4:09:27, 216.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 / val/metric=0.5403\n",
      "Epoch 32 / trn/loss=0.7401\n",
      "Epoch 32 / train/metric=0.5081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [1:55:05<4:04:43, 215.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / val/metric=0.5501\n",
      "Epoch 33 / trn/loss=0.7357\n",
      "Epoch 33 / train/metric=0.5013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [1:58:39<4:00:28, 215.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 / val/metric=0.5771\n",
      "Epoch 34 / trn/loss=0.7184\n",
      "Epoch 34 / train/metric=0.4900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [2:02:14<3:56:47, 215.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 / val/metric=0.6279\n",
      "Epoch 35 / trn/loss=0.7322\n",
      "Epoch 35 / train/metric=0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [2:05:45<3:51:44, 213.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 / val/metric=0.6324\n",
      "Epoch 36 / trn/loss=0.7266\n",
      "Epoch 36 / train/metric=0.4960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [2:09:20<3:48:30, 214.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 / val/metric=0.5947\n",
      "Epoch 37 / trn/loss=0.7140\n",
      "Epoch 37 / train/metric=0.4858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [2:12:54<3:44:57, 214.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 / val/metric=0.5455\n",
      "Epoch 38 / trn/loss=0.7205\n",
      "Epoch 38 / train/metric=0.4856\n",
      "Epoch 38 / val/metric=0.5246\n",
      "Best val_metric 0.5245904874424305 at epoch 38!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [2:16:28<3:41:12, 214.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 / trn/loss=0.7003\n",
      "Epoch 39 / train/metric=0.4686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [2:20:04<3:38:20, 214.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 / val/metric=0.5399\n",
      "Epoch 40 / trn/loss=0.7109\n",
      "Epoch 40 / train/metric=0.4758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [2:23:46<3:36:55, 216.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 / val/metric=0.5686\n",
      "Epoch 41 / trn/loss=0.6939\n",
      "Epoch 41 / train/metric=0.4665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [2:27:23<3:33:13, 216.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 / val/metric=0.5468\n",
      "Epoch 42 / trn/loss=0.6935\n",
      "Epoch 42 / train/metric=0.4642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [2:31:01<3:30:00, 217.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 / val/metric=0.6607\n",
      "Epoch 43 / trn/loss=0.6842\n",
      "Epoch 43 / train/metric=0.4560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [2:34:36<3:25:49, 216.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / val/metric=0.5675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted size vs. prev_size\n",
      "                                                     \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 503235) is killed by signal: Aborted. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/baseline_3d_v2_crop.ipynb Cell 24\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/baseline_3d_v2_crop.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m scaler\u001b[39m.\u001b[39mscale(avg_loss\u001b[39m/\u001b[39maccum_scale[accum_counter])\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/baseline_3d_v2_crop.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39mif\u001b[39;00m(counter\u001b[39m==\u001b[39maccum_points[accum_counter]):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/baseline_3d_v2_crop.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m     scaler\u001b[39m.\u001b[39mstep(optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/baseline_3d_v2_crop.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/baseline_3d_v2_crop.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     scaler\u001b[39m.\u001b[39mupdate()    \n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:416\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    412\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    413\u001b[0m     \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    414\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 416\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    418\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n\u001b[1;32m    420\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_opt_step\u001b[39m(\u001b[39mself\u001b[39m, optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    315\u001b[0m         retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_opt_step\u001b[39m(\u001b[39mself\u001b[39m, optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    315\u001b[0m         retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[39m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m previous_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(previous_handler)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 503235) is killed by signal: Aborted. "
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    config = wandb_config,\n",
    "    project= PROJECT_NAME,\n",
    "    group  = GROUP_NAME,\n",
    "    name   = RUN_NAME,\n",
    "    dir    = BASE_PATH)\n",
    "\n",
    "backbone = backbone.replace('/', '_')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_dataset = AbdominalCTDataset(train_meta_df[train_meta_df['fold']!=0], is_train = True, transform_set  = transforms_train)\n",
    "    valid_dataset = AbdominalCTDataset(train_meta_df[train_meta_df['fold']==0], is_train = False, transform_set = transforms_valid)\n",
    "        \n",
    "    \n",
    "    train_loader = DataLoader(dataset = train_dataset, shuffle = True, batch_size = BATCH_SIZE, pin_memory = False, \n",
    "                            num_workers = N_WORKERS, drop_last = False)\n",
    "\n",
    "    valid_loader = DataLoader(dataset = valid_dataset, shuffle = False, batch_size = BATCH_SIZE, pin_memory = False, \n",
    "                            num_workers = N_WORKERS, drop_last = False)     \n",
    "    \n",
    "    ttl_iters = N_EPOCHS * len(train_loader)\n",
    "    \n",
    "    #gradient accumulation for stability of the training\n",
    "    accum_len = int(np.ceil(len(train_loader)/ACCUM_STEPS)+0.001)\n",
    "    accum_points = np.zeros(accum_len, int)\n",
    "    accum_scale  = np.zeros(accum_len, int)\n",
    "    \n",
    "    prev_step = -1\n",
    "    for i in range(0, accum_len):\n",
    "        accum_points[i] = min(prev_step+ACCUM_STEPS, len(train_loader)-1)\n",
    "        accum_scale[i]  = accum_points[i] - prev_step\n",
    "        prev_step = accum_points[i]\n",
    "    \n",
    "    #print(accum_points)\n",
    "    #print(accum_scale)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = LR)\n",
    "    n_batch_iters = len(train_loader)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR, \n",
    "                                                    steps_per_epoch= n_batch_iters, epochs = N_EPOCHS)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "    gc.collect()\n",
    "\n",
    "    val_metrics = np.ones(N_EPOCHS)*100\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    for epoch in tqdm(range(0, N_EPOCHS), leave = False):     \n",
    "        train_meters = {'loss': AverageMeter()}\n",
    "        val_meters   = {'loss': AverageMeter()}\n",
    "        \n",
    "        model.train()\n",
    "        #pbar = tqdm(train_loader, leave=False)  \n",
    "\n",
    "        X_outs=[]\n",
    "        ys=[]\n",
    "        accum_counter = 0\n",
    "        counter = 0\n",
    "        last_count_on = False\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            current_lr = float(scheduler.get_last_lr()[0])\n",
    "            wandb.log({'lr': current_lr})\n",
    "            \n",
    "            batch_size = X.shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=True):  \n",
    "                X_out, X_any  = model(X)\n",
    "                do_mixup = False\n",
    "                if np.random.random() < p_mixup:\n",
    "                    do_mixup = True\n",
    "                    X, y, labels_shuffled, lam = mixup(X, y)                \n",
    "                \n",
    "                bowel_loss, extrav_loss, kidney_loss, liver_loss, spleen_loss, any_in_loss, avg_loss = calculate_loss(X_out, X_any, y)\n",
    "                if do_mixup:\n",
    "                    bowel_loss2, extrav_loss2, kidney_loss2, liver_loss2, spleen_loss2, any_in_loss2, avg_loss2 = calculate_loss(X_out, X_any, labels_shuffled)\n",
    "                    bowel_loss  = bowel_loss * lam  + bowel_loss2 * (1 - lam)\n",
    "                    extrav_loss = extrav_loss * lam  + extrav_loss2 * (1 - lam)\n",
    "                    kidney_loss = kidney_loss * lam  + kidney_loss2 * (1 - lam)         \n",
    "                    liver_loss  = liver_loss * lam  + liver_loss2 * (1 - lam) \n",
    "                    spleen_loss = spleen_loss * lam  + spleen_loss2 * (1 - lam) \n",
    "                    any_in_loss = any_in_loss * lam  + any_in_loss2 * (1 - lam) \n",
    "                    avg_loss = avg_loss * lam  + avg_loss2 * (1 - lam)       \n",
    "                    \n",
    "                step = 'train'\n",
    "                wandb.log({f'{step}_bowel_loss': bowel_loss.item(),\n",
    "                           f'{step}_extrav_loss': extrav_loss.item(),\n",
    "                           f'{step}_kidney_loss': kidney_loss.item(),\n",
    "                           f'{step}_liver_loss': liver_loss.item(),\n",
    "                           f'{step}_spleen_loss': spleen_loss.item(),\n",
    "                           f'{step}_any_loss': any_in_loss.item(),\n",
    "                           f'{step}_avg_loss': avg_loss.item()\n",
    "                           })\n",
    "                \n",
    "                scaler.scale(avg_loss/accum_scale[accum_counter]).backward()\n",
    "                if(counter==accum_points[accum_counter]):\n",
    "                    scaler.step(optimizer)\n",
    "                    scheduler.step()\n",
    "                    scaler.update()    \n",
    "                    accum_counter+=1\n",
    "                \n",
    "            counter+=1                   \n",
    "\n",
    "            #Metric calculation\n",
    "            y_any = torch.cat([torch.ones(batch_size, 1).to(DEVICE)- y[:,13:14],y[:,13:14]], dim = 1)    \n",
    "            X_out = apply_softmax_to_labels(X_out).detach().to('cpu').numpy()\n",
    "            X_any = X_any.detach().to('cpu').numpy()\n",
    "            X_out = np.hstack([X_out, X_any])\n",
    "            X_outs.append(X_out)\n",
    "\n",
    "            y     = y.to('cpu').numpy()[:,:-1]\n",
    "            y_any = y_any.to('cpu').numpy()\n",
    "            y     = np.hstack([y, y_any])\n",
    "            ys.append(y)\n",
    "\n",
    "            trn_loss = avg_loss.item()      \n",
    "            train_meters['loss'].update(trn_loss, n=X.size(0))     \n",
    "            #pbar.set_description(f'Train loss: {trn_loss}')   \n",
    "            \n",
    "            \n",
    "        print('Epoch {:d} / trn/loss={:.4f}'.format(epoch+1, train_meters['loss'].avg))    \n",
    "\n",
    "        X_outs = np.vstack(X_outs) \n",
    "        ys     = np.vstack(ys)\n",
    "        metric = calculate_score(X_outs, ys, 'train')                 \n",
    "        print('Epoch {:d} / train/metric={:.4f}'.format(epoch+1, metric))   \n",
    "\n",
    "        del X, X_outs, y, ys, X_any\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        X_outs=[]\n",
    "        ys=[]\n",
    "        model.eval()\n",
    "        for X, y in valid_loader:            \n",
    "            batch_size = X.shape[0]        \n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)                 \n",
    "            with torch.cuda.amp.autocast(enabled=True):                \n",
    "                with torch.no_grad():                 \n",
    "                    X_out, X_any = model(X)                                           \n",
    "                    y_any = torch.cat([torch.ones(batch_size, 1).to(DEVICE)- y[:,13:14],y[:,13:14]], dim = 1)              \n",
    "                              \n",
    "                    X_out = apply_softmax_to_labels(X_out).to('cpu').numpy()\n",
    "\n",
    "                    X_any = X_any.to('cpu').numpy()\n",
    "                    X_out = np.hstack([X_out, X_any])\n",
    "                    X_outs.append(X_out)\n",
    "\n",
    "                    y     = y.to('cpu').numpy()[:,:-1]\n",
    "                    y_any = y_any.to('cpu').numpy()\n",
    "                    y     = np.hstack([y, y_any])\n",
    "                    ys.append(y)\n",
    "\n",
    "        X_outs = np.vstack(X_outs) \n",
    "        ys     = np.vstack(ys)\n",
    "        metric = calculate_score(X_outs, ys, 'valid')                \n",
    "        print('Epoch {:d} / val/metric={:.4f}'.format(epoch+1, metric))           \n",
    "        \n",
    "        del X, X_outs, y, ys, X_any\n",
    "        gc.collect()        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        #Save the best model    \n",
    "        if(metric < np.min(val_metrics)):\n",
    "            try:\n",
    "                os.makedirs(f'{BASE_PATH}/weights')\n",
    "            except:\n",
    "                a = 1\n",
    "            best_metric = metric\n",
    "            print(f'Best val_metric {best_metric} at epoch {epoch+1}!')\n",
    "            torch.save(model, f'{BASE_PATH}/weights/{backbone}_lr{LR}_epochs_{N_EPOCHS}_resol{UP_RESOL}_batch{BATCH_SIZE}.pt')    \n",
    "            not_improve_counter=0\n",
    "            val_metrics[epoch] = metric\n",
    "            continue            \n",
    "        \n",
    "        val_metrics[epoch] = metric                        \n",
    "        not_improve_counter+=1\n",
    "        if(not_improve_counter == EARLY_STOP_COUNT):\n",
    "            print(f'Not improved for {not_improve_counter} epochs, terminate the train')\n",
    "            break\n",
    "wandb.log({'best_total_log_loss': best_metric})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "try:\n",
    "    wandb.log({'best_total_log_loss': best_metric})\n",
    "    wandb.finish()\n",
    "    \n",
    "except:\n",
    "    print('Wandb is already finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
