{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Modification of the original code\n","I just took the code of the winner of the RSNA 2022 cervical spine fracture detection competition.  \n","Link: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage1"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:25.743219Z","iopub.status.busy":"2022-10-29T06:00:25.742950Z","iopub.status.idle":"2022-10-29T06:00:34.162024Z","shell.execute_reply":"2022-10-29T06:00:34.160905Z","shell.execute_reply.started":"2022-10-29T06:00:25.743184Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/junseonglee/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import sys\n","import gc\n","import ast\n","import cv2\n","import imageio\n","import time\n","import timm\n","import pickle\n","import random\n","import pydicom\n","import dicomsdl\n","import argparse\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import glob\n","import nibabel as nib\n","from PIL import Image\n","from tqdm import tqdm\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gzip\n","import pickle\n","from joblib import Parallel, delayed\n","import lz4.frame\n","import mgzip\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.cuda.amp as amp\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","\n","\n","%matplotlib inline\n","rcParams['figure.figsize'] = 20, 8\n","device = torch.device('cuda')\n","torch.backends.cudnn.benchmark = True\n","\n","os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n","sys.path.append('./lib_models')"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:29.485211Z","iopub.status.busy":"2022-10-29T06:03:29.484820Z","iopub.status.idle":"2022-10-29T06:03:29.494317Z","shell.execute_reply":"2022-10-29T06:03:29.493294Z","shell.execute_reply.started":"2022-10-29T06:03:29.485168Z"},"trusted":true},"outputs":[],"source":["RESOL = 128\n","\n","BASE_PATH = '/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection'\n","MASK_SAVE_PATH = f'{BASE_PATH}/mask_preprocessed'\n","MASK_VALID_PATH = f'{BASE_PATH}/mask_validation'\n","TRAIN_PATH = f'{BASE_PATH}/train_images'\n","\n","BATCH_MASK_PRED = 8\n","N_PROCESS_CROP = 12\n","\n","kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n","load_kernel = None\n","load_last = True\n","n_blocks = 4\n","backbone = 'resnet18d'\n","\n","image_sizes = [128, 128, 128]\n","data_dir = '../input/rsna-2022-cervical-spine-fracture-detection'\n","use_amp = True\n","\n","\n","num_workers = 24\n","out_dim = 5\n","\n","model_dir = f'{BASE_PATH}/seg_models_backup'\n","seg_inference_dir = f'{BASE_PATH}/seg_infer_results'\n","cropped_img_dir   = f'{BASE_PATH}/3d_preprocessed_crop_ratio'\n","os.makedirs(model_dir, exist_ok=True)\n","os.makedirs(MASK_VALID_PATH, exist_ok=True)\n","os.makedirs(seg_inference_dir, exist_ok = True)\n","os.makedirs(cropped_img_dir, exist_ok = True)"]},{"cell_type":"markdown","metadata":{},"source":["# Path to save cropped image"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.404497Z","iopub.status.busy":"2022-10-29T06:00:34.403952Z","iopub.status.idle":"2022-10-29T06:00:34.486417Z","shell.execute_reply":"2022-10-29T06:00:34.485281Z","shell.execute_reply.started":"2022-10-29T06:00:34.404459Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv(f'{BASE_PATH}/train_meta.csv')\n","mask_paths = []\n","cropped_paths = []\n","for i in range(0, len(df_train)):\n","    row = df_train.iloc[i]\n","    file_name = row['path'].split('/')[-1]\n","    mask_paths.append(f'{seg_inference_dir}/{file_name}')\n","    cropped_paths.append(f'{cropped_img_dir}/{file_name}')\n","df_train['cropped_path'] = cropped_paths\n","df_train['mask_path']    = mask_paths\n","df_train.tail()\n","\n","df_train.to_csv(f'{BASE_PATH}/train_meta.csv', index = False)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>patient_id</th>\n","      <th>series</th>\n","      <th>bowel_healthy</th>\n","      <th>bowel_injury</th>\n","      <th>extravasation_healthy</th>\n","      <th>extravasation_injury</th>\n","      <th>kidney_healthy</th>\n","      <th>kidney_low</th>\n","      <th>kidney_high</th>\n","      <th>liver_healthy</th>\n","      <th>liver_low</th>\n","      <th>liver_high</th>\n","      <th>spleen_healthy</th>\n","      <th>spleen_low</th>\n","      <th>spleen_high</th>\n","      <th>any_injury</th>\n","      <th>fold</th>\n","      <th>path</th>\n","      <th>mask_path</th>\n","      <th>cropped_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>26473</td>\n","      <td>11365</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>35348</td>\n","      <td>41274</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>37260</td>\n","      <td>21973</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37260</td>\n","      <td>19788</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>18301</td>\n","      <td>1407</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4706</th>\n","      <td>8076</td>\n","      <td>41087</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4707</th>\n","      <td>20087</td>\n","      <td>30769</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4708</th>\n","      <td>20087</td>\n","      <td>45305</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4709</th>\n","      <td>43639</td>\n","      <td>64933</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","    <tr>\n","      <th>4710</th>\n","      <td>43639</td>\n","      <td>59734</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4711 rows × 20 columns</p>\n","</div>"],"text/plain":["      patient_id  series  bowel_healthy  bowel_injury  extravasation_healthy  \\\n","0          26473   11365              1             0                      1   \n","1          35348   41274              1             0                      1   \n","2          37260   21973              1             0                      1   \n","3          37260   19788              1             0                      1   \n","4          18301    1407              1             0                      1   \n","...          ...     ...            ...           ...                    ...   \n","4706        8076   41087              1             0                      1   \n","4707       20087   30769              1             0                      1   \n","4708       20087   45305              1             0                      1   \n","4709       43639   64933              1             0                      1   \n","4710       43639   59734              1             0                      1   \n","\n","      extravasation_injury  kidney_healthy  kidney_low  kidney_high  \\\n","0                        0               1           0            0   \n","1                        0               1           0            0   \n","2                        0               1           0            0   \n","3                        0               1           0            0   \n","4                        0               1           0            0   \n","...                    ...             ...         ...          ...   \n","4706                     0               1           0            0   \n","4707                     0               0           1            0   \n","4708                     0               0           1            0   \n","4709                     0               1           0            0   \n","4710                     0               1           0            0   \n","\n","      liver_healthy  liver_low  liver_high  spleen_healthy  spleen_low  \\\n","0                 1          0           0               1           0   \n","1                 1          0           0               1           0   \n","2                 1          0           0               1           0   \n","3                 1          0           0               1           0   \n","4                 1          0           0               1           0   \n","...             ...        ...         ...             ...         ...   \n","4706              1          0           0               0           1   \n","4707              1          0           0               1           0   \n","4708              1          0           0               1           0   \n","4709              1          0           0               1           0   \n","4710              1          0           0               1           0   \n","\n","      spleen_high  any_injury  fold  \\\n","0               0           0     3   \n","1               0           0     0   \n","2               0           0     0   \n","3               0           0     0   \n","4               0           0     4   \n","...           ...         ...   ...   \n","4706            0           1     1   \n","4707            0           1     2   \n","4708            0           1     2   \n","4709            0           0     2   \n","4710            0           0     2   \n","\n","                                                   path  \\\n","0     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","1     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","2     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","3     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","...                                                 ...   \n","4706  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4707  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4708  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4709  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4710  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","\n","                                              mask_path  \\\n","0     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","1     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","2     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","3     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4     /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","...                                                 ...   \n","4706  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4707  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4708  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4709  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","4710  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","\n","                                           cropped_path  \n","0     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","1     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","2     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","3     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4     /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","...                                                 ...  \n","4706  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4707  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4708  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4709  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","4710  /home/junseonglee/Desktop/01_codes/inputs/rsna...  \n","\n","[4711 rows x 20 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df_train"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.497353Z","iopub.status.busy":"2022-10-29T06:00:34.496904Z","iopub.status.idle":"2022-10-29T06:00:34.514198Z","shell.execute_reply":"2022-10-29T06:00:34.513162Z","shell.execute_reply.started":"2022-10-29T06:00:34.497317Z"},"trusted":true},"outputs":[],"source":["def compress(name, data):\n","    with gzip.open(name, 'wb') as f:\n","        pickle.dump(data, f)\n","\n","def decompress(name):\n","    with gzip.open(name, 'rb') as f:\n","        data = pickle.load(f)\n","    return data\n","\n","def compress_fast(name, data):  \n","    np.save(name, data)\n","\n","def decompress_fast(name):\n","    data = np.load(f'{name}.npy')\n","    return data\n","\n","\n","class SEGDataset(Dataset):\n","    def __init__(self, df, mode):\n","\n","        self.df = df.reset_index()\n","        self.mode = mode\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        \n","        image = decompress(row['path'])[None]\n","        image = torch.from_numpy(image).to(torch.float32)\n","        save_path = row['mask_path']\n","\n","        return image, save_path\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.059903Z","iopub.status.busy":"2022-10-29T06:02:59.058663Z","iopub.status.idle":"2022-10-29T06:02:59.070787Z","shell.execute_reply":"2022-10-29T06:02:59.069705Z","shell.execute_reply.started":"2022-10-29T06:02:59.059860Z"},"trusted":true},"outputs":[],"source":["class TimmSegModel(nn.Module):\n","    def __init__(self, backbone, segtype='unet', pretrained=False):\n","        super(TimmSegModel, self).__init__()\n","\n","        self.encoder = timm.create_model(\n","            backbone,\n","            in_chans=1,\n","            features_only=True,\n","            pretrained=pretrained\n","        )\n","        g = self.encoder(torch.rand(1, 1, 64, 64))\n","        encoder_channels = [1] + [_.shape[1] for _ in g]\n","        decoder_channels = [256, 128, 64, 32, 16]\n","        if segtype == 'unet':\n","            self.decoder = smp.unet.decoder.UnetDecoder(\n","                encoder_channels=encoder_channels[:n_blocks+1],\n","                decoder_channels=decoder_channels[:n_blocks],\n","                n_blocks=n_blocks,\n","            )\n","\n","        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","    def forward(self,x):\n","        global_features = [0] + self.encoder(x)[:n_blocks]\n","        seg_features = self.decoder(*global_features)\n","        seg_features = self.segmentation_head(seg_features)\n","        return seg_features"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.073051Z","iopub.status.busy":"2022-10-29T06:02:59.072642Z","iopub.status.idle":"2022-10-29T06:03:13.720715Z","shell.execute_reply":"2022-10-29T06:03:13.719595Z","shell.execute_reply.started":"2022-10-29T06:02:59.073017Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 5, 128, 128, 128])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from timm.models.layers.conv2d_same import Conv2dSame\n","from conv3d_same import Conv3dSame\n","\n","\n","def convert_3d(module):\n","\n","    module_output = module\n","    if isinstance(module, torch.nn.BatchNorm2d):\n","        module_output = torch.nn.BatchNorm3d(\n","            module.num_features,\n","            module.eps,\n","            module.momentum,\n","            module.affine,\n","            module.track_running_stats,\n","        )\n","        if module.affine:\n","            with torch.no_grad():\n","                module_output.weight = module.weight\n","                module_output.bias = module.bias\n","        module_output.running_mean = module.running_mean\n","        module_output.running_var = module.running_var\n","        module_output.num_batches_tracked = module.num_batches_tracked\n","        if hasattr(module, \"qconfig\"):\n","            module_output.qconfig = module.qconfig\n","            \n","    elif isinstance(module, Conv2dSame):\n","        module_output = Conv3dSame(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.Conv2d):\n","        module_output = torch.nn.Conv3d(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","            padding_mode=module.padding_mode\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.MaxPool2d):\n","        module_output = torch.nn.MaxPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            dilation=module.dilation,\n","            ceil_mode=module.ceil_mode,\n","        )\n","    elif isinstance(module, torch.nn.AvgPool2d):\n","        module_output = torch.nn.AvgPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            ceil_mode=module.ceil_mode,\n","        )\n","\n","    for name, child in module.named_children():\n","        module_output.add_module(\n","            name, convert_3d(child)\n","        )\n","    del module\n","\n","    return module_output\n","\n","m = TimmSegModel(backbone)\n","m = convert_3d(m)\n","m(torch.rand(1, 1, 128,128,128)).shape"]},{"cell_type":"markdown","metadata":{},"source":["# Inference mask"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.738907Z","iopub.status.busy":"2022-10-29T06:03:13.738511Z","iopub.status.idle":"2022-10-29T06:03:13.755925Z","shell.execute_reply":"2022-10-29T06:03:13.755044Z","shell.execute_reply.started":"2022-10-29T06:03:13.738871Z"},"trusted":true},"outputs":[],"source":["def infer_func(model, loader_valid):\n","    model.eval()\n","    valid_loss = []\n","    outputs = []\n","    th = 0.2\n","    batch_metrics = [[]]\n","    bar = tqdm(loader_valid)\n","    \n","    with torch.no_grad():\n","        for images, save_paths in bar:\n","            images = images.cuda()\n","            logits = model(images)\n","            preds = (logits.sigmoid() > 0.2).float().detach().cpu().numpy()\n","            y_preds = (preds+0.1).astype(np.uint8)\n","        \n","            def save_mask(ind, preds = y_preds): \n","                compress(save_paths[ind], preds[ind])\n","            \n","            Parallel(n_jobs = len(y_preds))(delayed(save_mask)(i) for i in range(len(y_preds)))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.979257Z","iopub.status.busy":"2022-10-29T06:03:13.978679Z","iopub.status.idle":"2022-10-29T06:03:13.992437Z","shell.execute_reply":"2022-10-29T06:03:13.991378Z","shell.execute_reply.started":"2022-10-29T06:03:13.979202Z"},"trusted":true},"outputs":[],"source":["def run(fold):\n","    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n","    dataset_train = SEGDataset(df_train, 'valid')\n","    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_MASK_PRED, shuffle=False, num_workers=BATCH_MASK_PRED)\n","\n","    model = TimmSegModel(backbone, pretrained=True)\n","    model = convert_3d(model)\n","\n","    model.load_state_dict(torch.load(model_file))\n","    model = model.to(device)\n","\n","    print(len(dataset_train))\n","    \n","    infer_func(model, loader_train)\n","\n","    del model\n","    torch.cuda.empty_cache()\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:33.006143Z","iopub.status.busy":"2022-10-29T06:03:33.005515Z"},"trusted":true},"outputs":[],"source":["#run(0)"]},{"cell_type":"markdown","metadata":{},"source":["# Postprocss to get crop regions"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["#The order of the crop region data format\n","#Z start/end, Y start/end, X start/end for each mask channels + total region for the extravasation prediction\n","def calc_crop_region(mask):\n","    crop_range = np.zeros((6, 6))\n","    crop_range[:,::2]=10000\n","    mask_z = np.max(mask, axis = (2, 3)).astype(bool)\n","    mask_y = np.max(mask, axis = (1, 3)).astype(bool)\n","    mask_x = np.max(mask, axis = (1, 2)).astype(bool)\n","    \n","    template_range = np.arange(0, RESOL)\n","\n","    for mi in range(0, 5):\n","        zrange = template_range[mask_z[mi]]\n","        yrange = template_range[mask_y[mi]]\n","        xrange = template_range[mask_x[mi]]\n","        # For incomplete organ\n","        if(len(zrange)==0):\n","            zrange = template_range.copy()\n","            yrange = template_range.copy()\n","            xrange = template_range.copy()\n","\n","        crop_range[mi] = np.min(zrange), np.max(zrange)+1, np.min(yrange), np.max(yrange)+1, np.min(xrange), np.max(xrange)+1\n","\n","    crop_range[5] = np.min(crop_range[:5, 0]), np.max(crop_range[:5, 1]), np.min(crop_range[:5, 2]), \\\n","                    np.max(crop_range[:5, 3]), np.min(crop_range[:5,4]), np.max(crop_range[:5, 5])\n","    \n","    crop_range[:,:2]/=len(mask_z[0])\n","    crop_range[:,2:4]/=len(mask_y[0])\n","    crop_range[:,4:6]/=len(mask_x[0])\n","\n","    # Then make extravasation (# 5 mask) to reference one and convert other mask's crop respective to it\n","    # --> To minimize the loading size due to speed issue.\n","    zmin, rel_zrange = crop_range[5,0], crop_range[5,1]-crop_range[5,0]\n","    ymin, rel_yrange = crop_range[5,2], crop_range[5,3]-crop_range[5,2]\n","    xmin, rel_xrange = crop_range[5,4], crop_range[5,5]-crop_range[5,4]\n","\n","    crop_range[:5,:2] = (crop_range[:5,:2]-zmin)/rel_zrange\n","    crop_range[:5,2:4] = (crop_range[:5,2:4]-ymin)/rel_yrange\n","    crop_range[:5,4:6] = (crop_range[:5,4:6]-xmin)/rel_xrange\n","\n","    return crop_range\n","\n","def crop_resize_avg_and_std_3d(data, region):\n","    shapes = np.shape(data)\n","    region[:2]*=shapes[0]\n","    region[2:4]*=shapes[1]\n","    region[4:6]*=shapes[2]\n","    region = region.astype(int)\n","\n","    cropped = data[region[0]:region[1], region[2]:region[3], region[4]:region[5]].copy()   \n","    cropped_shape = np.array(np.shape(cropped)).astype(float)\n","    n_cropped_pixels = cropped_shape[0]*cropped_shape[1]*cropped_shape[2]    \n","    \n","    if(n_cropped_pixels > RESOL**3):\n","        shrink_ratio = (RESOL**3 / n_cropped_pixels)**(1/3)\n","        shrinked_shape = (cropped_shape*shrink_ratio).astype(int)\n","            \n","        slices = []\n","        for i in range(0, len(cropped)):\n","            slices.append(cv2.resize(cropped[i], (shrinked_shape[2], shrinked_shape[1]))[None])\n","        \n","        slices = np.vstack(slices)\n","        resized_cropped = np.zeros(shrinked_shape)\n","        for i in range(0, len(slices[0,0])):\n","            resized_cropped[:,:,i] = cv2.resize(slices[:,:,i], (shrinked_shape[1], shrinked_shape[0]))\n","    else:\n","        resized_cropped = cropped.copy()\n","        \n","    std = np.std(resized_cropped)\n","    avg = np.average(resized_cropped)\n","    resized_cropped = (resized_cropped-avg)/std\n","    resized_cropped = resized_cropped.astype(np.float32)\n","    \n","    del cropped, slices\n","    gc.collect()\n","    return resized_cropped\n","\n","def standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n","    \"\"\"\n","    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n","    \"\"\"\n","    # Correct DICOM pixel_array if PixelRepresentation == 1.\n","    pixel_array = dcm.pixel_array\n","    pixel_rep = dcm.PixelRepresentation\n","\n","    return pixel_array\n","\n","# Read each slice and stack them to make 3d data\n","def process_3d_crop(save_path, mask_path, data_path = TRAIN_PATH):\n","    tmp = save_path.split('/')[-1][:-4]\n","    tmp = tmp.split('_')\n","    patient, study = int(tmp[0]), int(tmp[1])\n","    \n","    mask = decompress(mask_path)\n","    crop_regions = calc_crop_region(mask)\n","    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n","\n","    crop_regions[5] = 0, 1, 0, 1, 0, 1\n","\n","    imgs = {}    \n","    \n","    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):      \n","        try:            \n","            dicom = pydicom.dcmread(f)        \n","            img = standardize_pixel_array(dicom)\n","            img_shape = np.shape(img)\n","            xy_crop_range = absolute_crop[2:].copy()   \n","            xy_crop_range[0:2]*=img_shape[0]\n","            xy_crop_range[2:4]*=img_shape[1]            \n","            xy_crop_range = xy_crop_range.astype(int)\n","            img = img.astype(float)\n","            break\n","        except:\n","            continue\n","            \n","    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):\n","        #For the case that some of the image can't be read -> error without this though don't know why  \n","        img = dicomsdl.open(f).pixelData(storedvalue=True)[xy_crop_range[0]:xy_crop_range[1], xy_crop_range[2]:xy_crop_range[3]]\n","        img = img.astype(float)\n","        \n","        pos_z = -int((f.split('/')[-1])[:-4])\n","        imgs[pos_z] = img\n","\n","    imgs_3d = []\n","    n_imgs = len(imgs)    \n","    z_crop_range= (absolute_crop[0:2]*n_imgs).astype(int)\n","\n","    #print(z_crop_range)\n","    for i, k in enumerate(sorted(imgs.keys())):\n","        #if i in sample_z:\n","        if(i >= z_crop_range[0] and i < z_crop_range[1]):\n","            img = imgs[k]\n","            imgs_3d.append(img[None])\n","        \n","    imgs_3d = np.vstack(imgs_3d)\n","    imgs_3d = ((imgs_3d - imgs_3d.min()) / (imgs_3d.max() - imgs_3d.min()))\n","\n","    if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n","        imgs_3d = 1.0 - imgs_3d\n","\n","    #Loaded original imgs_3d    \n","    #processed_img_3d = np.zeros((6, RESOL, RESOL, RESOL))\n","    \n","    origin_shape = np.shape(imgs_3d)\n","    for i in range(0, 6):    \n","        #To deal with almost not detected slices\n","        try:   \n","            # To deal with possible noises\n","            if(((crop_regions[i,1]-crop_regions[i,0]) < 5/origin_shape[0]) or \n","                ((crop_regions[i,3]-crop_regions[i,2]) < 5/origin_shape[1]) or \n","                ((crop_regions[i,5]-crop_regions[i,4]) < 5/origin_shape[2])):\n","                dummy_failure_function()\n","            \n","            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, crop_regions[i]).astype(np.float16))\n","            compress_fast(f'{save_path}_{i}', processed_img_3d)      \n","            del processed_img_3d\n","            gc.collect()\n","        except:\n","            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, np.array([0, 1, 0, 1, 0, 1])).astype(np.float16))\n","            compress_fast(f'{save_path}_{i}', processed_img_3d) \n","            del processed_img_3d\n","            gc.collect()  \n","\n","    del imgs, img\n","    gc.collect()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Preprocess dataset\n","rng_samples = np.linspace(0, len(df_train), N_PROCESS_CROP+1, dtype = int)\n","def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = df_train):\n","    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n","        if not os.path.isfile(f\"{df_train.iloc[i]['cropped_path']}_0.npy\"):\n","            process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'])     \n","        \n","        #For error correction\n","        else:            \n","            try:\n","                data = decompress_fast(df_train.iloc[i]['cropped_path'])\n","            except: \n","                process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'])  \n","        "]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 392/392 [12:36<00:00,  1.93s/it]\n","100%|██████████| 393/393 [12:36<00:00,  1.92s/it]\n","100%|██████████| 392/392 [12:37<00:00,  1.93s/it]\n","100%|██████████| 392/392 [12:40<00:00,  1.94s/it]\n","100%|██████████| 392/392 [12:41<00:00,  1.94s/it]\n","100%|██████████| 393/393 [12:50<00:00,  1.96s/it]\n","100%|██████████| 393/393 [12:55<00:00,  1.97s/it]\n","100%|██████████| 392/392 [12:59<00:00,  1.99s/it]\n","100%|██████████| 393/393 [12:59<00:00,  1.98s/it]\n","100%|██████████| 393/393 [13:11<00:00,  2.01s/it]\n","100%|██████████| 393/393 [13:15<00:00,  2.02s/it]\n","100%|█████████▉| 392/393 [13:34<00:01,  1.08s/it]"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 3.5 s, sys: 1.53 s, total: 5.03 s\n","Wall time: 13min 35s\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 393/393 [13:34<00:00,  2.07s/it]\n"]},{"data":{"text/plain":["[None, None, None, None, None, None, None, None, None, None, None, None]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","Parallel(n_jobs = N_PROCESS_CROP)(delayed(process_3d_wrapper)(i) for i in range(N_PROCESS_CROP))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
