{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junseonglee/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjunseonglee\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/junseonglee/.netrc\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "sys.path.append('./lib_models')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.metrics\n",
    "import warnings\n",
    "import pydicom\n",
    "import dicomsdl\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import gzip\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch import nn\n",
    "from resnet3d import generate_model\n",
    "from timm.utils import AverageMeter\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.login(key = '585f58f321685308f7933861d9dde7488de0970b')\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH  = '/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection'\n",
    "TRAIN_PATH = f'{BASE_PATH}/train_images'\n",
    "DATA_PATH = f'{BASE_PATH}/3d_preprocessed'\n",
    "\n",
    "if not os.path.isdir(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "\n",
    "RESOL = 256\n",
    "BATCH_SIZE = 8\n",
    "LR = 0.001\n",
    "N_EPOCHS = 30\n",
    "N_FOLDS  = 5\n",
    "N_PREPROCESS_CHUNKS = 24\n",
    "N_WORKERS = 12\n",
    "train_df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "train_df = train_df.sort_values(by=['patient_id'])\n",
    "\n",
    "\n",
    "wandb_config = {\n",
    "    'RESOL': RESOL,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'LR': LR,\n",
    "    'N_EPOCHS': N_EPOCHS,\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4]), array([630, 629, 630, 629, 629]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "train_meta = pd.read_csv(f'{BASE_PATH}/train_series_meta.csv')\n",
    "train_df = train_df.sort_values(by=['patient_id'])\n",
    "train_df\n",
    "\n",
    "TRAIN_PATH = BASE_PATH + \"/train_images/\"\n",
    "n_chunk = 8\n",
    "patients = os.listdir(TRAIN_PATH)\n",
    "n_patients = len(patients)\n",
    "rng_patients = np.linspace(0, n_patients+1, n_chunk+1, dtype = int)\n",
    "patients_cts = glob.glob(f'{TRAIN_PATH}/*/*')\n",
    "n_cts = len(patients_cts)\n",
    "patients_cts_arr = np.zeros((n_cts, 2), int)\n",
    "data_paths=[]\n",
    "for i in range(0, n_cts):\n",
    "    patient, ct = patients_cts[i].split('/')[-2:]\n",
    "    patients_cts_arr[i] = patient, ct\n",
    "    data_paths.append(f'{BASE_PATH}/3d_preprocessed/{patients_cts_arr[i,0]}_{patients_cts_arr[i,1]}.pkl')\n",
    "TRAIN_IMG_PATH = BASE_PATH + '/processed' \n",
    "\n",
    "#Generate tables for training\n",
    "train_meta_df = pd.DataFrame(patients_cts_arr, columns = ['patient_id', 'series'])\n",
    "\n",
    "#5-fold splitting\n",
    "train_df['fold'] = 0\n",
    "labels = train_df[['bowel_healthy','bowel_injury',\n",
    "                    'extravasation_healthy','extravasation_injury',\n",
    "                    'kidney_healthy','kidney_low','kidney_high',\n",
    "                    'liver_healthy','liver_low','liver_high',\n",
    "                    'spleen_healthy','spleen_low','spleen_high',\n",
    "                    'any_injury']].to_numpy()\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=0)\n",
    "counter = 0\n",
    "for train_index, test_index in mskf.split(np.ones(len(train_df)), labels):\n",
    "    for i in range(0, len(test_index)):\n",
    "        train_df['fold'][test_index[i]] = counter\n",
    "    counter+=1\n",
    "\n",
    "train_meta_df = train_meta_df.join(train_df.set_index('patient_id'), on='patient_id')\n",
    "train_meta_df['path']=data_paths\n",
    "train_meta_df.to_csv(f'{BASE_PATH}/train_meta.csv', index = False)\n",
    "np.unique(train_df['fold'].to_numpy(), return_counts = True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(name, data):\n",
    "    with gzip.open(name, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def decompress(name):\n",
    "    with gzip.open(name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n",
    "    \"\"\"\n",
    "    # Correct DICOM pixel_array if PixelRepresentation == 1.\n",
    "    pixel_array = dcm.pixel_array\n",
    "    if dcm.PixelRepresentation == 1:\n",
    "        bit_shift = dcm.BitsAllocated - dcm.BitsStored\n",
    "        dtype = pixel_array.dtype \n",
    "        new_array = (pixel_array << bit_shift).astype(dtype) >>  bit_shift\n",
    "        pixel_array = pydicom.pixel_data_handlers.util.apply_modality_lut(new_array, dcm)\n",
    "    return pixel_array\n",
    "'''\n",
    "def standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n",
    "    \"\"\"\n",
    "    # Correct DICOM pixel_array if PixelRepresentation == 1.\n",
    "    pixel_array = dcm.pixel_array\n",
    "    pixel_rep = dcm.PixelRepresentation\n",
    "    if dcm.PixelRepresentation == 1:\n",
    "        bit_shift = dcm.BitsAllocated - dcm.BitsStored\n",
    "        dtype = pixel_array.dtype \n",
    "        pixel_array = (pixel_array << bit_shift).astype(dtype) >>  bit_shift\n",
    "        \n",
    "        #pixel_array = pydicom.pixel_data_handlers.util.apply_modality_lut(new_array, dcm)\n",
    "\n",
    "        return pixel_array, pixel_rep, bit_shift, dtype\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each slice and stack them to make 3d data\n",
    "def process_3d(save_path, data_path = TRAIN_PATH):\n",
    "    tmp = save_path.split('/')[-1][:-4]\n",
    "    tmp = tmp.split('_')\n",
    "    patient, study = int(tmp[0]), int(tmp[1])\n",
    "    imgs = {}    \n",
    "    \n",
    "    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):      \n",
    "        pixel_rep = 0\n",
    "        bit_shift = 0\n",
    "        dtype = 0\n",
    "        try:\n",
    "            dicom = pydicom.dcmread(f)        \n",
    "            img, pixel_rep, bit_shift, dtype = standardize_pixel_array(dicom)\n",
    "            img = img.astype(float)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):\n",
    "        #For the case that some of the image can't be read -> error without this though don't know why  \n",
    "        img = dicomsdl.open(f).pixelData(storedvalue=True).astype(float)\n",
    "        #dicom = pydicom.dcmread(f)\n",
    "        #img = standardize_pixel_array(dicom).astype(float)\n",
    "        #ind = int((f.split('/')[-1])[:-4])\n",
    "        pos_z = -int((f.split('/')[-1])[:-4])\n",
    "        imgs[pos_z] = img\n",
    "\n",
    "\n",
    "    sample_z = np.linspace(0, len(imgs)-1, RESOL, dtype=int)\n",
    "\n",
    "    imgs_3d = []\n",
    "    for i, k in enumerate(sorted(imgs.keys())):\n",
    "        if i in sample_z:\n",
    "            img = imgs[k]\n",
    "            imgs_3d.append(cv2.resize(img, (RESOL, RESOL))[None])\n",
    "    \n",
    "    imgs_3d = np.vstack(imgs_3d)\n",
    "    \n",
    "    \n",
    "    nu = np.zeros((RESOL, RESOL, RESOL))\n",
    "\n",
    "    for i in range(0, len(imgs_3d[0,0])):\n",
    "        nu[:,:,i] = cv2.resize(imgs_3d[:,:,i], (RESOL, RESOL))\n",
    "    imgs_3d  = nu            \n",
    "\n",
    "    # To deal with random image edge    \n",
    "\n",
    "    imgs_3d = ((imgs_3d - imgs_3d.min()) / (imgs_3d.max() - imgs_3d.min()))\n",
    "\n",
    "    if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        imgs_3d = 1.0 - imgs_3d\n",
    "    \n",
    "    #Samplewise standardization to deal with the variety of the test datset.\n",
    "    std = np.std(imgs_3d)\n",
    "    avg = np.average(imgs_3d)\n",
    "    imgs_3d = (imgs_3d-avg)/std\n",
    "    imgs_3d = imgs_3d.astype(np.float32)\n",
    "\n",
    "    #here to\n",
    "    compress(save_path, imgs_3d)                      \n",
    "\n",
    "    del imgs, img, nu\n",
    "    gc.collect()\n",
    "\n",
    "    return imgs_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "rng_samples = np.linspace(0, len(train_meta_df), N_PREPROCESS_CHUNKS+1, dtype = int)\n",
    "def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = train_meta_df):\n",
    "    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n",
    "        if not os.path.isfile(train_meta_df.iloc[i]['path']):\n",
    "            process_3d(train_meta_df.iloc[i]['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:00<00:00, 22263.60it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 30610.80it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 22318.00it/s]\n",
      "100%|██████████| 197/197 [00:00<00:00, 22893.02it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 37052.49it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 22747.82it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 18958.62it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 27403.70it/s]\n",
      "100%|██████████| 197/197 [00:00<00:00, 19213.53it/s]\n",
      "100%|██████████| 197/197 [00:00<00:00, 37535.91it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 18969.12it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 20773.85it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 18790.48it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 37677.42it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 36328.76it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 37348.76it/s]\n",
      "100%|██████████| 197/197 [00:00<00:00, 19380.73it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 36785.56it/s]\n",
      "100%|██████████| 197/197 [00:00<00:00, 37371.23it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 36836.65it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 36019.96it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 37015.79it/s]\n",
      "100%|██████████| 197/197 [00:00<00:00, 36170.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95.2 ms, sys: 228 ms, total: 324 ms\n",
      "Wall time: 637 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:00<00:00, 37789.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#if __name__ == '__main__':\n",
    "Parallel(n_jobs = N_PREPROCESS_CHUNKS)(delayed(process_3d_wrapper)(i) for i in range(N_PREPROCESS_CHUNKS))\n",
    "    #with Pool(N_PREPROCESS_CHUNKS) as p:\n",
    "    #    p.map(process_3d_wrapper, range(0, N_PREPROCESS_CHUNKS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AbdominalCTDataset(Dataset):\n",
    "    def __init__(self, meta_df, is_train = True):\n",
    "        self.meta_df = meta_df\n",
    "        self.is_train = is_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.meta_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.meta_df.iloc[idx]\n",
    "        label = row[['bowel_healthy','bowel_injury',\n",
    "                    'extravasation_healthy','extravasation_injury',\n",
    "                    'kidney_healthy','kidney_low','kidney_high',\n",
    "                    'liver_healthy','liver_low','liver_high',\n",
    "                    'spleen_healthy','spleen_low','spleen_high', 'any_injury']]\n",
    "\n",
    "        #To avoid loading issue when applying multiprocessing to the unzip module\n",
    "        try:\n",
    "            data_3d = decompress(row['path'])\n",
    "        except:\n",
    "            data_3d = process_3d(row['path'])           \n",
    "\n",
    "        data_3d = data_3d.reshape(1, RESOL, RESOL, RESOL).astype(np.float32)  # channel, 3D \n",
    "        data_3d = torch.from_numpy(data_3d)\n",
    "        \n",
    "        #augmentation  \n",
    "        #if self.is_train:            \n",
    "        #    random_angle = np.random.rand(1)[0]*360.0-180.0\n",
    "        #    data_3d = transforms.functional.rotate(data_3d, random_angle, transforms.InterpolationMode.BILINEAR)\n",
    "            \n",
    "\n",
    "        label = label.to_numpy().astype(np.float32)\n",
    "                \n",
    "        label = torch.from_numpy(label)\n",
    "        return data_3d, label        \n",
    "\n",
    "train_dataset = AbdominalCTDataset(train_meta_df)\n",
    "data_3d, label = train_dataset[0]\n",
    "print(label)\n",
    "\n",
    "del train_dataset, data_3d, label\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#normalization parameter\\ntrain_dataset = AbdominalCTDataset(train_meta_df)\\ndata_3d, label = train_dataset[0]\\n\\navgs = np.zeros(len(train_dataset))\\nstds = np.zeros(len(train_dataset))\\nfor i in tqdm(range(0, len(train_dataset))):\\n    data_3d, label = train_dataset[i]\\n    data_3d = data_3d.numpy()\\n    avgs[i] = np.average(data_3d)\\n    stds[i] = np.std(data_3d)\\nprint(np.average(avgs))\\nprint(np.average(stds))    \\n\\ndel train_dataset, data_3d, label, avgs, stds\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#normalization parameter\n",
    "train_dataset = AbdominalCTDataset(train_meta_df)\n",
    "data_3d, label = train_dataset[0]\n",
    "\n",
    "avgs = np.zeros(len(train_dataset))\n",
    "stds = np.zeros(len(train_dataset))\n",
    "for i in tqdm(range(0, len(train_dataset))):\n",
    "    data_3d, label = train_dataset[i]\n",
    "    data_3d = data_3d.numpy()\n",
    "    avgs[i] = np.average(data_3d)\n",
    "    stds[i] = np.std(data_3d)\n",
    "print(np.average(avgs))\n",
    "print(np.average(stds))    \n",
    "\n",
    "del train_dataset, data_3d, label, avgs, stds\n",
    "gc.collect()\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbdominalClassifier(nn.Module):\n",
    "    def __init__(self, model_depth, device = DEVICE):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.resnet3d = generate_model(model_depth = model_depth, n_input_channels = 1)\n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.dropout  = nn.Dropout(p=0.5)\n",
    "        self.softmax  = nn.Softmax(dim=1)\n",
    "        size_res_out  = 56832\n",
    "        self.fc_bowel = nn.Linear(size_res_out, 2)\n",
    "        self.fc_extrav= nn.Linear(size_res_out, 2)\n",
    "        self.fc_kidney= nn.Linear(size_res_out, 3)\n",
    "        self.fc_liver = nn.Linear(size_res_out, 3)\n",
    "        self.fc_spleen= nn.Linear(size_res_out, 3)\n",
    "        \n",
    "        self.maxpool  = nn.MaxPool1d(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet3d(x)\n",
    "        for i in range(0, 4):\n",
    "            x[i] = self.flatten(x[i])\n",
    "        x = torch.cat(x, axis = 1)\n",
    "        x     = self.dropout(x)\n",
    "        bowel = self.fc_bowel(x)\n",
    "        extrav= self.fc_extrav(x)\n",
    "        kidney= self.fc_kidney(x)\n",
    "        liver = self.fc_liver(x)\n",
    "        spleen= self.fc_spleen(x)\n",
    "\n",
    "        labels = torch.cat([bowel, extrav, kidney, liver, spleen], dim = 1)\n",
    "\n",
    "        bowel_soft = self.softmax(bowel)\n",
    "        extrav_soft = self.softmax(extrav)\n",
    "        kidney_soft = self.softmax(kidney)\n",
    "        liver_soft = self.softmax(liver)\n",
    "        spleen_soft = self.softmax(spleen)\n",
    "\n",
    "        any_in = torch.cat([1-bowel_soft[:,0:1], 1-extrav_soft[:,0:1], \n",
    "                            1-kidney_soft[:,0:1], 1-liver_soft[:,0:1], 1-spleen_soft[:,0:1]], dim = 1) \n",
    "        any_in = self.maxpool(any_in)\n",
    "        any_not_in = 1-any_in\n",
    "        any_in = torch.cat([any_not_in, any_in], dim = 1)\n",
    "\n",
    "        return labels, any_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15094349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AbdominalClassifier(10)\n",
    "\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "print(get_n_params(model))\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbdominalClassifier(10)\n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=ttl_iters, eta_min=1e-6)\n",
    "\n",
    "\n",
    "weights = np.ones(2)\n",
    "weights[1] = 2\n",
    "crit_bowel  = nn.CrossEntropyLoss(weight = torch.from_numpy(weights).to(DEVICE))\n",
    "weights[1] = 6\n",
    "crit_extrav = nn.CrossEntropyLoss(weight = torch.from_numpy(weights).to(DEVICE))\n",
    "crit_any = nn.CrossEntropyLoss(weight = torch.from_numpy(weights).to(DEVICE))\n",
    "\n",
    "weights = np.ones((3))\n",
    "weights[1] = 2\n",
    "weights[2] = 4\n",
    "crit_kidney = nn.CrossEntropyLoss(weight = torch.from_numpy(weights).to(DEVICE))\n",
    "crit_liver  = nn.CrossEntropyLoss(weight = torch.from_numpy(weights).to(DEVICE))\n",
    "crit_spleen = nn.CrossEntropyLoss(weight = torch.from_numpy(weights).to(DEVICE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_one(tensor):\n",
    "    norm = torch.sum(tensor, 1)\n",
    "    for i in range(0, tensor.shape[1]):\n",
    "        tensor[:,i]/=norm\n",
    "    return tensor\n",
    "\n",
    "def normalize_arr_to_one(arr):\n",
    "    norm = np.sum(arr, axis = 1)\n",
    "    for i in range(0, len(arr[0])):\n",
    "        arr[:,i]/=norm\n",
    "    return arr\n",
    "\n",
    "def apply_softmax_to_labels(X_out):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    X_out[:,:2]    = normalize_to_one(softmax(X_out[:,:2]))\n",
    "    X_out[:,2:4]   = normalize_to_one(softmax(X_out[:,2:4]))\n",
    "    X_out[:,4:7]   = normalize_to_one(softmax(X_out[:,4:7]))\n",
    "    X_out[:,7:10]  = normalize_to_one(softmax(X_out[:,7:10]))\n",
    "    X_out[:,10:13] = normalize_to_one(softmax(X_out[:,10:13]))\n",
    "\n",
    "    return X_out\n",
    "\n",
    "def apply_normalization_to_labels(X_out):\n",
    "    X_out[:,:2]    = normalize_arr_to_one(X_out[:,:2])\n",
    "    X_out[:,2:4]   = normalize_arr_to_one(X_out[:,2:4])\n",
    "    X_out[:,4:7]   = normalize_arr_to_one(X_out[:,4:7])\n",
    "    X_out[:,7:10]  = normalize_arr_to_one(X_out[:,7:10])\n",
    "    X_out[:,10:13] = normalize_arr_to_one(X_out[:,10:13])\n",
    "\n",
    "    return X_out\n",
    "\n",
    "def calc_log_loss(ys, X_outs, bowel_weights, extrav_weights, kidney_weights, \\\n",
    "                  liver_weights, spleen_weights, any_in_weights):\n",
    "    loss = (\n",
    "              sklearn.metrics.log_loss(ys[:,:2], X_outs[:,:2], sample_weight = bowel_weights)\n",
    "            + sklearn.metrics.log_loss(ys[:,2:4], X_outs[:,2:4], sample_weight = extrav_weights)\n",
    "            + sklearn.metrics.log_loss(ys[:,4:7], X_outs[:,4:7], sample_weight = kidney_weights)\n",
    "            + sklearn.metrics.log_loss(ys[:,7:10], X_outs[:,7:10], sample_weight = liver_weights)\n",
    "            + sklearn.metrics.log_loss(ys[:,10:13], X_outs[:,10:13], sample_weight = spleen_weights)\n",
    "            + sklearn.metrics.log_loss(ys[:,13:15], X_outs[:,13:15], sample_weight =  any_in_weights)\n",
    "        ) /6\n",
    "    return loss\n",
    "    \n",
    "\n",
    "def calculate_score(X_outs, ys):\n",
    "    X_outs = X_outs.astype(np.float64)\n",
    "    ys     = ys.astype(np.float64)\n",
    "\n",
    "    bowel_weights  = ys[:,0] + 2*ys[:,1]\n",
    "    extrav_weights = ys[:,2] + 6*ys[:,3]\n",
    "    kidney_weights = ys[:,4] + 2*ys[:,5] + 4*ys[:,6]\n",
    "    liver_weights  = ys[:,7] + 2*ys[:,8] + 4*ys[:,9]\n",
    "    spleen_weights = ys[:,10] + 2*ys[:,11] + 4*ys[:,12]\n",
    "    any_in_weights = ys[:,13] + 6*ys[:,14]        \n",
    "\n",
    "    loss = calc_log_loss(ys, X_outs, bowel_weights, extrav_weights, kidney_weights, \n",
    "                        liver_weights, spleen_weights, any_in_weights)  \n",
    "\n",
    "    return loss\n",
    "\n",
    "def calculate_loss(X_out, X_any, y):\n",
    "    batch_size = X_out.shape[0]\n",
    "    cpu_y = y.clone().detach().cpu().numpy()\n",
    "    bowel_weights  =  cpu_y[:,0] + 2*cpu_y[:,1]\n",
    "    extrav_weights = cpu_y[:,2] + 6*cpu_y[:,3]\n",
    "    kidney_weights = cpu_y[:,4] + 2*cpu_y[:,5] + 4*cpu_y[:,6]\n",
    "    liver_weights  = cpu_y[:,7] + 2*cpu_y[:,8] + 4*cpu_y[:,9]\n",
    "    spleen_weights = cpu_y[:,10] + 2*cpu_y[:,11] + 4*cpu_y[:,12]\n",
    "    any_in_weights = (np.ones(np.shape(cpu_y[:,13])) - cpu_y[:,13]) + 6*cpu_y[:,13]\n",
    "    \n",
    "    sum_bowel_weights  = np.sum(bowel_weights)\n",
    "    sum_extrav_weights = np.sum(extrav_weights)\n",
    "    sum_kidney_weights = np.sum(kidney_weights)\n",
    "    sum_liver_weights  = np.sum(liver_weights)\n",
    "    sum_spleen_weights = np.sum(spleen_weights)\n",
    "    sum_any_in_weights = np.sum(any_in_weights)\n",
    "    sum_total_weights  = sum_bowel_weights  + sum_extrav_weights + \\\n",
    "                         sum_kidney_weights + sum_liver_weights  + \\\n",
    "                         sum_spleen_weights + sum_any_in_weights\n",
    "    \n",
    "    bowel_ratio  = sum_bowel_weights / sum_total_weights\n",
    "    extrav_ratio = sum_extrav_weights / sum_total_weights\n",
    "    kidney_ratio = sum_kidney_weights / sum_total_weights\n",
    "    liver_ratio  = sum_liver_weights / sum_total_weights\n",
    "    spleen_ratio = sum_spleen_weights / sum_total_weights\n",
    "    any_in_ratio = sum_any_in_weights / sum_total_weights\n",
    "\n",
    "    loss  = crit_bowel(X_out[:,:2], y[:,:2]) * bowel_ratio\n",
    "    loss += crit_extrav(X_out[:,2:4], y[:,2:4]) * extrav_ratio\n",
    "    loss += crit_kidney(X_out[:,4:7], y[:,4:7]) * kidney_ratio\n",
    "    loss += crit_liver(X_out[:,7:10], y[:,7:10])* liver_ratio\n",
    "    loss += crit_spleen(X_out[:,10:13], y[:,10:13]) * spleen_ratio\n",
    "    loss += crit_any(X_any,  torch.cat([torch.ones(batch_size, 1).to(DEVICE)- y[:,13:14],y[:,13:14]], dim = 1)) * any_in_ratio\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / trn/loss=1.1237\n",
      "Best scales: [0.92219788 0.44487828 0.77525975 0.67475441 0.67475441 0.69858797\n",
      " 0.69858797 0.51114335 0.        ]\n",
      "Epoch 1 / train/metric=0.6871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [ 8.21434358  4.24757155  0.1         0.13200884  0.1         0.22219469\n",
      "  2.52353917 19.56398344  0.        ]\n",
      "Epoch 1 / val/metric=0.5686\n",
      "Best val_metric 0.5685991180104767 at epoch 1!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / trn/loss=1.1402\n",
      "Best scales: [0.95477161 0.42970047 0.92219788 0.67475441 0.67475441 0.49370479\n",
      " 0.69858797 0.51114335 0.        ]\n",
      "Epoch 2 / train/metric=0.7086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [ 5.60716994  0.33700643 12.03377841  1.49926843  0.92219788 68.26071834\n",
      "  0.54789012  0.83099419  0.        ]\n",
      "Epoch 2 / val/metric=0.5644\n",
      "Best val_metric 0.5643671367117412 at epoch 2!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 / trn/loss=1.2029\n",
      "Best scales: [0.65173396 0.38720388 0.89073546 0.65173396 0.6294989  0.58727866\n",
      " 0.83099419 0.54789012 0.        ]\n",
      "Epoch 3 / train/metric=0.7705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [  0.89073546   0.273644     0.1          2.70495973 100.\n",
      "  20.25501939   0.23004301   0.56724261   0.        ]\n",
      "Epoch 3 / val/metric=0.5503\n",
      "Best val_metric 0.5503281731331932 at epoch 3!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / trn/loss=1.2187\n",
      "Best scales: [0.6294989  0.42970047 0.77525975 0.60802243 0.65173396 0.52919787\n",
      " 0.72326339 0.54789012 0.        ]\n",
      "Epoch 4 / train/metric=0.7996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [100.           0.26430815   0.20729218   2.19638537   0.25529081\n",
      "   0.29331663   3.57078596   1.0234114    0.        ]\n",
      "Epoch 4 / val/metric=0.6046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 / trn/loss=1.1708\n",
      "Best scales: [0.74881039 0.41504048 0.69858797 0.54789012 0.69858797 0.49370479\n",
      " 0.67475441 0.6294989  0.        ]\n",
      "Epoch 5 / train/metric=0.7439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [2.70495973 0.67475441 0.18679136 0.1        0.67475441 1.44811823\n",
      " 1.60705282 1.60705282 0.        ]\n",
      "Epoch 5 / val/metric=0.5964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 / trn/loss=1.0704\n",
      "Best scales: [0.77525975 0.42970047 0.92219788 0.74881039 0.74881039 0.6294989\n",
      " 0.65173396 0.51114335 0.        ]\n",
      "Epoch 6 / train/metric=0.6523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [7.66341087 1.35099352 0.4605922  0.56724261 0.69858797 1.3987131\n",
      " 1.0969858  1.05956018 0.        ]\n",
      "Epoch 6 / val/metric=0.5790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 / trn/loss=1.0188\n",
      "Best scales: [0.95477161 0.41504048 0.77525975 0.67475441 0.74881039 0.6294989\n",
      " 0.67475441 0.49370479 0.        ]\n",
      "Epoch 7 / train/metric=0.5945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [2.61267523 3.21764175 0.67475441 0.83099419 1.84642494 2.19638537\n",
      " 0.6294989  0.41504048 0.        ]\n",
      "Epoch 7 / val/metric=0.5540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 / trn/loss=1.0057\n",
      "Best scales: [0.92219788 0.44487828 0.92219788 0.74881039 0.89073546 0.67475441\n",
      " 0.60802243 0.41504048 0.        ]\n",
      "Epoch 8 / train/metric=0.5787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [0.32550886 0.20022004 0.51114335 0.4605922  0.56724261 0.92219788\n",
      " 0.34891012 0.31440355 0.        ]\n",
      "Epoch 8 / val/metric=0.5363\n",
      "Best val_metric 0.5363354210430628 at epoch 8!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 / trn/loss=0.9914\n",
      "Best scales: [0.92219788 0.38720388 0.92219788 0.80264335 0.83099419 0.72326339\n",
      " 0.6294989  0.49370479 0.        ]\n",
      "Epoch 9 / train/metric=0.5653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [0.69858797 0.28330961 0.74881039 0.49370479 0.37399373 0.36123427\n",
      " 0.37399373 0.273644   0.        ]\n",
      "Epoch 9 / val/metric=0.5434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / trn/loss=0.9756\n",
      "Best scales: [0.89073546 0.38720388 0.95477161 0.83099419 0.95477161 0.77525975\n",
      " 0.6294989  0.4605922  0.        ]\n",
      "Epoch 10 / train/metric=0.5544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [0.80264335 0.47686117 0.58727866 0.47686117 1.17584955 0.83099419\n",
      " 0.20729218 0.25529081 0.        ]\n",
      "Epoch 10 / val/metric=0.5418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 / trn/loss=0.9806\n",
      "Best scales: [0.95477161 0.42970047 0.9884959  0.74881039 0.92219788 0.77525975\n",
      " 0.56724261 0.42970047 0.        ]\n",
      "Epoch 11 / train/metric=0.5574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [0.83099419 0.14649714 0.67475441 0.77525975 0.83099419 0.67475441\n",
      " 0.36123427 0.60802243 0.        ]\n",
      "Epoch 11 / val/metric=0.5359\n",
      "Best val_metric 0.5358745968276217 at epoch 11!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 / trn/loss=0.9733\n",
      "Best scales: [0.89073546 0.38720388 0.92219788 0.80264335 0.89073546 0.72326339\n",
      " 0.60802243 0.42970047 0.        ]\n",
      "Epoch 12 / train/metric=0.5519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [1.05956018 0.44487828 0.47686117 0.58727866 0.74881039 1.26038293\n",
      " 0.6294989  0.58727866 0.        ]\n",
      "Epoch 12 / val/metric=0.5465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 / trn/loss=0.9580\n",
      "Best scales: [0.9884959  0.40088063 0.95477161 0.83099419 0.89073546 0.80264335\n",
      " 0.60802243 0.4605922  0.        ]\n",
      "Epoch 13 / train/metric=0.5443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [1.21738273 0.18041864 0.95477161 0.49370479 1.30490198 2.27396575\n",
      " 0.69858797 0.37399373 0.        ]\n",
      "Epoch 13 / val/metric=0.5452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 / trn/loss=0.9526\n",
      "Best scales: [0.9884959  0.4605922  0.95477161 0.77525975 0.80264335 0.69858797\n",
      " 0.60802243 0.44487828 0.        ]\n",
      "Epoch 14 / train/metric=0.5398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [0.54789012 0.37399373 1.49926843 0.6294989  1.17584955 1.13573336\n",
      " 0.44487828 0.37399373 0.        ]\n",
      "Epoch 14 / val/metric=0.5451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 / trn/loss=0.9331\n",
      "Best scales: [0.95477161 0.44487828 0.92219788 0.83099419 0.83099419 0.67475441\n",
      " 0.60802243 0.4605922  0.        ]\n",
      "Epoch 15 / train/metric=0.5275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [0.74881039 0.24658111 1.17584955 1.05956018 0.6294989  0.80264335\n",
      " 0.34891012 0.4605922  0.        ]\n",
      "Epoch 15 / val/metric=0.5386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 / trn/loss=0.9207\n",
      "Best scales: [0.9884959  0.40088063 0.95477161 0.86034644 0.92219788 0.83099419\n",
      " 0.60802243 0.4605922  0.        ]\n",
      "Epoch 16 / train/metric=0.5183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [0.80264335 0.21461412 0.40088063 0.42970047 0.9884959  0.60802243\n",
      " 0.95477161 0.52919787 0.        ]\n",
      "Epoch 16 / val/metric=0.5431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 / trn/loss=0.9050\n",
      "Best scales: [0.89073546 0.38720388 0.83099419 0.77525975 0.80264335 0.74881039\n",
      " 0.67475441 0.49370479 0.        ]\n",
      "Epoch 17 / train/metric=0.5062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [0.80264335 0.32550886 0.89073546 0.58727866 0.77525975 0.95477161\n",
      " 0.58727866 0.4605922  0.        ]\n",
      "Epoch 17 / val/metric=0.5558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 / trn/loss=0.8817\n",
      "Best scales: [0.89073546 0.4605922  0.86034644 0.77525975 0.83099419 0.74881039\n",
      " 0.6294989  0.4605922  0.        ]\n",
      "Epoch 18 / train/metric=0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [1.44811823 0.19338918 1.05956018 0.49370479 0.23816856 0.52919787\n",
      " 0.86034644 0.6294989  0.        ]\n",
      "Epoch 18 / val/metric=0.5541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 / trn/loss=0.8552\n",
      "Best scales: [0.86034644 0.42970047 0.89073546 0.83099419 0.86034644 0.67475441\n",
      " 0.6294989  0.4605922  0.        ]\n",
      "Epoch 19 / train/metric=0.4674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [3.00183581 1.0969858  2.27396575 0.52919787 1.26038293 5.80522552\n",
      " 0.28330961 0.38720388 0.        ]\n",
      "Epoch 19 / val/metric=0.5811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 / trn/loss=0.8268\n",
      "Best scales: [0.86034644 0.42970047 0.92219788 0.69858797 0.77525975 0.67475441\n",
      " 0.6294989  0.47686117 0.        ]\n",
      "Epoch 20 / train/metric=0.4464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [0.33700643 1.0969858  1.3987131  1.30490198 0.72326339 1.60705282\n",
      " 0.32550886 1.21738273 0.        ]\n",
      "Epoch 20 / val/metric=0.6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / trn/loss=0.7999\n",
      "Best scales: [0.67475441 0.42970047 0.92219788 0.83099419 0.72326339 0.65173396\n",
      " 0.67475441 0.52919787 0.        ]\n",
      "Epoch 21 / train/metric=0.4280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [1.49926843 0.32550886 1.3987131  0.273644   0.4605922  0.69858797\n",
      " 0.72326339 2.19638537 0.        ]\n",
      "Epoch 21 / val/metric=0.6244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 / trn/loss=0.7577\n",
      "Best scales: [0.80264335 0.41504048 0.92219788 0.77525975 0.69858797 0.65173396\n",
      " 0.67475441 0.56724261 0.        ]\n",
      "Epoch 22 / train/metric=0.3983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [1.60705282 0.52919787 0.40088063 1.0234114  0.67475441 1.05956018\n",
      " 0.6294989  1.0969858  0.        ]\n",
      "Epoch 22 / val/metric=0.6491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 / trn/loss=0.7086\n",
      "Best scales: [0.77525975 0.40088063 0.92219788 0.72326339 0.69858797 0.6294989\n",
      " 0.67475441 0.52919787 0.        ]\n",
      "Epoch 23 / train/metric=0.3622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [2.70495973 0.9884959  0.47686117 1.21738273 2.43744415 2.43744415\n",
      " 0.40088063 1.0969858  0.        ]\n",
      "Epoch 23 / val/metric=0.6947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 / trn/loss=0.6589\n",
      "Best scales: [0.89073546 0.40088063 0.92219788 0.80264335 0.65173396 0.6294989\n",
      " 0.74881039 0.60802243 0.        ]\n",
      "Epoch 24 / train/metric=0.3275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [1.72258597 1.72258597 0.42970047 0.83099419 0.6294989  0.92219788\n",
      " 0.42970047 1.0234114  0.        ]\n",
      "Epoch 24 / val/metric=0.7223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 / trn/loss=0.6208\n",
      "Best scales: [0.77525975 0.41504048 0.83099419 0.77525975 0.65173396 0.6294989\n",
      " 0.6294989  0.60802243 0.        ]\n",
      "Epoch 25 / train/metric=0.2975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [2.70495973 0.92219788 0.28330961 0.44487828 0.69858797 1.35099352\n",
      " 0.51114335 0.56724261 0.        ]\n",
      "Epoch 25 / val/metric=0.7190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 / trn/loss=0.5836\n",
      "Best scales: [0.69858797 0.38720388 0.86034644 0.77525975 0.72326339 0.56724261\n",
      " 0.69858797 0.58727866 0.        ]\n",
      "Epoch 26 / train/metric=0.2715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [1.55222536 0.83099419 0.80264335 0.86034644 0.67475441 1.84642494\n",
      " 0.36123427 0.56724261 0.        ]\n",
      "Epoch 26 / val/metric=0.7722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 / trn/loss=0.5538\n",
      "Best scales: [0.72326339 0.38720388 0.89073546 0.77525975 0.69858797 0.72326339\n",
      " 0.69858797 0.58727866 0.        ]\n",
      "Epoch 27 / train/metric=0.2513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [1.84642494 0.9884959  0.69858797 0.9884959  0.60802243 1.13573336\n",
      " 0.74881039 0.89073546 0.        ]\n",
      "Epoch 27 / val/metric=0.8353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 / trn/loss=0.5278\n",
      "Best scales: [0.69858797 0.37399373 0.89073546 0.72326339 0.6294989  0.60802243\n",
      " 0.74881039 0.58727866 0.        ]\n",
      "Epoch 28 / train/metric=0.2317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scales: [2.61267523 1.30490198 0.77525975 1.30490198 1.05956018 1.44811823\n",
      " 0.69858797 1.17584955 0.        ]\n",
      "Epoch 28 / val/metric=0.8153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 / trn/loss=0.5196\n",
      "Best scales: [0.65173396 0.41504048 0.80264335 0.69858797 0.67475441 0.67475441\n",
      " 0.74881039 0.56724261 0.        ]\n",
      "Epoch 29 / train/metric=0.2252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(enabled\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):                \n\u001b[1;32m     76\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():                 \n\u001b[0;32m---> 77\u001b[0m         X_out, X_any \u001b[39m=\u001b[39m model(X)                                           \n\u001b[1;32m     78\u001b[0m         y_any \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mones(batch_size, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(DEVICE)\u001b[39m-\u001b[39m y[:,\u001b[39m13\u001b[39m:\u001b[39m14\u001b[39m],y[:,\u001b[39m13\u001b[39m:\u001b[39m14\u001b[39m]], dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)              \n\u001b[1;32m     80\u001b[0m         X_out \u001b[39m=\u001b[39m apply_softmax_to_labels(X_out)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m, in \u001b[0;36mAbdominalClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnet3d(x)\n\u001b[1;32m     20\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m4\u001b[39m):\n\u001b[1;32m     21\u001b[0m         x[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x[i])\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/./lib_models/resnet3d.py:198\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    197\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m--> 198\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m    199\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m    200\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool_small(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m   2479\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled\n\u001b[1;32m   2480\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_dataset = AbdominalCTDataset(train_meta_df[train_meta_df['fold']!=0], is_train = True)\n",
    "    valid_dataset = AbdominalCTDataset(train_meta_df[train_meta_df['fold']==0], is_train = False)\n",
    "\n",
    "    train_loader = DataLoader(dataset = train_dataset, shuffle = True, batch_size = BATCH_SIZE, pin_memory = False, \n",
    "                            num_workers = N_WORKERS, drop_last = False)\n",
    "\n",
    "    valid_loader = DataLoader(dataset = valid_dataset, shuffle = False, batch_size = BATCH_SIZE, pin_memory = False, \n",
    "                            num_workers = N_WORKERS, drop_last = False)          \n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = LR)\n",
    "    ttl_iters = N_EPOCHS * len(train_loader)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR, \n",
    "                                                    steps_per_epoch=len(train_loader), epochs = N_EPOCHS)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "    val_metrics = np.ones(N_EPOCHS)*100\n",
    "\n",
    "    for epoch in range(0, N_EPOCHS):\n",
    "        train_meters = {'loss': AverageMeter()}\n",
    "        val_meters   = {'loss': AverageMeter()}\n",
    "        \n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, leave=False)  \n",
    "\n",
    "        X_outs=[]\n",
    "        ys=[]\n",
    "\n",
    "        for X, y in pbar:\n",
    "            batch_size = X.shape[0]\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=True):  \n",
    "                X_out, X_any  = model(X)\n",
    "                loss = calculate_loss(X_out, X_any, y)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scheduler.step()\n",
    "                scaler.update()          \n",
    "\n",
    "            #Metric calculation\n",
    "            y_any = torch.cat([torch.ones(batch_size, 1).to(DEVICE)- y[:,13:14],y[:,13:14]], dim = 1)    \n",
    "            X_out = apply_softmax_to_labels(X_out).detach().to('cpu').numpy()\n",
    "            X_any = X_any.detach().to('cpu').numpy()\n",
    "            X_out = np.hstack([X_out, X_any])\n",
    "            X_outs.append(X_out)\n",
    "\n",
    "            y     = y.to('cpu').numpy()[:,:-1]\n",
    "            y_any = y_any.to('cpu').numpy()\n",
    "            y     = np.hstack([y, y_any])\n",
    "            ys.append(y)\n",
    "\n",
    "            trn_loss = loss.item()      \n",
    "            train_meters['loss'].update(trn_loss, n=X.size(0))     \n",
    "            pbar.set_description(f'Train loss: {trn_loss}')   \n",
    "        print('Epoch {:d} / trn/loss={:.4f}'.format(epoch+1, train_meters['loss'].avg))    \n",
    "\n",
    "        X_outs = np.vstack(X_outs) \n",
    "        ys     = np.vstack(ys)\n",
    "        metric = calculate_score(X_outs, ys)                 \n",
    "        print('Epoch {:d} / train/metric={:.4f}'.format(epoch+1, metric))   \n",
    "\n",
    "        del X, X_outs, y, ys, X_any\n",
    "        gc.collect()\n",
    "\n",
    "        X_outs=[]\n",
    "        ys=[]\n",
    "        model.eval()\n",
    "        for X, y in tqdm(valid_loader, leave=False):        \n",
    "            batch_size = X.shape[0]        \n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "                 \n",
    "            with torch.cuda.amp.autocast(enabled=True):                \n",
    "                with torch.no_grad():                 \n",
    "                    X_out, X_any = model(X)                                           \n",
    "                    y_any = torch.cat([torch.ones(batch_size, 1).to(DEVICE)- y[:,13:14],y[:,13:14]], dim = 1)              \n",
    "                              \n",
    "                    X_out = apply_softmax_to_labels(X_out).to('cpu').numpy()\n",
    "\n",
    "                    X_any = X_any.to('cpu').numpy()\n",
    "                    X_out = np.hstack([X_out, X_any])\n",
    "                    X_outs.append(X_out)\n",
    "\n",
    "                    y     = y.to('cpu').numpy()[:,:-1]\n",
    "                    y_any = y_any.to('cpu').numpy()\n",
    "                    y     = np.hstack([y, y_any])\n",
    "                    ys.append(y)\n",
    "\n",
    "        X_outs = np.vstack(X_outs) \n",
    "        ys     = np.vstack(ys)\n",
    "        metric = calculate_score(X_outs, ys)                \n",
    "        print('Epoch {:d} / val/metric={:.4f}'.format(epoch+1, metric))   \n",
    "        \n",
    "        #Save the best model    \n",
    "        if(metric < np.min(val_metrics)):\n",
    "            try:\n",
    "                os.makedirs(f'{BASE_PATH}/weights')\n",
    "            except:\n",
    "                a = 1\n",
    "            best_metric = metric\n",
    "            print(f'Best val_metric {best_metric} at epoch {epoch+1}!')\n",
    "            torch.save(model, f'{BASE_PATH}/weights/best.pt')    \n",
    "        val_metrics[epoch] = metric\n",
    "        \n",
    "        del X, X_outs, y, ys, X_any\n",
    "        gc.collect()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.06",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
