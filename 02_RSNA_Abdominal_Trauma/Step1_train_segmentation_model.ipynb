{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Modification of the original code\n","I just took the code of the winner of the RSNA 2022 cervical spine fracture detection competition.  \n","Link: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage1"]},{"cell_type":"markdown","metadata":{},"source":["# 1st Place Solution Training 3D Semantic Segmentation (Stage1)\n","\n","Hi all,\n","\n","I'm very exciting to writing this notebook and the summary of our solution here.\n","\n","This is FULL version of training my final models (stage1), using resnet18d as backbone, unet as decoder and using 128x128x128 as input.\n","\n","NOTE: **You need to run this code locally because the RAM is not enough here.**\n","\n","NOTE2: **It is highly recommended to pre-process the 3D semantic segmentation training data first and save it locally, which can greatly speed up the loading of the data.**\n","\n","My brief summary of winning solution: https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/362607\n","\n","* Train Stage1 Notebook: This notebook\n","* Train Stage2 (Type1) Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage2-type1\n","* Train Stage2 (Type2) Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage2-type2\n","* Inference Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-inference\n","\n","**If you find these notebooks helpful please upvote. Thanks! **"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:25.743219Z","iopub.status.busy":"2022-10-29T06:00:25.742950Z","iopub.status.idle":"2022-10-29T06:00:34.162024Z","shell.execute_reply":"2022-10-29T06:00:34.160905Z","shell.execute_reply.started":"2022-10-29T06:00:25.743184Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/junseonglee/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import sys\n","import gc\n","import ast\n","import cv2\n","import time\n","import timm\n","import pickle\n","import random\n","import pydicom\n","import argparse\n","import warnings\n","import numpy as np\n","import pandas as pd\n","from glob import glob\n","import nibabel as nib\n","from PIL import Image\n","from tqdm import tqdm\n","import albumentations\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gzip\n","import pickle\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.cuda.amp as amp\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","\n","from monai.transforms import Resize\n","import monai.transforms as transforms\n","\n","\n","%matplotlib inline\n","rcParams['figure.figsize'] = 20, 8\n","device = torch.device('cuda')\n","torch.backends.cudnn.benchmark = True\n","\n","sys.path.append('./lib_models')"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:29.485211Z","iopub.status.busy":"2022-10-29T06:03:29.484820Z","iopub.status.idle":"2022-10-29T06:03:29.494317Z","shell.execute_reply":"2022-10-29T06:03:29.493294Z","shell.execute_reply.started":"2022-10-29T06:03:29.485168Z"},"trusted":true},"outputs":[],"source":["DEBUG = False\n","OPT_SHAPE = [96, 176, 176]\n","\n","BASE_PATH = '/home/junseonglee/Desktop/01_codes/inputs/rsna-2023-abdominal-trauma-detection'\n","MASK_SAVE_PATH = f'{BASE_PATH}/mask_preprocessed'\n","LOAD_WEIGHT_PATH = None\n","\n","kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n","load_kernel = None\n","load_last = True\n","n_blocks = 4\n","n_folds = 5\n","backbone = 'resnet50d'\n","\n","R = Resize(OPT_SHAPE)\n","\n","init_lr = 3e-3\n","batch_size = 4\n","drop_rate = 0.\n","drop_path_rate = 0.\n","loss_weights = [1, 1]\n","p_mixup = 0.1\n","\n","data_dir = '../input/rsna-2022-cervical-spine-fracture-detection'\n","use_amp = True\n","num_workers = 8\n","out_dim = 5\n","\n","n_epochs = 1000\n","\n","log_dir = f'{BASE_PATH}/seg_models/logs'\n","model_dir = f'{BASE_PATH}/seg_models/models'\n","os.makedirs(log_dir, exist_ok=True)\n","os.makedirs(model_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.390438Z","iopub.status.busy":"2022-10-29T06:00:34.389599Z","iopub.status.idle":"2022-10-29T06:00:34.402322Z","shell.execute_reply":"2022-10-29T06:00:34.401400Z","shell.execute_reply.started":"2022-10-29T06:00:34.390400Z"},"trusted":true},"outputs":[],"source":["transforms_train = transforms.Compose([\n","    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=0),\n","    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=1),\n","    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=2),\n","    transforms.RandAffined(keys=[\"image\", \"mask\"], translate_range=[int(x*y) for x, y in zip(OPT_SHAPE, [0.3, 0.3, 0.3])], padding_mode='zeros', prob=0.7),\n","    transforms.RandGridDistortiond(keys=(\"image\", \"mask\"), prob=0.5, distort_limit=(-0.01, 0.01), mode=\"nearest\"),    \n","])\n","\n","transforms_train_images = transforms.Compose([\n","    #transforms.RandGaussianNoise(prob=0.1),\n","    #transforms.RandAdjustContrast(prob=0.1),\n","    #transforms.RandGaussianSmooth(prob=0.1),        \n","    #transforms.RandGaussianSharpen(prob=0.1),\n","    #transforms.RandHistogramShift(prob=0.1),\n","])\n","\n","transforms_valid = transforms.Compose([\n","])"]},{"cell_type":"markdown","metadata":{},"source":["# DataFrame"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.404497Z","iopub.status.busy":"2022-10-29T06:00:34.403952Z","iopub.status.idle":"2022-10-29T06:00:34.486417Z","shell.execute_reply":"2022-10-29T06:00:34.485281Z","shell.execute_reply.started":"2022-10-29T06:00:34.404459Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>patient_id</th>\n","      <th>series</th>\n","      <th>mask_path</th>\n","      <th>img_path</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>201</th>\n","      <td>13741</td>\n","      <td>6172</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>202</th>\n","      <td>18877</td>\n","      <td>61403</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>203</th>\n","      <td>15876</td>\n","      <td>38633</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>204</th>\n","      <td>60744</td>\n","      <td>397</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>205</th>\n","      <td>24524</td>\n","      <td>40496</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>/home/junseonglee/Desktop/01_codes/inputs/rsna...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     patient_id  series                                          mask_path  \\\n","201       13741    6172  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","202       18877   61403  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","203       15876   38633  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","204       60744     397  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","205       24524   40496  /home/junseonglee/Desktop/01_codes/inputs/rsna...   \n","\n","                                              img_path  fold  \n","201  /home/junseonglee/Desktop/01_codes/inputs/rsna...     2  \n","202  /home/junseonglee/Desktop/01_codes/inputs/rsna...     2  \n","203  /home/junseonglee/Desktop/01_codes/inputs/rsna...     2  \n","204  /home/junseonglee/Desktop/01_codes/inputs/rsna...     3  \n","205  /home/junseonglee/Desktop/01_codes/inputs/rsna...     0  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df_seg = pd.read_csv(f'{BASE_PATH}/seg_info.csv')\n","\n","kf = KFold(5, shuffle = True, random_state = 0)\n","df_seg['fold'] = -1\n","for fold, (train_idx, valid_idx) in enumerate(kf.split(df_seg, df_seg)):\n","    df_seg.loc[valid_idx, 'fold'] = fold\n","\n","df_seg.tail()"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.497353Z","iopub.status.busy":"2022-10-29T06:00:34.496904Z","iopub.status.idle":"2022-10-29T06:00:34.514198Z","shell.execute_reply":"2022-10-29T06:00:34.513162Z","shell.execute_reply.started":"2022-10-29T06:00:34.497317Z"},"trusted":true},"outputs":[],"source":["def decompress(name):\n","    with gzip.open(name, 'rb') as f:\n","        data = pickle.load(f)\n","    return data\n","\n","\n","def load_sample(row, has_mask=True):\n","    image = decompress(row['img_path'])[None]\n","\n","    if has_mask:\n","        mask = decompress(row['mask_path'])\n","        \n","        return image, mask\n","    else:\n","        return image\n","\n","\n","\n","class SEGDataset(Dataset):\n","    def __init__(self, df, mode, transform, transform_image=None):\n","\n","        self.df = df.reset_index()\n","        self.mode = mode\n","        self.transform = transform\n","        self.transform_image = transform_image\n","        self.images = []\n","        self.masks  = []\n","        for i in tqdm(range(0, len(df))):\n","            image, mask = load_sample(df.iloc[i], has_mask=True)\n","            self.images.append(image)\n","            self.masks.append(mask)\n","            \n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, index):\n","        #row = self.df.iloc[index]\n","        #image, mask = load_sample(row, has_mask=True)\n","        image = torch.clone(self.images[index])\n","        mask  = self.masks[index].copy()\n","        #image = torch.from_numpy(image).to(torch.float32)\n","        mask  = torch.from_numpy(mask).to(torch.float32)\n","\n","        res = self.transform({'image':image, 'mask':mask})\n","        image = res['image']\n","        mask  = res['mask']\n","        \n","        if self.transform_image is not None:\n","            image = self.transform_image(image)\n","                    \n","        #image = R(image)\n","        #mask  = R(mask)\n","\n","        image_avg = torch.mean(image, (1, 2, 3))\n","        image_std = torch.std(image,  (1, 2, 3))\n","        image = (image-image_avg)/image_std\n","\n","        return image, mask\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.516237Z","iopub.status.busy":"2022-10-29T06:00:34.515883Z","iopub.status.idle":"2022-10-29T06:00:34.528793Z","shell.execute_reply":"2022-10-29T06:00:34.527571Z","shell.execute_reply.started":"2022-10-29T06:00:34.516189Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 206/206 [00:05<00:00, 37.96it/s]\n"]}],"source":["rcParams['figure.figsize'] = 20,8\n","\n","df_show = df_seg\n","dataset_show = SEGDataset(df_show, 'train', transform=transforms_train)"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.059903Z","iopub.status.busy":"2022-10-29T06:02:59.058663Z","iopub.status.idle":"2022-10-29T06:02:59.070787Z","shell.execute_reply":"2022-10-29T06:02:59.069705Z","shell.execute_reply.started":"2022-10-29T06:02:59.059860Z"},"trusted":true},"outputs":[],"source":["class TimmSegModel(nn.Module):\n","    def __init__(self, backbone, segtype='unet', pretrained=False):\n","        super(TimmSegModel, self).__init__()\n","\n","        self.encoder = timm.create_model(\n","            backbone,\n","            in_chans=1,\n","            features_only=True,\n","            drop_rate=drop_rate,\n","            drop_path_rate=drop_path_rate,\n","            pretrained=pretrained\n","        )\n","        g = self.encoder(torch.rand(1, 1, 64, 64))\n","        encoder_channels = [1] + [_.shape[1] for _ in g]\n","        decoder_channels = [256, 128, 64, 32, 16]\n","        if segtype == 'unet':\n","            self.decoder = smp.unet.decoder.UnetDecoder(\n","                encoder_channels=encoder_channels[:n_blocks+1],\n","                decoder_channels=decoder_channels[:n_blocks],\n","                n_blocks=n_blocks,\n","            )\n","\n","        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","    def forward(self,x):\n","        global_features = [0] + self.encoder(x)[:n_blocks]\n","        for i in range(1, len(global_features)):\n","            print(global_features[i].shape)\n","        \n","        seg_features = self.decoder(*global_features)\n","        seg_features = self.segmentation_head(seg_features)\n","        return seg_features"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.073051Z","iopub.status.busy":"2022-10-29T06:02:59.072642Z","iopub.status.idle":"2022-10-29T06:03:13.720715Z","shell.execute_reply":"2022-10-29T06:03:13.719595Z","shell.execute_reply.started":"2022-10-29T06:02:59.073017Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 64, 49, 87, 88])\n","torch.Size([1, 256, 25, 44, 44])\n","torch.Size([1, 512, 13, 22, 22])\n","torch.Size([1, 1024, 7, 11, 11])\n"]},{"ename":"RuntimeError","evalue":"Sizes of tensors must match except in dimension 1. Expected size 14 but got size 13 for tensor number 1 in the list.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb Cell 14\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m m \u001b[39m=\u001b[39m TimmSegModel(backbone)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m m \u001b[39m=\u001b[39m convert_3d(m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m m(torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, OPT_SHAPE[\u001b[39m0\u001b[39m], OPT_SHAPE[\u001b[39m1\u001b[39m], OPT_SHAPE[\u001b[39m2\u001b[39m]))\u001b[39m.\u001b[39mshape\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mdel\u001b[39;00m m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(global_features)):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mprint\u001b[39m(global_features[i]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m seg_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\u001b[39m*\u001b[39mglobal_features)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m seg_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegmentation_head(seg_features)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mreturn\u001b[39;00m seg_features\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/segmentation_models_pytorch/unet/decoder.py:119\u001b[0m, in \u001b[0;36mUnetDecoder.forward\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mfor\u001b[39;00m i, decoder_block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n\u001b[1;32m    118\u001b[0m     skip \u001b[39m=\u001b[39m skips[i] \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(skips) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     x \u001b[39m=\u001b[39m decoder_block(x, skip)\n\u001b[1;32m    121\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/segmentation_models_pytorch/unet/decoder.py:38\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     36\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(x, scale_factor\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m skip \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x, skip], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention1(x)\n\u001b[1;32m     40\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n","\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 14 but got size 13 for tensor number 1 in the list."]}],"source":["from timm.models.layers.conv2d_same import Conv2dSame\n","from conv3d_same import Conv3dSame\n","\n","\n","def convert_3d(module):\n","\n","    module_output = module\n","    if isinstance(module, torch.nn.BatchNorm2d):\n","        module_output = torch.nn.BatchNorm3d(\n","            module.num_features,\n","            module.eps,\n","            module.momentum,\n","            module.affine,\n","            module.track_running_stats,\n","        )\n","        if module.affine:\n","            with torch.no_grad():\n","                module_output.weight = module.weight\n","                module_output.bias = module.bias\n","        module_output.running_mean = module.running_mean\n","        module_output.running_var = module.running_var\n","        module_output.num_batches_tracked = module.num_batches_tracked\n","        if hasattr(module, \"qconfig\"):\n","            module_output.qconfig = module.qconfig\n","            \n","    elif isinstance(module, Conv2dSame):\n","        module_output = Conv3dSame(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.Conv2d):\n","        module_output = torch.nn.Conv3d(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","            padding_mode=module.padding_mode\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.MaxPool2d):\n","        module_output = torch.nn.MaxPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            dilation=module.dilation,\n","            ceil_mode=module.ceil_mode,\n","        )\n","    elif isinstance(module, torch.nn.AvgPool2d):\n","        module_output = torch.nn.AvgPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            ceil_mode=module.ceil_mode,\n","        )\n","\n","    for name, child in module.named_children():\n","        module_output.add_module(\n","            name, convert_3d(child)\n","        )\n","    del module\n","\n","    return module_output\n","\n","\n","m = TimmSegModel(backbone)\n","m = convert_3d(m)\n","m(torch.rand(1, 1, OPT_SHAPE[0], OPT_SHAPE[1], OPT_SHAPE[2])).shape\n","del m\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["TimmSegModel(\n","  (encoder): FeatureListNet(\n","    (conv1): Sequential(\n","      (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n","      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","      (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): ReLU(inplace=True)\n","      (6): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","    )\n","    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (act1): ReLU(inplace=True)\n","    (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Identity()\n","          (1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","          (2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): AvgPool3d(kernel_size=2, stride=2, padding=0)\n","          (1): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","          (2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): AvgPool3d(kernel_size=2, stride=2, padding=0)\n","          (1): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","          (2): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): AvgPool3d(kernel_size=2, stride=2, padding=0)\n","          (1): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","          (2): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act1): ReLU(inplace=True)\n","        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act2): ReLU(inplace=True)\n","        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n","        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act3): ReLU(inplace=True)\n","      )\n","    )\n","  )\n","  (decoder): UnetDecoder(\n","    (center): Identity()\n","    (blocks): ModuleList(\n","      (0): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv3d(1536, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (1): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv3d(512, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (2): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv3d(192, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (3): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","          (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n","          (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","    )\n","  )\n","  (segmentation_head): Conv3d(32, 5, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["m"]},{"cell_type":"markdown","metadata":{},"source":["# Loss & Metric"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.724951Z","iopub.status.busy":"2022-10-29T06:03:13.724580Z","iopub.status.idle":"2022-10-29T06:03:13.736570Z","shell.execute_reply":"2022-10-29T06:03:13.735400Z","shell.execute_reply.started":"2022-10-29T06:03:13.724920Z"},"trusted":true},"outputs":[],"source":["from typing import Any, Dict, Optional\n","\n","\n","def binary_dice_score(\n","    y_pred: torch.Tensor,\n","    y_true: torch.Tensor,\n","    threshold: Optional[float] = None,\n","    nan_score_on_empty=False,\n","    eps: float = 1e-7,\n",") -> float:\n","\n","    if threshold is not None:\n","        y_pred = (y_pred > threshold).to(y_true.dtype)\n","\n","    intersection = torch.sum(y_pred * y_true).item()\n","    cardinality = (torch.sum(y_pred) + torch.sum(y_true)).item()\n","\n","    score = (2.0 * intersection) / (cardinality + eps)\n","\n","    has_targets = torch.sum(y_true) > 0\n","    has_predicted = torch.sum(y_pred) > 0\n","\n","    if not has_targets:\n","        if nan_score_on_empty:\n","            score = np.nan\n","        else:\n","            score = float(not has_predicted)\n","    return score\n","\n","\n","def multilabel_dice_score(\n","    y_true: torch.Tensor,\n","    y_pred: torch.Tensor,\n","    threshold=None,\n","    eps=1e-7,\n","    nan_score_on_empty=False,\n","):\n","    ious = []\n","    num_classes = y_pred.size(0)\n","    for class_index in range(num_classes):\n","        iou = binary_dice_score(\n","            y_pred=y_pred[class_index],\n","            y_true=y_true[class_index],\n","            threshold=threshold,\n","            nan_score_on_empty=nan_score_on_empty,\n","            eps=eps,\n","        )\n","        ious.append(iou)\n","\n","    return ious\n","\n","\n","def dice_loss(input, target):\n","    input = torch.sigmoid(input)\n","    smooth = 1.0\n","    iflat = input.view(-1)\n","    tflat = target.view(-1)\n","    intersection = (iflat * tflat).sum()\n","    return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n","\n","\n","def bce_dice(input, target, loss_weights=loss_weights):\n","    loss1 = loss_weights[0] * nn.BCEWithLogitsLoss()(input, target)\n","    loss2 = loss_weights[1] * dice_loss(input, target)\n","    return (loss1 + loss2) / sum(loss_weights)\n","\n","criterion = bce_dice"]},{"cell_type":"markdown","metadata":{},"source":["# Train & Valid func"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.738907Z","iopub.status.busy":"2022-10-29T06:03:13.738511Z","iopub.status.idle":"2022-10-29T06:03:13.755925Z","shell.execute_reply":"2022-10-29T06:03:13.755044Z","shell.execute_reply.started":"2022-10-29T06:03:13.738871Z"},"trusted":true},"outputs":[],"source":["def mixup(input, truth, clip=[0, 1]):\n","    indices = torch.randperm(input.size(0))\n","    shuffled_input = input[indices]\n","    shuffled_labels = truth[indices]\n","\n","    lam = np.random.uniform(clip[0], clip[1])\n","    input = input * lam + shuffled_input * (1 - lam)\n","    return input, truth, shuffled_labels, lam\n","\n","\n","def train_func(model, loader_train, optimizer, scaler=None):\n","    model.train()\n","    train_loss = []\n","    bar = tqdm(loader_train)\n","    for images, gt_masks in bar:\n","        optimizer.zero_grad()\n","        images = images.cuda()\n","        gt_masks = gt_masks.cuda()\n","\n","        do_mixup = False\n","        if random.random() < p_mixup:\n","            do_mixup = True\n","            images, gt_masks, gt_masks_sfl, lam = mixup(images, gt_masks)\n","\n","        with amp.autocast():\n","            logits = model(images)\n","            loss = criterion(logits, gt_masks)\n","            if do_mixup:\n","                loss2 = criterion(logits, gt_masks_sfl)\n","                loss = loss * lam  + loss2 * (1 - lam)\n","\n","        train_loss.append(loss.item())\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        bar.set_description(f'smth:{np.mean(train_loss[-30:]):.4f}')\n","\n","    return np.mean(train_loss)\n","\n","\n","def valid_func(model, loader_valid):\n","    model.eval()\n","    valid_loss = []\n","    outputs = []\n","    ths = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","    batch_metrics = [[]] * 9\n","    bar = tqdm(loader_valid)\n","    with torch.no_grad():\n","        with amp.autocast():\n","            for images, gt_masks in bar:\n","                images = images.cuda()\n","                gt_masks = gt_masks.cuda()\n","\n","                logits = model(images)\n","                loss = criterion(logits, gt_masks)\n","                valid_loss.append(loss.item())\n","                for thi, th in enumerate(ths):\n","                    pred = (logits.sigmoid() > th).float().detach().cpu()\n","                    for i in range(logits.shape[0]):\n","                        tmp = multilabel_dice_score(\n","                            #y_pred=logits[i].sigmoid().cpu(),\n","                            y_pred = pred[i],\n","                            y_true=gt_masks[i].cpu(),\n","                            threshold=0.5,\n","                        )\n","                        batch_metrics[thi].extend(tmp)\n","                bar.set_description(f'smth:{np.mean(valid_loss[-30:]):.4f}')\n","            \n","    metrics = [np.mean(this_metric) for this_metric in batch_metrics]\n","    print('best th:', ths[np.argmax(metrics)], 'best dc:', np.max(metrics))\n","\n","    return np.mean(valid_loss), np.max(metrics), ths[np.argmax(metrics)]\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.757646Z","iopub.status.busy":"2022-10-29T06:03:13.757188Z","iopub.status.idle":"2022-10-29T06:03:13.977069Z","shell.execute_reply":"2022-10-29T06:03:13.976109Z","shell.execute_reply.started":"2022-10-29T06:03:13.757610Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/junseonglee/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","/home/junseonglee/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7fa1a72ba310>]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 2000x200 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["rcParams['figure.figsize'] = 20, 2\n","optimizer = optim.AdamW(m.parameters(), lr=init_lr)\n","scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000)\n","lrs = []\n","for epoch in range(1, 1000+1):\n","    scheduler_cosine.step(epoch-1)\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","plt.plot(range(len(lrs)), lrs)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.979257Z","iopub.status.busy":"2022-10-29T06:03:13.978679Z","iopub.status.idle":"2022-10-29T06:03:13.992437Z","shell.execute_reply":"2022-10-29T06:03:13.991378Z","shell.execute_reply.started":"2022-10-29T06:03:13.979202Z"},"trusted":true},"outputs":[],"source":["def run(fold):\n","\n","    log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n","    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n","\n","    train_ = df_seg[df_seg['fold'] != fold].reset_index(drop=True)\n","    valid_ = df_seg[df_seg['fold'] == fold].reset_index(drop=True)\n","    dataset_train = SEGDataset(train_, 'train', transform=transforms_train, transform_image=transforms_train_images)\n","    dataset_valid = SEGDataset(valid_, 'valid', transform=transforms_valid)\n","    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","    loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","    model = TimmSegModel(backbone, pretrained=True)\n","    model = convert_3d(model)\n","    if LOAD_WEIGHT_PATH is not None:\n","        model.load_state_dict(torch.load(LOAD_WEIGHT_PATH))\n","    model = model.to(device)\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=init_lr)\n","    scaler = torch.cuda.amp.GradScaler()\n","    from_epoch = 0\n","    metric_best = 0.\n","    loss_min = np.inf\n","\n","    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, n_epochs)\n","\n","    print(len(dataset_train), len(dataset_valid))\n","\n","    for epoch in range(1, n_epochs+1):\n","        scheduler_cosine.step(epoch-1)\n","\n","        print(time.ctime(), 'Epoch:', epoch)\n","\n","        train_loss = train_func(model, loader_train, optimizer, scaler)\n","        valid_loss, metric, best_thres = valid_func(model, loader_valid)\n","\n","        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f},'\n","        content+= f'train loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}, metric: {(metric):.6f} at thres {best_thres}.'\n","        print(content)\n","        with open(log_file, 'a') as appender:\n","            appender.write(content + '\\n')\n","\n","        if metric > metric_best:\n","            print(f'metric_best ({metric_best:.6f} --> {metric:.6f}). Saving model ...')\n","            torch.save(model.state_dict(), model_file)\n","            metric_best = metric\n","\n","        # Save Last\n","        if not DEBUG:\n","            torch.save(\n","                {\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'scaler_state_dict': scaler.state_dict() if scaler else None,\n","                    'score_best': metric_best,\n","                },\n","                model_file.replace('_best', '_last')\n","            )\n","\n","    del model\n","    torch.cuda.empty_cache()\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:33.006143Z","iopub.status.busy":"2022-10-29T06:03:33.005515Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 164/164 [00:04<00:00, 38.03it/s]\n","100%|██████████| 42/42 [00:01<00:00, 37.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["164 42\n","Fri Sep 29 12:55:39 2023 Epoch: 1\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/41 [00:04<?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"Sizes of tensors must match except in dimension 1. Expected size 14 but got size 13 for tensor number 1 in the list.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m run(\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m run(\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m run(\u001b[39m2\u001b[39m)\n","\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m scheduler_cosine\u001b[39m.\u001b[39mstep(epoch\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(time\u001b[39m.\u001b[39mctime(), \u001b[39m'\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m'\u001b[39m, epoch)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m train_loss \u001b[39m=\u001b[39m train_func(model, loader_train, optimizer, scaler)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m valid_loss, metric, best_thres \u001b[39m=\u001b[39m valid_func(model, loader_valid)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m content \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mctime() \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m, Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, lr: \u001b[39m\u001b[39m{\u001b[39;00moptimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.7f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\n","\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     images, gt_masks, gt_masks_sfl, lam \u001b[39m=\u001b[39m mixup(images, gt_masks)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mautocast():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     logits \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(logits, gt_masks)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mif\u001b[39;00m do_mixup:\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32m/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     global_features \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)[:n_blocks]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     seg_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\u001b[39m*\u001b[39mglobal_features)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     seg_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegmentation_head(seg_features)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.55.170/home/junseonglee/Desktop/01_codes/Kaggle-Competition-Results/02_RSNA_Abdominal_Trauma/Step1_train_segmentation_model.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m seg_features\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/segmentation_models_pytorch/unet/decoder.py:119\u001b[0m, in \u001b[0;36mUnetDecoder.forward\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mfor\u001b[39;00m i, decoder_block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n\u001b[1;32m    118\u001b[0m     skip \u001b[39m=\u001b[39m skips[i] \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(skips) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     x \u001b[39m=\u001b[39m decoder_block(x, skip)\n\u001b[1;32m    121\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/segmentation_models_pytorch/unet/decoder.py:38\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     36\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(x, scale_factor\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m skip \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x, skip], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention1(x)\n\u001b[1;32m     40\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/monai/data/meta_tensor.py:276\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 276\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m__torch_function__(func, types, args, kwargs)\n\u001b[1;32m    277\u001b[0m \u001b[39m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m#     return ret\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m _not_requiring_metadata(ret):\n","File \u001b[0;32m~/miniconda3/envs/rsna_abtd/lib/python3.11/site-packages/torch/_tensor.py:1386\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1385\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1386\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1387\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1388\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n","\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 14 but got size 13 for tensor number 1 in the list."]}],"source":["run(0)\n","run(1)\n","run(2)\n","run(3)\n","run(4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
